{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your own XGBoost models\n",
    "\n",
    "Before proceeding, confirm that you have done the `Feature Sets and Logging Lab`\n",
    "\n",
    "Our goal here to pick up the training data we generated for the `Title` judgments and use it to train an xgboost model.\n",
    "\n",
    "We have an example set of features logged out for you in `data/title_features.csv`. But you are encouraged to you the features you created in `Feature Sets and Logging Lab`. To save your own features out to disk in that notebook, run: `df.to_csv('data/title_features.csv', index=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/title_features.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Set up the DM matrix for XGBoost from df using all of the logged features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define required params for your ranking model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your ranking model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of you model\n",
    "print(___your_model____.eval(___your_data_____))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Modify your feature set\n",
    "\n",
    "Create a new feature (or two) and re-log the feature values to `title_features.csv`. The goal here is to train a new model and see if it have a higher eval metric than your original\n",
    "\n",
    "You might want to plot you models to see how and where new features are getting used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot your model to see the splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your own Ranklib models\n",
    "\n",
    "Again we will use the `Title` judgments to experiment with different rankers and objectives.\n",
    "\n",
    "Our goal here is to show how good `LambdaMart` is for ranking tasks.\n",
    "\n",
    "Because Ranklib supports a range of models and objective metrics, this is the easier tool to use compared to XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ltr\n",
    "from ltr.client import _____\n",
    "from ltr.log import FeatureLogger\n",
    "from ltr.judgments import judgments_open\n",
    "from itertools import groupby\n",
    "\n",
    "# Set up your engine\n",
    "client = ______\n",
    "\n",
    "ftr_logger=FeatureLogger(client, index='tmdb' , feature_set= ____ )\n",
    "\n",
    "\n",
    "with judgments_open('data/title_judgments.txt') as judgment_list:\n",
    "    for qid, query_judgments in groupby(judgment_list, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(judgments=query_judgments, \n",
    "                               qid=qid,\n",
    "                               keywords=judgment_list.keywords(qid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to grab the raw ranklib command\n",
    "from ltr.ranklib import train\n",
    "\n",
    "train(client, training_set=ftr_logger.logged, \n",
    "      index='tmdb', featureSet= ____, modelName= _____)\n",
    "\n",
    "# raw command looks like:\n",
    "# java -jar /var/folders/7_/cvjz84n54vx7zv_pw3gmdqr00000gn/T/RankyMcRankFace.jar -ranker 6 -shrinkage 0.1 -metric2t DCG@10 -tree 50 -bag 1 -leaf 10 -frate 1.0 -srate 1.0 -train /var/folders/7_/cvjz84n54vx7zv_pw3gmdqr00000gn/T/training.txt -save data/tates_model.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the raw command, remember to pre-pend with a bang (!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set -tree to 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set -metric2t to NDCG@10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set -ranker to 9 (Linear Regression)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set -ranker to 8 (Random Forest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which model performs the best?\n",
    "\n",
    "Boosted trees are a great model option in general and that holds true for search. Their ability to hand correlated features and lack of feature scaling requirments make them very attractive for relevancy features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
