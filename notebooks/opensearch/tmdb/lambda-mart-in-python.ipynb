{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LambdaMART in Python\n",
    "\n",
    "This is an implementation of LambdaMART in Python using sklearn and pandas. This is for educational purposes. \n",
    "\n",
    "But a secondary goal in getting this into Python is to more easily hack the algorithm to try new ideas. For example, this [blog article on two-sided marketplaces](https://opensourceconnections.com/blog/2017/07/04/optimizing-user-product-match-economies/), perhaps as more of an online algorithm (retiring old trees in the ensemble, adding new ones over time), perhaps with different model architectures in the ensemble (BERTy transformery things?) but all that preserve some of the nice things about LambdaMART (directly optimizing a list-wise metric)\n",
    "\n",
    "This is adapted from [RankLib](https://github.com/o19s/RankyMcRankFace/blob/master/src/ciir/umass/edu/learning/tree/LambdaMART.java#L444) based on [this paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf) from Microsoft Research.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup - TheMovieDB corpus and log OpenSearch features from TMDB](#Part-Zero---Setup---Get-TheMovieDB-Corpus-and-Log-Simple-Features) - plumbing to interact with OpenSearch Learning to Rank to log a few basic features for our exploration\n",
    "2. [Pairwise swapping](#Part-One---Collect-pair-wise-DCG-diffs) - here we demonstrate the core operation of LambdaMART - pairwise swapping of pairs and examining DCG (or another ranking metric) impact\n",
    "3. [Scale to learn errors, not just swaps](#Part-Two---Compute-the-swaps-but-scaled-to-current-model's-error) - here we show how LambdaMART isn't just about learning the pairwise DCG difference of a swap, but the error currently in the model at predicting the DCG impact of that swap\n",
    "4. [Weigh predictions](#Part-Three---Weigh-each-leaf's-predictions) - here we weigh the predictions of the next model in the ensemble based on how much DCG remains to be learned. \n",
    "5. [Putting it all together](#Part-Four---Putting-it-all-together,-from-the-top!) - the full algorithm in one place. You can also compare this notebook's output and learning to Ranklib.\n",
    "\n",
    "---\n",
    "\n",
    "6. [A Pandas version!](#5.-Pure-Pandas-Implementation?) -- walking through a faster version computing the per-tree training data using Pandas - a much for useful toy example.\n",
    "\n",
    "## Known Issues\n",
    "\n",
    "I'm still learning the nooks and crannies of the algorithm. So there are some known issues as this is actively being developed.\n",
    "\n",
    "1. **Performance** - a single training round takes about 9 seconds. There's room for improvement in the hot part of the loop (dcg computation and swapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Zero - Setup - Get TheMovieDB Corpus and Log Simple Features\n",
    "\n",
    "In this step we download TheMovieDB Corpus and log some featurs (title and overview BM25). At the end we have a simple dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.client import OpenSearchClient\n",
    "client = OpenSearchClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and index TMDB corpus and training set\n",
    "\n",
    "Download [TheMovieDB](http://themoviedb.org) corpus and small toy training set with 40 queries labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/tmdb.json already exists\n",
      "data/title_judgments.txt already exists\n",
      "Index tmdb already exists. Use `force = True` to delete and recreate\n"
     ]
    }
   ],
   "source": [
    "from ltr import download\n",
    "corpus='http://es-learn-to-rank.labs.o19s.com/tmdb.json'\n",
    "judgments='http://es-learn-to-rank.labs.o19s.com/title_judgments.txt'\n",
    "\n",
    "download([corpus, judgments], dest='data/');\n",
    "from ltr.index import rebuild\n",
    "from ltr.helpers.movies import indexable_movies\n",
    "\n",
    "movies=indexable_movies(movies='data/tmdb.json')\n",
    "rebuild(client, index='tmdb', doc_src=movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log two features - title & overview\n",
    "\n",
    "Using the OpenSearch Learning to Rank plugin, we:\n",
    "\n",
    "1. Log two features: title and overview bm25\n",
    "2. Create a pandas dataframe containing the labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Default LTR feature store [Status: 200]\n",
      "Initialize Default LTR feature store [Status: 200]\n",
      "Create movies feature set [Status: 201]\n",
      "Recognizing 40 queries in: data/title_judgments.txt\n"
     ]
    }
   ],
   "source": [
    "from ltr.log import FeatureLogger\n",
    "from ltr.judgments import judgments_open\n",
    "from itertools import groupby\n",
    "from ltr.judgments import judgments_to_dataframe\n",
    "\n",
    "client.reset_ltr(index='tmdb')\n",
    "\n",
    "config = {\"validation\": {\n",
    "              \"index\": \"tmdb\",\n",
    "              \"params\": {\n",
    "                  \"keywords\": \"rambo\"\n",
    "              }\n",
    "    \n",
    "           },\n",
    "           \"featureset\": {\n",
    "            \"features\": [\n",
    "                { #1\n",
    "                    \"name\": \"title_bm25\",\n",
    "                    \"params\": [\"keywords\"],\n",
    "                    \"template\": {\n",
    "                        \"match\": {\"title\": \"{{keywords}}\"}\n",
    "                    }\n",
    "                },\n",
    "                { #2\n",
    "                    \"name\": \"overview_bm25\",\n",
    "                    \"params\": [\"keywords\"],\n",
    "                    \"template\": {\n",
    "                        \"match\": {\"overview\": \"{{keywords}}\"}\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "    }}\n",
    "\n",
    "\n",
    "client.create_featureset(index='tmdb', name='movies', ftr_config=config)\n",
    "\n",
    "# Log features for each query\n",
    "ftr_logger=FeatureLogger(client, index='tmdb', feature_set='movies')\n",
    "with judgments_open('data/title_judgments.txt') as judgment_list:\n",
    "    for qid, query_judgments in groupby(judgment_list, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(judgments=query_judgments, \n",
    "                               qid=qid,\n",
    "                               keywords=judgment_list.keywords(qid))\n",
    "        \n",
    "# Convert to Pandas Dataframe\n",
    "judgments = judgments_to_dataframe(ftr_logger.logged, unnest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine judgments dataframe\n",
    "\n",
    "In the dataframe we have a set of (query, document, grade) that label how relevant a document (movie) is for each query.\n",
    "\n",
    "* qid - 'query id' - a unique identifier for this query\n",
    "* docId - an identifier for the document (here movie) being labeled\n",
    "* grade - how relevant a movie is on a 0-4 scale\n",
    "* keywords - the query keywords that go along with the query id\n",
    "* features - the two features we logged, 0th is title_bm25, 1st is overview_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            uid  qid   keywords   docId  grade                features\n",
       "0        1_7555    1      rambo    7555      4  [11.657399, 10.083591]\n",
       "1        1_1370    1      rambo    1370      3   [9.456276, 13.265001]\n",
       "2        1_1369    1      rambo    1369      3   [6.036743, 11.113943]\n",
       "3       1_13258    1      rambo   13258      2         [0.0, 6.869545]\n",
       "4        1_1368    1      rambo    1368      4        [0.0, 11.113943]\n",
       "...         ...  ...        ...     ...    ...                     ...\n",
       "1385   40_37079   40  star wars   37079      0              [0.0, 0.0]\n",
       "1386  40_126757   40  star wars  126757      0              [0.0, 0.0]\n",
       "1387   40_39797   40  star wars   39797      0              [0.0, 0.0]\n",
       "1388   40_18112   40  star wars   18112      0              [0.0, 0.0]\n",
       "1389   40_43052   40  star wars   43052      0              [0.0, 0.0]\n",
       "\n",
       "[1390 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judgments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One - Collect pair-wise DCG diffs\n",
    "\n",
    "The first-pass iteration of LambdaMART, for each query, we examine the DCG\\* impact of swapping each result with another result in the listing.\n",
    "\n",
    "\\* replace DCG with your metric of interest: MAP, Precision@N, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>427.587115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>9.463946</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>219.800566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>62.204093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>3.014736</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>43.694734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>1.160558</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>5.542298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>1385</td>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>25</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.598524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1386</td>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>26</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.715586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1387</td>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>27</td>\n",
       "      <td>0.205847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.826142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1388</td>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>28</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.930783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1389</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-21.030027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index        uid  qid   keywords   docId  grade  \\\n",
       "qid                                                       \n",
       "1   0       0     1_7555    1      rambo    7555      4   \n",
       "    1       4     1_1368    1      rambo    1368      4   \n",
       "    2       1     1_1370    1      rambo    1370      3   \n",
       "    3       2     1_1369    1      rambo    1369      3   \n",
       "    4       3    1_13258    1      rambo   13258      2   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1385   40_37079   40  star wars   37079      0   \n",
       "    26   1386  40_126757   40  star wars  126757      0   \n",
       "    27   1387   40_39797   40  star wars   39797      0   \n",
       "    28   1388   40_18112   40  star wars   18112      0   \n",
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  display_rank  discount       gain        dcg  \\\n",
       "qid                                                                            \n",
       "1   0   [11.657399, 10.083591]             0  1.000000  15.000000  33.734341   \n",
       "    1         [0.0, 11.113943]             1  0.630930   9.463946  33.734341   \n",
       "    2    [9.456276, 13.265001]             2  0.500000   3.500000  33.734341   \n",
       "    3    [6.036743, 11.113943]             3  0.430677   3.014736  33.734341   \n",
       "    4          [0.0, 6.869545]             4  0.386853   1.160558  33.734341   \n",
       "...                        ...           ...       ...        ...        ...   \n",
       "40  25              [0.0, 0.0]            25  0.210310   0.000000  31.225149   \n",
       "    26              [0.0, 0.0]            26  0.208015   0.000000  31.225149   \n",
       "    27              [0.0, 0.0]            27  0.205847   0.000000  31.225149   \n",
       "    28              [0.0, 0.0]            28  0.203795   0.000000  31.225149   \n",
       "    29              [0.0, 0.0]             8  0.301030   0.000000  31.225149   \n",
       "\n",
       "            lambda  \n",
       "qid                 \n",
       "1   0   427.587115  \n",
       "    1   219.800566  \n",
       "    2    62.204093  \n",
       "    3    43.694734  \n",
       "    4     5.542298  \n",
       "...            ...  \n",
       "40  25  -20.598524  \n",
       "    26  -20.715586  \n",
       "    27  -20.826142  \n",
       "    28  -20.930783  \n",
       "    29  -21.030027  \n",
       "\n",
       "[1390 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log, exp\n",
    "import numpy as np \n",
    "\n",
    "def rank_with_swap(ranked_list, rank1=0, rank2=0):\n",
    "    \"\"\" Set the display rank of positions given the provided swap \"\"\"\n",
    "    ranked_list['display_rank'] = ranked_list.index.to_series()\n",
    "    \n",
    "    if rank1 != rank2:\n",
    "        ranked_list.loc[rank1, 'display_rank'] = rank2\n",
    "        ranked_list.loc[rank2, 'display_rank'] = rank1\n",
    "    return ranked_list\n",
    "    \n",
    "\n",
    "def dcg(ranked_list, at=10):\n",
    "    \"\"\"Given a list, compute DCG -- \n",
    "       uses same variant as lambdamart 2**grade / log2(displayrank)\n",
    "    \"\"\"\n",
    "    ranked_list['discount'] = 1 / np.log2(2 + ranked_list['display_rank'])\n",
    "    ranked_list['gain'] = (2**ranked_list['grade'] - 1) * ranked_list['discount'] # TODO - precompute gain on swapping\n",
    "    return sum(ranked_list['gain'].head(at))\n",
    "\n",
    "def compute_swaps(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\"\"\"\n",
    "    \n",
    "    # Sort to see ideal ordering\n",
    "    # This isn't strictly nescesarry, but it's helpful to understand the algorithm\n",
    "    query_judgments = query_judgments.sort_values('grade', kind='stable', ascending=False).reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    best_dcg = query_judgments.loc[0, 'dcg']\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    \n",
    "    # TODO - redo inner body as \n",
    "    for better in range(0,len(query_judgments)):\n",
    "        for worse in range(0,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                break\n",
    "\n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                query_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "\n",
    "                dcg_after_swap = query_judgments.loc[0, 'dcg']\n",
    "                delta = abs(best_dcg - dcg_after_swap)\n",
    "\n",
    "                if delta > 0.0:\n",
    "\n",
    "                    # Add delta to better's lambda (-delta to worse's lambda)\n",
    "                    query_judgments.loc[better, 'lambda'] += delta\n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta\n",
    "\n",
    "    # print(query_judgments[['keywords', 'docId', 'grade', 'lambda', 'features']])\n",
    "    return query_judgments\n",
    "\n",
    "# For each query, compute lambdas\n",
    "# %prun -s cumulative lambdas_per_query = judgments.groupby('qid').apply(compute_swaps, axis=1)\n",
    "# judgments\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps, axis=1)\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Precision instead of DCG\n",
    "\n",
    "We can really use any ranking metric to achieve goals important to our product. This includes potentially ones we invent or come up with ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>5_603</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>603</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.040129]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150</td>\n",
       "      <td>5_604</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>604</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 9.392262]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>151</td>\n",
       "      <td>5_605</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>605</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 0.0]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>152</td>\n",
       "      <td>5_55931</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>55931</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 10.798681]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "      <td>5_73262</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>73262</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>32</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>153</td>\n",
       "      <td>5_1857</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>1857</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 9.65805]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>154</td>\n",
       "      <td>5_10999</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>10999</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 11.466951]</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>155</td>\n",
       "      <td>5_4247</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>4247</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 8.114125]</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>156</td>\n",
       "      <td>5_21874</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>21874</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 7.8627386]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>157</td>\n",
       "      <td>5_181886</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>181886</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>158</td>\n",
       "      <td>5_21208</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>21208</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>159</td>\n",
       "      <td>5_125607</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>125607</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>160</td>\n",
       "      <td>5_56441</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>56441</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>12</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>161</td>\n",
       "      <td>5_124080</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>124080</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>13</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>162</td>\n",
       "      <td>5_1487</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>1487</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>14</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>163</td>\n",
       "      <td>5_72867</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>72867</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>15</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>164</td>\n",
       "      <td>5_11253</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>11253</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>165</td>\n",
       "      <td>5_213110</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>213110</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>17</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>166</td>\n",
       "      <td>5_13805</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>13805</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>18</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>167</td>\n",
       "      <td>5_104221</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>104221</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>19</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>168</td>\n",
       "      <td>5_183894</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>183894</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>20</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>169</td>\n",
       "      <td>5_3573</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>3573</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>21</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>170</td>\n",
       "      <td>5_12254</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>12254</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>22</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>171</td>\n",
       "      <td>5_17813</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>17813</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>23</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>173</td>\n",
       "      <td>5_17960</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>17960</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>24</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>174</td>\n",
       "      <td>5_33068</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>33068</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>25</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>175</td>\n",
       "      <td>5_28377</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>28377</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>26</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>176</td>\n",
       "      <td>5_13300</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>13300</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>27</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>177</td>\n",
       "      <td>5_680</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>28</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>178</td>\n",
       "      <td>5_28131</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>28131</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>29</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>179</td>\n",
       "      <td>5_37988</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>37988</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>180</td>\n",
       "      <td>5_18451</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>18451</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>31</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>181</td>\n",
       "      <td>5_75404</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>75404</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index       uid  qid keywords   docId  grade                features  \\\n",
       "0     149     5_603    5   matrix     603      4  [11.657399, 10.040129]   \n",
       "1     150     5_604    5   matrix     604      3    [9.456276, 9.392262]   \n",
       "2     151     5_605    5   matrix     605      3         [9.456276, 0.0]   \n",
       "3     152   5_55931    5   matrix   55931      2        [0.0, 10.798681]   \n",
       "4     172   5_73262    5   matrix   73262      1              [0.0, 0.0]   \n",
       "5     153    5_1857    5   matrix    1857      0          [0.0, 9.65805]   \n",
       "6     154   5_10999    5   matrix   10999      0        [0.0, 11.466951]   \n",
       "7     155    5_4247    5   matrix    4247      0         [0.0, 8.114125]   \n",
       "8     156   5_21874    5   matrix   21874      0        [0.0, 7.8627386]   \n",
       "9     157  5_181886    5   matrix  181886      0              [0.0, 0.0]   \n",
       "10    158   5_21208    5   matrix   21208      0              [0.0, 0.0]   \n",
       "11    159  5_125607    5   matrix  125607      0              [0.0, 0.0]   \n",
       "12    160   5_56441    5   matrix   56441      0              [0.0, 0.0]   \n",
       "13    161  5_124080    5   matrix  124080      0              [0.0, 0.0]   \n",
       "14    162    5_1487    5   matrix    1487      0              [0.0, 0.0]   \n",
       "15    163   5_72867    5   matrix   72867      0              [0.0, 0.0]   \n",
       "16    164   5_11253    5   matrix   11253      0              [0.0, 0.0]   \n",
       "17    165  5_213110    5   matrix  213110      0              [0.0, 0.0]   \n",
       "18    166   5_13805    5   matrix   13805      0              [0.0, 0.0]   \n",
       "19    167  5_104221    5   matrix  104221      0              [0.0, 0.0]   \n",
       "20    168  5_183894    5   matrix  183894      0              [0.0, 0.0]   \n",
       "21    169    5_3573    5   matrix    3573      0              [0.0, 0.0]   \n",
       "22    170   5_12254    5   matrix   12254      0              [0.0, 0.0]   \n",
       "23    171   5_17813    5   matrix   17813      0              [0.0, 0.0]   \n",
       "24    173   5_17960    5   matrix   17960      0              [0.0, 0.0]   \n",
       "25    174   5_33068    5   matrix   33068      0              [0.0, 0.0]   \n",
       "26    175   5_28377    5   matrix   28377      0              [0.0, 0.0]   \n",
       "27    176   5_13300    5   matrix   13300      0              [0.0, 0.0]   \n",
       "28    177     5_680    5   matrix     680      0              [0.0, 0.0]   \n",
       "29    178   5_28131    5   matrix   28131      0              [0.0, 0.0]   \n",
       "30    179   5_37988    5   matrix   37988      0              [0.0, 0.0]   \n",
       "31    180   5_18451    5   matrix   18451      0              [0.0, 0.0]   \n",
       "32    181   5_75404    5   matrix   75404      0              [0.0, 0.0]   \n",
       "\n",
       "    display_rank  dcg  lambda  \n",
       "0              0  0.3   2.300  \n",
       "1              1  0.3   1.725  \n",
       "2              2  0.3   1.725  \n",
       "3              3  0.3   1.150  \n",
       "4             32  0.3   0.575  \n",
       "5              5  0.3   0.000  \n",
       "6              6  0.3   0.000  \n",
       "7              7  0.3   0.000  \n",
       "8              8  0.3   0.000  \n",
       "9              9  0.3   0.000  \n",
       "10            10  0.3  -0.325  \n",
       "11            11  0.3  -0.325  \n",
       "12            12  0.3  -0.325  \n",
       "13            13  0.3  -0.325  \n",
       "14            14  0.3  -0.325  \n",
       "15            15  0.3  -0.325  \n",
       "16            16  0.3  -0.325  \n",
       "17            17  0.3  -0.325  \n",
       "18            18  0.3  -0.325  \n",
       "19            19  0.3  -0.325  \n",
       "20            20  0.3  -0.325  \n",
       "21            21  0.3  -0.325  \n",
       "22            22  0.3  -0.325  \n",
       "23            23  0.3  -0.325  \n",
       "24            24  0.3  -0.325  \n",
       "25            25  0.3  -0.325  \n",
       "26            26  0.3  -0.325  \n",
       "27            27  0.3  -0.325  \n",
       "28            28  0.3  -0.325  \n",
       "29            29  0.3  -0.325  \n",
       "30            30  0.3  -0.325  \n",
       "31            31  0.3  -0.325  \n",
       "32             4  0.3  -0.325  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision(ranked_list, max_grade=4.0, at=10):\n",
    "    \"\"\"Given a list, compute simple precision. Really this is cumalitive gain.\"\"\"\n",
    "    above_n = ranked_list[ranked_list['display_rank'] < at]\n",
    "    \n",
    "    if (max_grade * at) == 0.0:\n",
    "        print(\"0\")\n",
    "        return 0.0\n",
    "    \n",
    "    return float(sum(above_n['grade'])) / (max_grade * at)\n",
    "\n",
    "\n",
    "lambdas_per_query_prec = judgments.groupby('qid').apply(compute_swaps, axis=1, metric=precision)\n",
    "lambdas_per_query_prec.loc[5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a model on the lambdas\n",
    "\n",
    "The core operation is fitting an operation on the lambdas (the accumulated pairwise differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>lambda</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>427.587115</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>219.800566</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.204093</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43.694734</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.542298</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>-20.598524</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-20.715586</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-20.826142</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-20.930783</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-21.030027</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            lambda                features\n",
       "qid                                       \n",
       "1   0   427.587115  [11.657399, 10.083591]\n",
       "    1   219.800566        [0.0, 11.113943]\n",
       "    2    62.204093   [9.456276, 13.265001]\n",
       "    3    43.694734   [6.036743, 11.113943]\n",
       "    4     5.542298         [0.0, 6.869545]\n",
       "...            ...                     ...\n",
       "40  25  -20.598524              [0.0, 0.0]\n",
       "    26  -20.715586              [0.0, 0.0]\n",
       "    27  -20.826142              [0.0, 0.0]\n",
       "    28  -20.930783              [0.0, 0.0]\n",
       "    29  -21.030027              [0.0, 0.0]\n",
       "\n",
       "[1390 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(train_set['features'].tolist(), train_set['lambda'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCG-based Lambda Predictions\n",
    "\n",
    "We show predicting some known examples. In the first case, strong title and overview scores. In the second case, no title or overview scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([345.6500854])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.predict([[11.1, 10.08]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-15.3987952])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.predict([[0.0, 0.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's more typical we would restrict the complexity of each tree in the ensemble. We can dump the tree see [understanding sklearn's tree structure](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0.8333333333333334, 'X[0] <= 10.666\\nsquared_error = 4161.543\\nsamples = 1390\\nvalue = -0.0'),\n",
       " Text(0.25, 0.5, 'X[0] <= 9.182\\nsquared_error = 1038.341\\nsamples = 1329\\nvalue = -9.745'),\n",
       " Text(0.125, 0.16666666666666666, 'squared_error = 421.042\\nsamples = 1301\\nvalue = -11.724'),\n",
       " Text(0.375, 0.16666666666666666, 'squared_error = 21086.54\\nsamples = 28\\nvalue = 82.191'),\n",
       " Text(0.75, 0.5, 'X[0] <= 18.186\\nsquared_error = 25061.546\\nsamples = 61\\nvalue = 212.311'),\n",
       " Text(0.625, 0.16666666666666666, 'squared_error = 26150.423\\nsamples = 51\\nvalue = 188.147'),\n",
       " Text(0.875, 0.16666666666666666, 'squared_error = 1343.167\\nsamples = 10\\nvalue = 335.547')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAGFCAYAAABg2vAPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAABhhUlEQVR4nO3dd1gUZ/c38C9FmhBYJRo1KhpUwIodQXaXJliwN+xJJI+aRI1RbFHzi0GNj09iEjUaYxe7gA1FgUWxJioqwQIqomJDkNhY2rx/8O5GZBFEYBb2+7kuroSd2Zmz3HvGM/fOntETBEEAERER6Sx9sQMgIiIicbEYICIi0nEsBoiIiHQciwEiIiIdx2KAiIhIx7EYICIi0nEsBoiIiHQciwEiIiIdx2KAiIhIx7EYICIi0nEsBoiIiHQciwEiIiIdx2KAiIhIx7EYICIi0nEsBoiIiHQciwEiIiIdx2KAiIhIx7EYICIi0nEsBoiIiHQciwEiIiIdx2KAiIhIx7EYICIi0nEsBoiIiHQciwEiIiIdx2KAiIhIx7EYICIi0nEsBoiIiHQciwEiIiIdx2KAiIhIx7EYICIi0nEsBoiIiHQciwEiIiIdx2KAiIhIx7EYICIi0nEsBoiIiHQciwEiIiIdx2KAiIhIx7EYICIi0nEsBoiIiHQciwEiIiIdx2KAiIhIx7EYICIi0nEsBoiIiHQciwEiIiIdx2KAiIhIx7EYICIi0nGGYgdApI2Sk5ORmpoqdhhUTqytrdGgQQOxwyDSGiwGiF6TnJwMe3t7vHjxQuxQqJyYmZnh8uXLLAiI/j8WA0SvSU1NxYsXL7Bp0ybY29uLHQ6VscuXL2P48OFITU1lMUD0/7EYICqCvb092rZtK3YYRETljhcQEhER6TgWA0QiunDhAnx8fNS/e3h44Nq1a1i3bh1sbW0RHBwMAAgODkaXLl3g6uqKa9euAQA2btwIGxsbhISElHlcCoUCzZs3h7W1dYHHly1bBmdnZ3Tr1g0PHz4s9Lz4+Hj4+PhALpdj8eLFAICsrCxMmDAB7u7ucHd3V6+7a9cuuLm5QS6X4/Dhw2X+Goio5PgxAZGIWrdujcaNG2PXrl14+fIl2rdvj6ZNm+LEiRP4/PPP0bdvX+Tk5CAwMBBHjx7FjRs3MHPmTOzcuRMjRozA9evXS7Sf58+fo3r16iWOq02bNvjzzz/RpUsX9WOPHz/G1q1bcezYMYSFhWHRokVYsmRJgecFBARgy5YtsLKyUj+2bNkyuLm5YdmyZerH7t+/j+3bt+PIkSPQ1+c5CZHYmIVEIps/fz4WLFiApUuXYvbs2YWWJyQkwM7ODqampmjevDlu375dou0KgoCoqCiMHDkSQ4cOfauYrKysYGZmVuCxM2fOQCaTQV9fH926dcOpU6cKLL958yaUSiVGjx4NDw8PxMbGAgD27duHP//8EzKZDP/73/8AAGFhYTA1NUW3bt0wePBgpKWlvVV8RFS2ODNAJDKJRIJatWqhQYMGMDc3L7Q8PT0dlpaW6t/z8vLeuL1Hjx7ht99+Q2RkJJydnfHNN9+gSZMmAPKn8cePH1/oObt370aNGjXeuN1X4zA0NERWVlaB5ffv38fFixdx+fJlPHnyBCNHjsSxY8dw584dfPLJJwgMDETPnj3h5eWF+/fv4/79+zh06BC2bduGBQsWqD9WIKKKx2KASGRHjhxBzZo1ER8fj5s3b6JRo0YFlkskEmRkZKh/L25a/erVq9i8eTOGDx+OUaNGoX79+uplDg4OUCgUpYpTIpHg8uXLAIDc3FwYGRkVWG5lZQVHR0dIJBJIJBIolUr14+7u7tDX14ebmxvi4+NhZWUFuVwOfX19eHp6YuPGjaWKiYjKBj8mIBJRdnY2Zs+ejcWLF+OHH37AlClTCq3TpEkTXLlyBZmZmYiPj8eHH374xm26uLggPj4e7du3x7Rp09CjRw8EBQUByJ8ZkMlkhX5KMk3foUMHREdHQxAEhIeHo3PnzoXiTE9Ph1KpRFpaGgwMDAAArq6u6o8Mzp07h48++kjjY0QkHs4MEIlo6dKlGDhwID744AN88MEHMDc3x5EjRwqsY2hoiOnTp0Mul6NatWr4/fffi92uvr4+vL294e3tjbS0NPW3Eko6M3DhwgVMmTIF169fh4eHB77//nt06tQJAwcOhIuLC6pXr64+m1+4cCEGDBgAW1tbzJw5E+7u7sjJycGCBQsAANOmTcPo0aMxf/58dOzYEe3atQMANG3aFFKpFIaGhli/fv3b/NmIqIzpCYIgiB0EkTY5d+4c2rVrh7Nnz4rWdGjnzp0IDAzEN998g759+2pcZ+PGjViyZAl++OEHeHl5VXCElZc2jC+RtuHMAJEWGjBgAAYMGPDGdUaMGIERI0ZUUEREVJXxmgEiIiIdx2KAqJKQyWR48uSJ2GEUy8vLC5MmTQIAXLlyBW3atIGJiUmB2FNSUtC3b1+4ubmpL5rcunUrmjVrhjZt2mjc7rx589CiRQvIZDJ8+umnBZadOHECenp66n2MHTsWUqkUHTp0wI4dO8r6JRJVOfyYgEgL5OXllVsnvte3Xdy+3iWW6OhoGBr+e1ipX78+oqOj0bt37wLrff3111i6dGmBuwZ6eHigX79+6NixY5Hbnz9/Pvr06VPo8Z9++gnt27dX/75s2TIYGRnh6dOncHJywsCBA0v1eoh0BWcGiEogLi4OnTt3hlwux2effQYAiIqKgqOjI/r27QupVIrY2FgoFAr1WXFSUpL6H67FixdDJpOhbdu22LNnD4D8M91Ro0ahZ8+eOHPmDBYsWACpVApnZ2ecPn0aABAUFIR27dph8ODBSE9PLzI+Tc9t1aoVJk2ahEGDBhXaV0BAAFxcXCCVSnHlypVC65fW0qVLMWHCBPXv1atXL9AwCQBycnJw/fp1TJ8+HTKZTP3tCWtr60K9C1737bffwtXVFfv27VM/duTIEXTo0KFAu2XVdl68eAEHB4dSvx4iXcGZAaISOHToEPz9/fHxxx+rOwDOmDEDhw8fhqWlJVq1avXG548fPx5Tp07FkydP4OXlBV9fXwBA7dq1sX79esTFxSEuLg7R0dFITU3F0KFDcfDgQSxatAinT59GVlYWbGxsNG5b03MPHz6Mp0+fwt/fHw4ODpg3b556X+fOncP169cRExODuLg4BAQEIDQ0tMD6rwoKCsKqVasKPNagQQNs2LChwGNhYWHo0qVLsfdAePToES5cuICtW7fCysoKMpkM58+fL3Y24osvvsC8efPw+PFjeHh4wNnZGRKJBD///DO2bt2K/fv3F1i/X79+iImJwaJFi964XSJiMUBUImPGjMF3330HPz8/eHt7Y+TIkVAqleq7+qk+59bT01M/59Vv7QYFBWHDhg3Q19cvcG8BVeOe+Ph4nD59GjKZDED+Ge2jR49Qt25dmJiYwMTEBM2aNdMYm6bnAoCFhUWBf9hV+0pISECHDh0AAC1atEBKSorG9VX8/Pzg5+dX7N/o119/xY4dO3DmzJk3rmdlZYVGjRqpOy3Wr18fqampqFWr1hufV7NmTfV/nZyckJiYiHv37kEmkxW6jwKQ32I5LS0NHTp0wIgRIwp8fEFEBTE7iErA1NQUP/74IwRBgL29Pfz8/GBkZITHjx/D0tISFy5cAJDfsvfOnTsAgPPnz6ufv2TJEsTFxeHp06do0aKF+nHV2bCdnR1cXFywbt06APm3/TUwMEBKSgqUSiWysrLUty5+nabnvrrt1/dla2uL7du3A8ifVahbt67G9VVKMjPw9OlTpKSkoF+/fkhLS8PDhw8hk8k0fr5vamqK2rVrIy0tDdWrV8fdu3fV/9C/SUZGBiwtLZGVlYWzZ89i3rx5CA8PR2RkJMLDw3Hx4kWMGjUKoaGhUCqVMDY2RvXq1WFhYaHuhkhEmrEYICqBoKAgrF+/HoIgwMfHB4aGhggMDISnpycaNmyo/ge1ZcuWyM7OhoeHBxwdHdXPl8lkcHFxQbt27Qrc3lelVatWcHBwgFQqhb6+PpycnBAYGIipU6eiS5cuaNasGRo2bKgxtqKeW5R27dqhUaNGcHZ2hr6+PlauXPnG116SmQELCwt18aNQKBASEoI+ffrgwYMHGDZsGC5cuIA+ffpg8uTJ6N27NxYtWoTevXsjKysLAQEBMDAwQHh4OH744Qd118PVq1fDxMQEv/76K+bPn48pU6YgPj4eubm58Pf3R61atTBr1izMmjVL/TdWdTL09fVVF1GzZs0qMGNDRIWxAyHRa0rToW706NGYNGlSkV+LI+3BDoREhXFmgKgSuXr1qvrbDCorV64s8noCIqKSYDFAVAZUn9eXt2bNmpX6FsREREVhnwGiKqoiPrKQSqWwsrJCSEiI+rE5c+aou//98ssvAPIvMOzTpw/kcjnGjRun/npmcHAwunTpAldX1yIvkCSi8sdigIhKbcuWLeomSyqzZ89GdHQ0Tp48iWXLliE7OxurVq2Cj48PoqKiIJFIcPDgQeTk5CAwMBARERFYsWIFZs6cKc6LICIWA0Ri0dTVsKhOhX5+fujevTvc3d3xxx9/wN3dHV5eXsjNzYVCoYCnpyd8fX3Rrl27Qt/zT01NRZ8+feDm5oaBAwciMzNT475LQ/Utilepuv8plUo0btwY1apVQ2Jionqmom3btjh69CgSEhJgZ2cHU1NTNG/evED/BSKqWCwGiESi6moYFRWFFStWAMjvVKhQKBAZGYn58+er123atCkOHDiAxo0b486dO4iIiEDDhg1x4sQJAPnT8CEhIdi1axcCAgIK7GfhwoUYP348IiMjIZPJsGnTJo37ftXIkSMhk8kK/AQFBZX4tY0fPx62trZwcnICkN/cKCIiAgBw+PBhpKenIz09vUCrYtVHB0RU8VgMEIlkzJgxuHTpEvz8/LBp0yYA+f0Munbtit69exc4U27dujUAoF69eurWx/Xq1UNaWhoAwNHREfr6+rCxsSl0Z8P4+Hj83//9H2QyGTZu3IiHDx9q3PerNmzYAIVCUeCnJF0IVZYvX44bN24gJCQEt2/fxieffIKbN2/C3d0d1apVQ506dSCRSJCRkaF+TnndqImIisdvExCJRFNXw6I6Fb7aNEdTy+MLFy5AEAQkJycXampkZ2eHXr16QS6XA8jvUJibm1to36+26x05ciSSk5MLbMff379EBYGq+5+JiQnMzMxgamoKExMT/P777wCAyZMno0+fPmjSpAmuXLmCzMxM3LhxAx9++GEJ/3JEVNZYDBCJRFNXw+I6FRZFIpHA19cXKSkpWLZsWYFls2bNgr+/P7777jsAwNy5c5GYmFho3696/SZERRk+fDhOnDiB4OBgXLx4EXPmzIG/vz9u3bqFrKws+Pn5wdraGrGxsZg0aRIMDAzQt29f9fUD06dPh1wuR7Vq1dTFAhFVPHYgJHpNZetQp2r/+9NPP4kdSqVQ2caXqCLwQzoiIiIdx48JiCo51dX+RESlxZkBIiIiHcdigKiKEOOOicuWLYOzszO6deuGhw8fvvVyItIOLAaIqFQeP36MrVu34tixY/jyyy+xaNGit1pORNqDxQCRFvviiy8QExMDAPjrr78wduxYPHjwAHK5HK6urujVqxeUSmWB54wePRqxsbEA8lsZq24itGDBAkilUjg7O+P06dPvHNuZM2cgk8mgr6+Pbt264dSpU2+1nIi0B4sBIi02dOhQbN26FUD+TYH8/PwgkUgQHh6Oo0ePomXLlti7d2+x24mLi0NcXByio6MRGhqK2bNnF1rHy8urUAviyMjIIrf5ajthQ0NDZGVlvdVyItIe/DYBkRbr0qULJk+ejOzsbMTExGDx4sV4+PAhxo0bh7S0NDx48AC1a9cu8BxNHQrj4+Nx+vRp9bcOXrx4UWhf4eHhxcazdOlSBAcHw83NDR06dMDly5cBALm5ueobFKlIJJI3Lici7cFigEjLubm54dtvv4WzszP09fWxefNmeHl5Ydy4cZgxYwZe7xsmkUhw584dtGnTBufPn4ejoyPs7Ozg4uKCdevWAYDGs3QvL69Cj8+ZMwdubm7q3ydOnIiJEycCyL8b4oIFCyAIAsLDw9G5c+cCz+3QocMblxOR9mAxQKTl/Pz84OjoqP7M3d3dHSNGjEBYWBgsLCwKzQyMHj0aI0aMwJo1a2BsbAwAaNWqFRwcHCCVSqGvrw8nJycEBgYWeF5JZgZeZW1tjYEDB8LFxQXVq1fHxo0bAeTfJXHAgAGwtbXVuJyItA/bERO9hu1qqzaOL1FhvICQiIhIx7EYICIi0nG8ZoCoCKor4alq4bgSFcZigOg11tbWMDMzw/Dhw8UOhcqJmZkZrK2txQ6DSGvwAkIiDZKTk5GamipqDLm5uZg7dy4OHTqE77//Hl5eXqLGU1qHDh3CrFmz4OPjg3nz5sHAwEDskGBtbY0GDRqIHQaR1uDMAJEGDRo0EPUfi5ycHIwaNQrh4eHYsmULBg0aJFos76pt27Zo3Lgxhg0bBolEgvXr12tFQUBE/2IxQKRlcnJyMHLkSGzfvh1btmzBwIEDxQ7pnQ0ePBh6enrw8/ODIAhYv349DA15+CHSFsxGIi2Sk5ODESNGYOfOndi2bRv69+8vdkhlZtCgQdDX18eQIUOQl5eHjRs3siAg0hK8ZoBIS+Tk5GDYsGHYvXs3tm3bhn79+okdUrnYtWsXhgwZgv79+2PTpk0sCIi0AIsBIi2QnZ2NYcOGITg4GNu3b0ffvn3FDqlc7d69G4MHD0bfvn2xefNmVKtWTeyQiHQaiwEikWVnZ2Po0KHYs2cPduzYgd69e4sdUoUICQnBwIED0adPHwQFBbEgIBIRiwEiEWVnZ2PIkCHYu3cvdu7cCV9fX7FDqlChoaEYOHAgfH19sWXLFhYERCJhMUAkkqysLAwZMgT79u3Drl270KtXL7FDEsWePXswYMAA9OzZE1u3boWRkZHYIRHpHBYDRCLIysrCoEGDEBYWhl27dqFnz55ihySqvXv3on///ujRowe2bdvGgoCogrEYIKpgWVlZGDhwIA4ePIjdu3ejR48eYoekFfbt24f+/fvDx8cH27dvZ0FAVIFYDBBVIKVSiYEDByI8PBzBwcHw8fEROyStcuDAAfTt2xfdunXDjh07YGxsLHZIRDqBxQBRBVEqlejfvz+OHDmCkJAQeHt7ix2SVgoLC0Pfvn3h6emJnTt3siAgqgAsBogqQGZmJvr374+IiAiEhoaiW7duYoek1Q4ePIg+ffrAw8MDu3btYkFAVM5YDBCVs8zMTPTr1w9RUVEIDQ2ttHcfrGjh4eHo3bs33NzcsGvXLpiYmIgdElGVxWKAqBxlZmaiT58+iI6Oxt69e+Hh4SF2SJXK4cOH4evrC5lMhuDgYBYEROWExQBROXn58iX69OmDY8eOYe/evXB3dxc7pErpyJEj6NWrF6RSKUJCQlgQEJUDFgNE5eDly5fo3bs3YmJisG/fPri5uYkdUqUWERGBXr16oWvXrggJCYGpqanYIRFVKSwGiMrYixcv0Lt3bxw/fhz79++HXC4XO6QqITIyEj179oSzszNCQ0NhZmYmdkhEVQaLAaIy9OLFC/Tq1QunTp3C/v37IZPJxA6pSlEoFOjRowecnJywZ88eFgREZYTFAFEZef78OXr16oUzZ87gwIEDcHV1FTukKik6Ohrdu3dH586dsXfvXhYERGWAxQBRGXj+/Dl69uyJP//8E2FhYejatavYIVVpR48eRffu3dGxY0fs3bsX1atXFzskokqNxQDRO3r+/Dl69OiBs2fPIiwsDC4uLmKHpBOOHTsGHx8fdOjQAfv27WNBQPQOWAwQvYNnz56hR48eOHfuHA4ePAhnZ2exQ9IpMTEx8PHxQdu2bbF//36Ym5uLHRJRpcRigKiUnj17hu7duyM2NhYHDx5Ely5dxA5JJx0/fhze3t5wdHTEgQMHWBAQlQKLAaJSePr0KXx8fHDx4kUcOnQITk5OYoek006cOAFvb2+0bt0aBw4cgIWFhdghEVUqLAaI3tI///wDHx8fxMXF4dChQ+jcubPYIRGAkydPolu3bmjVqhXCwsJYEBC9BRYDRG/hn3/+gbe3N/7++2+Eh4ejU6dOYodErzh16hS6deuGFi1aICwsDO+9957YIRFVCiwGiEooIyMD3t7euHz5MsLDw9GxY0exQyINzpw5Ay8vLzg4OODgwYMsCIhKgMUAUQlkZGSgW7duuHr1Kg4fPoz27duLHRK9wZ9//glPT0/Y29vj4MGDsLS0FDskIq3GYoCoGE+ePEG3bt1w7do1HDlyBO3atRM7JCqBv/76C56enmjatCkOHToEKysrsUMi0losBoje4MmTJ/Dy8kJiYiKOHDmCtm3bih0SvYWzZ8/C09MTtra2CA8PZ0FAVAR9sQMg0lbp6enw9PTE9evXERERwUKgEmrXrh2OHDmCxMREeHp6Ij09XeyQiLQSZwaINEhLS4OnpyeSkpIQERGBNm3aiB0SvYPz58/Dw8MDjRo1wuHDhyGRSMQOiUirsBggek1aWho8PDyQnJyMiIgItG7dWuyQqAzExsbC3d0dNjY2OHz4MGrUqCF2SERagx8TEL3i8ePHcHd3x+3btxEZGclCoApp06YNIiMjcevWLXh4eCAtLU3skIi0BosBov8vNTUV7u7uuHPnDiIjI9GqVSuxQ6Iy1rp1a0RGRuL27dtwd3fH48ePxQ6JSCvwYwIi/FsI3Lt3D5GRkWjRooXYIVE5unTpEtzc3FCvXj0cOXIE1tbWYodEJCrODJDOe/ToEdzc3HD//n1ERUWxENABLVu2RFRUFFJSUuDu7o7U1FSxQyISFYsB0mmPHj2Cu7s7Hjx4gKioKDRv3lzskKiCtGjRAlFRUbh//z7c3Nzw6NEjsUMiEg2LAdJZDx8+hJubGx4+fIioqCg4ODiIHRJVsObNmyMqKqrAe4FIF/GaAdJJDx48gJubG9LS0hAVFQU7OzuxQyIRXb58GXK5HNbW1oiMjEStWrXEDomoQnFmgHQOCwF6nb29PRQKBR4/fgy5XI4HDx6IHRJRhWIxQDrl/v37kMvlSE9Ph0KhYCFAanZ2dlAoFEhPT4dcLsf9+/fFDomowrAYIJ1x7949yOVyZGRkQKFQoFmzZmKHRFqmWbNmUCgUyMjIgFwux71798QOiahC8JoB0gmqQuDZs2eIiopCkyZNxA6JtFhCQgLkcjnMzc0RFRWFOnXqiB0SUbnizABVeSkpKZDJZHj+/DkUCgULASpWkyZNoFAo8OzZM8hkMqSkpIgdElG5YjFAVdrdu3chk8nw8uVLKBQK2Nraih0SVRK2trZQKBR48eIFZDIZ7t69K3ZIROWGxQBVWXfu3IFMJoNSqYRCocBHH30kdkhUyagKgszMTMjlchYEVGWxGKAq6fbt25DJZMjKyoJCoUDjxo3FDokqqY8++ggKhQJKpRIymQx37twROySiMsdigKqc5ORkyGQy5OTkQKFQoFGjRmKHRJVc48aNoVAokJ2dDZlMhtu3b4sdElGZ4rcJqEpRFQKCICAqKgo2NjZih0RVSFJSEmQyGQwMDBAVFYUGDRqIHRJRmeDMAFUZt27dgkwmAwAoFAoWAlTmbGxsEB0djby8PMhkMty6dUvskIjKBIsBqhJUZ2xAfiHQsGFDcQOiKqthw4ZQKBQAwIKAqgwWA1TpqQoBfX19REdHc+qWyp2qINDX14dMJkNSUpLYIRG9ExYDVKndvHkTUqkUhoaGiI6ORv369cUOiXREgwYNoFAoYGBgAJlMhps3b4odElGpsRigSuvGjRuQSqUwMjKCQqHAhx9+KHZIpGPq168PhUIBQ0NDyGQy3LhxQ+yQiEqFxQBVStevX4dMJoOJiQkLARLVhx9+iOjoaBgbG7MgoEqLxQBVOomJiZDJZDA1NYVCoUC9evXEDol0XL169RAVFQUTExNIpVJcv35d7JCI3gqLAapUEhISIJPJUL16dURFRaFu3bpih0QEIL8gUCgUMDMzg1QqRWJiotghEZUYiwGqNFSFgIWFBQsB0kp169aFQqGAubk5ZDIZEhISxA6JqERYDFClcPXqVUilUlhaWvL+8qTV6tSpA4VCAQsLC8hkMly7dk3skIiKxWKAtN7Vq1chl8shkUgQFRWFDz74QOyQiN7ogw8+gEKhgKWlJWQyGa5evSp2SERvxGKAtNqVK1cgk8lQo0YNREZGonbt2mKHRFQitWvXRlRUFCQSCeRyOa5cuSJ2SERFYjFAWuvy5cuQyWSwtrZmIUCVkqogqFGjBuRyOS5fvix2SEQasRggrRQfHw+ZTIZatWohMjIStWrVEjskolKpVasWoqKiYG1tDblcjvj4eLFDIiqExQBpnb///htyuRwffPABIiIi8P7774sdEtE7ef/999VFLQsC0kYsBkgrKJVKAEBcXBzkcjnq1KnDQoCqFFVB8MEHH0Amk+Hvv/8G8O97n0hMLAZIdAqFAhKJBH/99Rfc3NxQr149REREwNraWuzQiMqU6vqXunXrQi6X4+zZs5BIJOpbIhOJRU8QBEHsIEi3jRkzBgqFAk+fPkWDBg1w+PBh1KxZU+ywiMrN48eP4eHhgdu3b8Pc3Bxubm5Ys2aN2GGRDuPMAIlKqVRi586duH//Pt577z04ODiwrztVeYmJiWjevDksLCzw4MED7Ny5E1lZWWKHRTqMxQCJav369Xj27BkyMzNx8+ZNxMfHo1q1amKHRVSujIyMEB8fj6SkJGRmZuLp06dYt26d2GGRDmMxQKJSKpV4//33MXfuXCQmJuLcuXNwdHQUOyyicuXo6Ihz584hISEBc+fOxfvvv88LCUlUvGaAiIhIx3FmgIiISMcZih1AZZGcnIzU1FSxw6ByYm1tjQYNGogdBlUw5rVuYZ4XjcVACSQnJ8Pe3h4vXrwQOxQqJ2ZmZrh8+TIPFDqEea17mOdFYzFQAqmpqXjx4gU2bdoEe3t7scOhMnb58mUMHz4cqampPEjoEOa1bmGevxmLgbdgb2+Ptm3bih0GEZUh5jURLyAkIiLSeSwGiIiIdByLAS124cIF+Pj4qH/38PDAtWvXsG7dOtja2iI4OBgAEBwcjC5dusDV1RXXrl0DAGzcuBE2NjYICQkp87hSUlLg6ekJqVSKefPmFVquUCjQvHnzQjcaWrx4MTp16oROnTrhwIEDAIDp06fDyckJnTt3xqZNm8o8ViJtoq05/TY5+6phw4bh/fffx08//aR+7Pbt25BKpZBKpejfv7+6zXJ8fDx8fHwgl8uxePHiMn8N9I4EKtbZs2cFAMLZs2crfN/jx48Xdu7cKWzcuFEICAgQBEEQ1q5dK/z444+CIAhCdna20L59e+HFixdCXFyc0L9/f/Vz586dKwQHBxe7j2fPnr1VTJ9//rlw8OBBQRAEYejQocLff/9dYHl6errw/PlzoXXr1urHcnJyhObNmwu5ubnCkydPhE6dOgmCIAgJCQmCIAiCUqkUHBwchJycnLeKpSyIOb4kHrHGXRtz+m1y9lV3794tELsgCMK3334rbNq0SRAEQZg2bZoQGhoqCIIg9OzZU0hPT3+ruMoS8/zNeAGhlps/fz48PT2hp6eHqKioQssTEhJgZ2cHU1NTNG/eHLdv3y7RdgVBgEKhwNq1a/HkyRPs2bOnxDElJiaiTZs2AIC2bdvi6NGjcHBwUC+3srIq9BwDAwM0aNAASqUS//zzDyQSCQDA1tYWAFCtWjXo6+tDT0+vxHEQVUbamNNvk7Ovqlu3bqHH7O3t8fDhQwBAeno6rK2tcfPmTSiVSowePRrPnj3Df//7X/UxhLQDiwEtJ5FIUKtWLTRo0ADm5uaFlqenp8PS0lL9e15e3hu39+jRI/z222+IjIyEs7MzvvnmGzRp0gRA/jTe+PHjCz1n9+7dqFGjhvr3Fi1aICIiAkOGDEFkZCS6du1aotcik8lgZ2cHpVKJLVu2FFj2008/YcCAAdDX5ydXVLVpY04X5U05W5TOnTvDx8cHy5cvR/369eHk5IRTp07h4sWLuHz5Mp48eYKRI0fi2LFjJdoeVQwWA1ruyJEjqFmzJuLj43Hz5k00atSowHKJRIKMjAz178X9Y3r16lVs3rwZw4cPx6hRo1C/fn31MgcHBygUimJjmjFjBsaNG4e1a9eifv36qFOnTrHPuXbtGsLCwnD9+nWkp6ejR48eOHPmDABg3759OHr0KHbt2lXsdogqO23MaU3elLNvEhAQgJ9++gkeHh6YM2cONm3ahPbt28PR0RESiQQSiYQ3ZdJCLAa0WHZ2NmbPno2QkBAkJSVhypQp2L17d4F1mjRpgitXriAzMxM3btzAhx9++MZturi4ID4+HuHh4Zg2bRr++ecfDBs2DH5+fiU+i6hRowa2bduGvLw8jBgxosAFUUXJzc2FpaUlDA0N8d577+Hly5cAgL/++gs//PADwsLCOCtAVZ625rQmReVscXJzc9UXIlpbWyMjIwNNmjRBeno6lEolnj9/DgMDgxJtiyoOiwEttnTpUgwcOBAffPABPvjgA5ibm+PIkSMF1jE0NMT06dMhl8tRrVo1/P7778VuV19fH97e3vD29kZaWpr6CuaSnkUcPHgQixYtgp6eHiZMmIDatWsDAD799FOsXr0aFy5cwJQpU3D9+nV4eHjg+++/R6dOndC4cWM4OzsjKysLU6dOBQBMmDABL168QI8ePQCUfPqSqDLS1px+m5xVffPBxcUFU6ZMwcGDB5GTk4OEhAQsW7YMM2fOxIQJE2BoaAgTExNs3boVhoaGmDlzJtzd3ZGTk4MFCxa8/R+PypfYVzBWBtp2FeqOHTsER0dHYffu3UWus2HDBqF169bCoUOHKjCyyknbxpcqhjaNO3O6/GnTeGsjzgxUQgMGDMCAAQPeuM6IESMwYsSICoqIiN4Fc5rExg9pKzGZTIYnT56IHUaRfvzxR9jY2KBPnz7qx3JzczF27Fh07doVY8eORW5uLgCgV69ekEql6NSpE6KjowEU3bxEJTQ0FC4uLnBxccGoUaPU2wKApKQkGBsbIzY2tshYiLSNtuf0yJEj4ezsDCcnJ4SHhwPIb1j04YcfQiaTQSaT4dmzZwA0N04SBAHffPMNPDw8IJPJkJqairS0NHTs2BHm5ubqfH1VUlISrK2t1dtPTExUL8vNzYWDg0OBpke//vor3N3dIZPJcOnSpfL7Y1QxLAa0VHFfJyrLbRe3r9LG4ufnh4iIiAKPHThwAFZWVjh27BisrKzUXc127dqF6OhobNu2DXPnzgUArF27Fv7+/oiOjoatrS0OHjxYYFs+Pj6IiYlBTEwM9PT0CnxVaeHChXBxcXljLEQVqSrk9Jw5c3D8+HEcOHAAM2bMUD8+YMAAKBQKKBQKmJubIycnB4GBgYiIiMCKFSswc+ZMAEBISAisrKxw5MgRKBQKWFtbw8LCAgcOHHjjzIiLi4t6+6reJACwadMm2NjYqH8/f/48rl69ioiICCgUCrRs2bJUr1MXsRgopbi4OHTu3BlyuRyfffYZACAqKgqOjo7o27cvpFIpYmNjoVAoMGnSJAD5Fa7qzHTx4sWQyWRo27atujnIvHnzMGrUKPTs2RNnzpzBggULIJVK4ezsjNOnTwMAgoKC0K5dOwwePBjp6elFxqfpua1atcKkSZMwaNCgQvsKCAiAi4sLpFIprly5Umj90qhdu3ahq4ZjYmLg7e0NAOjevTuOHz8OADAyMgIA/PPPP2jRogWA/OYlqrMkVfOSV6meIwgCBEFQf0Xr6tWrMDIyKvAVK02xEL2KOV081T/EJiYmBR4PCQlB165dERgYCKDoxkkhISFISUmBXC7HtGnTIAgCqlWrVii3X3fq1Cl07doVU6ZMQU5ODoD8b2bs2rULAwcOLBBHbm4u3N3dMXbsWGRmZpbqdeoiFgOldOjQIfj7+yMqKgorVqwAkP/9+8OHD2P79u3qDlxFGT9+PBQKBSIjIzF//nz147Vr18a+fftgbm6OuLg4REdHIzQ0FLNnz0Zubi4WLVqE48eP4/fffy+yM1lcXFyh5wLA06dP4e/vj507dxbYl5GREa5fv46YmBgsW7YMAQEBGtdXCQoKUk/ZqX5GjhxZor/bqw1VrKyskJaWBgDIysqCq6srPD090bNnTwD5zUtWrFiB5s2bIzk5GU5OToW2t2rVKtjb2+Px48fqbzUEBgaqXwNRSTGnS57T06dPx5dffgkAaN++Pa5du4aoqChcuHABhw8fLrJx0v3791GzZk1ERUUhIyND4/0OXlenTh0kJiaqZ/7Wrl0LAFi9ejVGjhxZoGvp/fv3kZOTg4iICDRu3Bh//PFHsdunfLyAsJTGjBmD7777Dn5+fvD29sbIkSOhVCrVFa6q1earb1RBENT/HxQUhA0bNkBfX7/AAaBz584A8juHnT59GjKZDADw4sULPHr0CHXr1oWJiQlMTEzQrFkzjbFpei4AWFhYFGgbrNpXQkICOnToACC/u2BKSorG9VX8/Pzg5+dXsj/Ua15tqJKRkaH+GqGRkRGOHj2KpKQk9OrVC97e3hqbl7x+AZW/vz/Gjh2LCRMmYOfOnWjZsiUkEgnq1atXqvhIdzGnS5bTv/32G3JycjBq1CgAKNBFsX///oiNjUXPnj01Nk6ysrKCu7s7AMDT0xN///23+mvFRTE2NoaxsTEAYPDgwVi/fj0yMzOxd+9e7N+/H+vXr1eva2VlhbZt26q3v2bNmhK9JmIxUGqmpqb48ccfIQgC7O3t4efnByMjIzx+/BiWlpa4cOECgPx//O7cuQMg//MslSVLliAuLg5Pnz5VT4sD/yaNnZ0dXFxcsG7dOgD5Z84GBgZISUmBUqlEVlaW+qKc12l67qvbfn1ftra22L59O4D8MxBVv/GimgAFBQVh1apVBR5r0KABNmzY8Ia/WD5nZ2eEh4fD3d0dYWFhcHFxQW5uLgRBUDc3UR1cNDUveZVSqYSxsTH09PRgaWkJMzMznD9/HufOnYO3tzcuXbqEhIQEhIeHo3r16sXGRrqNOV18Tu/btw8HDhxQ9zEA8ot61SxAdHQ0PD09i2yc5OrqitjYWHTq1Annzp1Du3btNMbzqn/++QfvvfcegPyLFZs0aYKbN2/i0aNH8PHxwd27d5GTk4NOnTrB1dUVMTExGDx4MM6dO4ePPvqo2O1TPhYDpRQUFIT169dDEAT4+PjA0NAQgYGB8PT0RMOGDdXJ17JlS2RnZ8PDwwOOjo7q58tkMri4uKBdu3YabxLSqlUrODg4QCqVQl9fH05OTggMDMTUqVPRpUsXNGvWDA0bNtQYW1HPLUq7du3QqFEjODs7Q19fHytXrnzjay/pWcSGDRuwatUqXLt2DR4eHti3bx969OiB0NBQdO3aFU2bNkX37t3x9OlT9O7dG/r6+sjJycHChQsBQGPzEuDf5kbLly9HaGgo8vLy0KRJE/Tu3RsGBgYYPXo0AGD06NGYNGkSqlevrjGW1z/3JN3GnC4+pz/55BM0aNAA7u7uMDIyQnh4OLZu3YrVq1fDyMgIrVu3Ru/evaGnp6excdKYMWMwZswYbNmyBR9++CG+++47APm3co6Pj8eVK1fw8ccfw9/fH5MmTcL8+fNx9OhRzJ07F9WrV0edOnWwbt06mJqa4s8//wSQ3wTpyZMncHJyQl5eHvbu3Qu5XA5zc3MEBQUV+5oon57w6jwXaaSqYM+ePauegiqO6h8i3plL+5VmfKnye9txZ05XbszzN+PMQCV39epV9ZXPKitXrizys0ci0m7MaRIDi4Fyovpsr7w1a9as1HclI6KSY05TVcavFuqQipjelEqlsLKyQkhIiPqxOXPmQCqVokOHDvjll18AANu3b0enTp3g4uKCiRMnqtddunQpunTpAnd3d9y4caPc4yWq6ioi7+Pj4+Hj4wO5XI7FixcDYNfPyobFAJWpLVu2qBuyqMyePRvR0dE4efIkli1bhuzsbHTs2BEnTpxATEwMHj58iFOnTuHBgwfYvXs3jh8/jhUrVhTocEZE2isgIABbtmxBVFSU+u6G7PpZubAY0CKaOqAV1dXMz88P3bt3h7u7O/744w+4u7vDy8sLubm5UCgU8PT0hK+vL9q1a4czZ84U2E9qair69OkDNzc3DBw4EJmZmRr3XRqqK65fpeoUqFQq0bhxY1SrVg02NjbqjoDGxsbQ19dHUlISHBwcoKenh6ZNm+Ls2bOljoOosqjseX/z5k0olUqMHj0aHh4e6vsLsOtn5cJrBrSIqgPaxx9/rO7YNX78eEydOhVPnjyBl5cXfH19AQBNmzbFvHnzMHbsWNy5cwcREREYO3YsTpw4ASC/09ihQ4eQnJyMMWPGICoqSr2fhQsXYvz48fDy8sKyZcuwadMmZGRkFNr3q0aOHInk5OQCj/n7+5e4Ucn48eMRHByM8ePHF3j89OnTuHfvHjp27IjHjx/j7NmzyMzMxPnz59Xf5Saqyip73t+/fx8XL17E5cuX8eTJE4wcObLAfUKocmAxoEU0dUArqqtZ69atAQD16tVDq1at1P+flpYGS0tLODo6Ql9fHzY2NoXughYfH49Tp04hMDAQmZmZ8PX1xX/+859C+35VSRoKvcny5cuxZMkSuLi4YPTo0ahfvz6SkpIwefJk9fUFNWvWxMSJE9GtWzfY29ujU6dO77RPosqgsue9lZUVHB0dIZFIIJFIoFQq3/EvQmJgMaBFNHVAK6qr2astUTW1R71w4QIEQUBycnKhBih2dnbo1asX5HI5gPxuZrm5uYX2bWj479vjXWYGVJ0CTUxMYGZmBlNTU6Snp2Po0KFYs2YNatWqpV532LBhGDZsGM6dO4fNmzeX4K9GVLlV9rxv0qQJ0tPToVQq8fz5c340UEmxGNAimjqgFdfVrCgSiQS+vr5ISUnBsmXLCiybNWsW/P391d2/5s6di8TExEL7flVJZwaGDx+OEydOIDg4GBcvXsScOXPg7++PW7duISsrC35+frC2tkZAQADu3LmDcePGAcj/xoGbmxuGDBmChw8fok6dOuqbxRBVZZU97w0NDTFz5ky4u7sjJycHCxYsUD+XXT8rD3YgLIHK1rlKoVAgJCQEP/30k9ihVAqVbXypbFS1cWfev1lVG++yxm8TEBER6Th+TFAFqe5HTkS6g3lP74IzA0RERDqOxUAVIMZd1KZNmwZXV1f07NkTqampBZZdunRJfZbSqlUr9OvXr8ByLy8vdZdChUKBDz/8UL3+s2fPKuolEGmtisjp6OhodO7cGa6urhg6dCiys7MBAFOnTkXdunULdBL9+eef0alTJzg5OeGHH34otK3Q0FC4uLjAxcUFo0aNQm5uLgDN7cnZplg7sRigt3b27Fncvn0bR48exZdfflno4NCyZUsoFAooFAr07dsXffv2VS+Ljo4udMXygAED1Oubm5tXyGsg0nW2trZQKBQ4evQobGxssGvXLgDAV199VehrvT169MDp06dx4sQJ7NmzB/fu3Suw3MfHBzExMYiJiYGenp666ZCm9uRsU6ydWAxoqS+++AIxMTEAgL/++gtjx47FgwcPIJfL4erqil69ehVq7jF69Gh1K9B58+apq/EFCxZAKpXC2dkZp0+ffufYEhMT1Wcubdu2xdGjR4tcNzQ0FL1791b/vnTpUkyYMKHAOiEhIejatSsCAwPfOTYibaVtOV2vXj31V/1ULcEBoE6dOgV6GADARx99BCC/t0G1atXU66qoWo4LggBBENCoUSMAmtuTs02xdmIxoKWGDh2KrVu3Asivrv38/CCRSBAeHo6jR4+iZcuW2Lt3b7HbiYuLQ1xcHKKjoxEaGorZs2cXWsfLy0s9Ta/6iYyMLHKbzZs3h0KhQF5eHg4fPoz09HSN6124cAGNGjXCe++9BwAICwtDly5dUL16dfU67du3x7Vr1xAVFYULFy7g8OHDxb4mospIW3P6xo0bOHjwYImm7Xfv3g0bGxvUrl270LJVq1bB3t4ejx8/1rictBu/TaClunTpgsmTJyM7OxsxMTFYvHgxHj58iHHjxiEtLQ0PHjwolHCaOpLFx8fj9OnT6quMX7x4UWhf4eHhxcazdOlSBAcHw83NDXPmzIGnpyfc3NzQuXNnNGzYUONztm3bhsGDB6t///XXX7Fjx44CN1B59WOB/v37IzY2Fp6ensXGQ1TZaFtOA0B6ejpGjBiB9evXq8/ui3L27Fn88ssv2L9/v8bl/v7+GDt2LCZMmICdO3di+PDhJYqBtAOLAS3m5uaGb7/9Fs7OztDX18fmzZvh5eWFcePGYcaMGXi9X5REIsGdO3fQpk0bnD9/Ho6OjrCzs4OLiwvWrVsHIL8F6eu8vLwKPa7qCKgyceJETJw4Uf37V199ha+++gohISGoU6eOxvj37duHWbNmAci/gUpKSgr69euHtLQ0PHz4EDKZDHK5HJaWlgDyrydgIUBVmTbltFKpxKBBgxAYGIhmzZq9Me6kpCSMHz8eoaGhMDMzK7Rc1XJcT08PlpaWGtch7cZiQIv5+fnB0dERp06dAgC4u7tjxIgRCAsLg4WFRaGziNGjR2PEiBFYs2YNjI2NAQCtWrWCg4MDpFIp9PX14eTkVOiz+ZKeRbxKJpPBwMAAzZo1U3c8W7hwIQYMGABbW1v89ddfaNasmfojAQsLC5w/fx7Av53S+vTpg5UrV2L16tUwMjJC69atC1xfQFTVaFNOr1mzBrGxsZg7dy6Af+85sHjxYmzduhWPHj3C7du3sWvXLgQEBCAtLQ1DhgwBkH/jMQcHB3z66adYvXo1li9fjtDQUOTl5aFJkybqPNbUnpxtirUT2xGXANtYVm0cX93EcdctHO834wWEREREOo7FABERkY5jMUBERKTjeAHhW7h8+bLYIVA54LjqNo6/buA4vxmLgRKwtraGmZkZvzdbhZmZmcHa2lrsMKgCMa91D/O8aPw2QQklJycXuiFPZbB8+XJs2rQJ+/btQ40aNcplH48fP0avXr0wYsQIjBs3rlz2Ud6sra3RoEEDscOgClZZ81qF+f12mOdFYzFQhaWlpcHGxgafffYZFi9eXK77+vrrr7Fq1SokJSWV20GJiP7F/KayxAsIq7AlS5YgNzcXU6dOLfd9TZs2Dbm5ufjf//5X7vsiIuY3lS0WA1VUamoqfv75Z3z++eeoVatWue+vVq1amDBhApYuXYrHjx+X+/6IdBnzm8oai4EqasmSJRAEoULOGlSmTp0KQRCwZMmSCtsnkS5iflNZYzFQBT169Ai//PILvvjiiwq9cvb999/H559/jl9++aVSX5RFpM2Y31QeWAxUQUuWLIGenh6mTJlS4fv++uuvefZAVI6Y31QeWAxUMY8ePcKvv/5a4WcNKtbW1vjiiy/wyy+/4NGjRxW+f6KqjPlN5YXFQBWzePFi6Ovri3LWoPL1119DT08P//3vf0WLgagqYn5TeWExUIU8fPgQy5Ytw5dffomaNWuKFkfNmjXx5Zdf4tdff8XDhw9Fi4OoKmF+U3liMVCF/PDDDzA0NMRXX30ldiiYMmUKDAwMyr0ZCpGuYH5TeWIxUEXcv38fy5cvx8SJE7WiQ1iNGjUwceJELFu2DA8ePBA7HKJKjflN5Y3FQBWxePFiVKtWDZMnTxY7FLXJkyejWrVqPHsgekfMbypvLAaqgPv372PFihWYNGkSJBKJ2OGoqc4eli9fjvv374sdDlGlxPymisBioApYtGgRjIyMtOqsQWXy5MkwMjLCDz/8IHYoRJUS85sqAouBSu7evXv47bffMHnyZFhZWYkdTiESiQSTJk3CihUrcO/ePbHDIapUmN9UUVgMVHILFy6EiYkJJk6cKHYoRZo0aRKMjY2xaNEisUMhqlSY31RRWAxUYnfv3sXKlSu19qxBxcrKCpMnT8Zvv/2GlJQUscMhqhSY31SRWAxUYosWLYKpqalWnzWoTJo0Caampjx7ICoh5jdVJBYDldTdu3exatUqTJkyBZaWlmKHUyxLS0t89dVXWLlyJe7evSt2OERajflNFY3FQCW1YMECVK9eHV9++aXYoZTYxIkTYWZmhoULF4odCpFWY35TRWMxUAndvn0bv//+O6ZMmYL33ntP7HBK7L333sOUKVOwatUq3LlzR+xwiLQS85vEoCcIgiB2EPR2xo8fj23btiEpKQkWFhZih/NW/vnnHzRq1AhDhgzBsmXLxA6HSOswv0kMnBmoZJKTk7F69Wp8/fXXle5AAfx79rB69Wrcvn1b7HCItArzm8TCmYFK5j//+Q927tyJmzdvVsqDBQA8ffoUjRo1wsCBA7FixQqxwyHSGsxvEgtnBiqRW7duYc2aNZg6dWqlPVAAgIWFBb7++mv88ccfSE5OFjscIq3A/CYxcWagEvnss8+we/du3Lx5E+bm5mKH806ePXuGRo0aoX///vjtt9/EDodIdMxvEhNnBiqJpKQkrFmzBtOmTav0BwoAMDc3x9SpU7FmzRrcunVL7HCIRMX8JrFxZqCSGDt2LEJDQ3Hz5k1Ur15d7HDKxPPnz2FjY4O+ffti1apVYodDJBrmN4mNMwOVwM2bN7Fu3TpMmzatyhwoAKB69eqYNm0a1q5di5s3b4odDpEomN+kDTgzUAl88skn2LdvH27cuFGlDhZA/tlD48aN0atXL6xevVrscIgqHPObtAFnBrTcjRs3sH79egQEBFS5AwXw79nD+vXrcePGDbHDIapQzG/SFpwZ0HIff/wxDhw4gBs3bsDMzEzscMrFixcv0LhxY/To0QN//PGH2OEQVRjmN2kLzgxoscTERGzYsAHTp0+vsgcKADAzM0NAQADWr1+P69evix0OUYVgfpM24cyAFhs9ejQOHTqEGzduwNTUVOxwytXLly/RuHFjeHt7Y+3atWKHQ1TumN+kTTgzoKUSEhKwceNGTJ8+vcofKADA1NQUAQEB2LhxIxITE8UOh6hcMb9J23BmQEuNHDkSR44cwfXr13XiYAHknz189NFH8PT0xPr168UOh6jcML+Z39qGMwNa6Nq1a9i8eTNmzJihMwcKIP/sYfr06di0aRMSEhLEDoeoXDC/md/aiDMDWmjEiBGIiopCYmIiTExMxA6nQmVmZuKjjz6Cu7s7NmzYIHY4RGWO+c381kacGdAyV69eRVBQEGbMmKFzBwoAMDExwYwZM7B582ZcvXpV7HCIyhTzm/mtrTgzoGWGDRuG6OhoXL9+HcbGxmKHI4rMzEzY2tpCJpNh06ZNYodDVGaY38xvbcWZAS1y+fJlbNmyBTNnztTZAwXw79nDli1bcOXKFbHDISoTzO98zG/txJkBLTJ06FAcP34cCQkJOn2wAAClUglbW1t07doVQUFBYodD9M6Y3/9ifmsfzgxoifj4eGzbtk3nzxpUjI2NMXPmTGzduhWXL18WOxyid8L8Loj5rX04M6AlhgwZgpMnTyIhIQFGRkZih6MVlEolmjRpAmdnZ2zZskXscIhKjfldGPNbu3BmQAv8/fff2L59O2bNmsUDxSuMjY0xa9YsbNu2DX///bfY4RCVCvNbM+a3duHMgBYYNGgQzpw5g2vXrvFg8ZqsrCw0adIEnTt3xrZt28QOh+itMb+LxvzWHpwZENmlS5ewY8cOnjUUwcjICLNmzcKOHTsQFxcndjhEb4X5/WbMb+3BmQGRDRgwAGfPnsW1a9dQrVo1scPRSllZWWjatCk6dOiAHTt2iB0OUYkxv4vH/NYOnBkQ0cWLF7Fr1y7Mnj2bB4o3MDIywuzZs7Fz505cunRJ7HCISoT5XTLMb+3AmQER9e/fH7Gxsbhy5QoPFsXIzs5Gs2bN0LZtW+zcuVPscIiKxfwuOea3+DgzIJLY2Fjs3r2bZw0lVK1aNcyePRu7du3ChQsXxA6H6I2Y32+H+S0+zgyIpG/fvrh48SLPGt5CdnY27Ozs0Lp1a+zevVvscIiKxPx+e8xvcXFmQATnz59HSEgIvvnmGx4o3oLq7CE4OBixsbFih0OkEfO7dJjf4uLMgAh69+6N+Ph4XL58GYaGhmKHU6nk5OTAzs4OLVq0QEhIiNjhEBXC/C495rd4ODNQwc6dO4c9e/bgm2++4YGiFAwNDfHNN98gNDQU58+fFzscogKY3++G+S0ezgxUMF9fX1y5cgXx8fE8WJRSTk4OHBwcYG9vj9DQULHDIVJjfr875rc4ODNQgf766y/s3bsXc+bM4YHiHajOHvbs2YOzZ8+KHQ4RAOZ3WWF+i4MzAxWoZ8+eSEhIwN9//82DxTtSnT00a9YMe/fuFTscIuZ3GWJ+VzzODFSQM2fOYP/+/TxrKCOGhoaYM2cO9u3bhz///FPscEjHMb/LFvO74nFmoIL06NEDN27cQFxcHAwMDMQOp0rIzc1F8+bNYWtri3379okdDukw5nfZY35XLM4MVIDTp0/jwIEDmDNnDg8UZcjAwABz5szB/v37cebMGbHDIR3F/C4fzO+KxZmBCuDj44Nbt27h0qVLPFiUsdzcXLRs2RI2NjY4cOCA2OGQDmJ+lx/md8XhzEA5O3nyJA4ePIi5c+fyQFEOVGcPYWFhOHXqlNjhkI5hfpcv5nfF4cxAOevWrRvu3LmDixcv8mBRTlRnDw0aNMDBgwfFDod0CPO7/DG/KwZnBsrRiRMnEB4ezrOGcmZgYIC5c+fi0KFDOHnypNjhkI5gflcM5nfF4MxAOfLy8sK9e/dw4cIF6Ouz7ipPeXl5aNWqFerVq4dDhw6JHQ7pAOZ3xWF+lz++g8vJ8ePHcfjwYcydO5cHigqgr6+PuXPnIjw8HCdOnBA7HKrimN8Vi/ld/jgzUE48PDzw6NEjnD9/ngeLCpKXl4c2bdqgdu3aOHz4sNjhUBXG/K54zO/yxXdxOTh27BgiIiJ41lDBVGcPR44cQUxMjNjhUBXF/BYH87t8cWagHLi5ueHx48c8axBBXl4eHB0dYW1tjYiICLHDoSqI+S0e5nf54Tu5jDx69AifffYZIiIiEBUVhXnz5vFAIQLV2UNkZCQiIyPx2Wef4dGjR2KHRZUc81s7ML/LD2cGykhYWBi6d+8OJycnvHjxAmFhYahTp47YYemklJQU+Pj4wNzcHCdOnEBYWBi8vb3FDosqMea39mB+lw+WtmVEVVOdPHkSxsbG+Oijj5CZmSlyVLonMzMTH330EUxMTNRXHbPepXfF/NYOzO/yw2KgjOTl5QHIv/VmQkICtm7dChMTE5Gj0j0mJibYtm0bEhIS1LeS5cGC3hXzWzswv8sPi4Eycu/ePQCAra0tYmNj4evrK3JEusvX1xfnz5/HRx99BAC4e/euyBFRZcf81h7M7/LBawbKyLNnz7Bo0SJ88803MDIyEjscApCVlYXvvvsOAQEBMDc3FzscqsSY39qH+V22WAwQERHpOH5MQEREpOMM3+XJycnJSE1NLatYSCTW1tZo0KCBxmUc46pNNfYcZ93CcdcNbzq2FyKU0q1btwQzMzMBAH8q+Y+ZmZlw69YtjrEO/piZmQknTpzgOOvYD8ddN36KOrZrUuqZgdTUVLx48QKbNm2Cvb19aTdDIrt8+TKGDx+O1NTUQhUkx7hqU439jRs3OM46hOOuG950bNfknT4mAAB7e3u0bdv2XTdDWoxjrBs4zrqJ404ALyAkIiLSeVWqGJDJZHjy5InYYRTLy8sLkyZNAgBs374dnTp1gouLCyZOnKheZ9iwYXj//ffx008/adxGcHAwunTpAldXV1y7dq3AMn9/f/Tp0wcAEB0djc6dO8PV1RVDhw5FdnZ2ebykCqXt4zxy5Eg4OzvDyckJ4eHhAIArV66gTZs2MDExKRC7pnG8ffs2pFIppFIp+vfvj6ysLABAfHw8fHx8IJfLsXjx4kL7NTc3h0wmg0wmK3SL11ffc5VBZRxjANi1axfc3Nwgl8tx+PBhAJpzed68eWjRogVkMhk+/fRT9eOzZs1C165dMWDAALx48ULjvk+cOAE9PT08efIEeXl56NatG1xcXODi4oLz588DAEJDQ9WPjRo1Crm5ueXwVygf2j72P/74I2xsbNTHWAA4c+YMunTpAqlUil69euHZs2fqZU+fPoW1tTVCQkIAACtWrEDXrl3RuXNnTJ8+vdD2t27dimbNmqFNmzYFHo+JiYGHhwfkcjk2btwIAHB3d4dMJoOrqytq1Kjxbi+stBcQnj17VgAgnD17trSbeCu5ubnFriOVSoX09PR33nZx+ypJLEVRKBSCj4+PMHHiREEQBOHmzZtCTk6OIAiCMGTIEOHkyZOCIAjC3bt3hbVr1wo//vhjoW1kZ2cL7du3F168eCHExcUJ/fv3Vy9LTEwUfH19hd69ewuCIAh37twRXr58KQiCIEyfPl3YsmVLgW29aRwreowFoWqMc0JCgiAIgpCWlia0bdtWEARBePbsmfDkyZMCsRc1jt9++62wadMmQRAEYdq0aUJoaKggCILQs2fPN77u1q1ba3z89fecimp8N23axFx+S5rG+N69e8KgQYMKbVNTLs+dO1cIDg4usN7FixeFAQMGCIIgCL/++qvw888/a9z3wIEDhfbt2wvp6elCXl6ecP36dUEQBOHKlStCt27dBEEQBKVSqV5/1KhRQlRUlPp3scZdEKrG2N+/f19ITExUH2MFQRCysrLU/z937lxh7dq16t/nzZsneHt7q8f71bGRSqXCzZs3C2z/0aNHglKpLJDPL1++FHr06CFkZmZqjCkyMlL4+OOPCzz2tsfvMpsZiIuLQ+fOnSGXy/HZZ58BAKKiouDo6Ii+fftCKpUiNjYWCoVCfYaSlJSkrq4WL14MmUyGtm3bYs+ePQDyq+dRo0ahZ8+eOHPmDBYsWACpVApnZ2ecPn0aABAUFIR27dph8ODBSE9PLzI+Tc9t1aoVJk2ahEGDBhXaV0BAAFxcXCCVSnHlypVC65fW0qVLMWHCBPXvNjY2MDAwAAAYGxurb4tat27dIreRkJAAOzs7mJqaonnz5rh9+7Z6WWBgIKZOnar+vV69euoe6q9uv7Q4zsWztbUFgAK966tXrw5LS8sC6xU1jvb29uozo/T0dFhbW+PmzZtQKpUYPXo0PDw8EBsbW2i/SUlJcHV1xccff4ynT5+qH3/9PVccjnHxNI1xWFgYTE1N0a1bNwwePBhpaWkAis7lb7/9Fq6urti3bx+A/DM/1d33unfvjuPHjxd6zpEjR9ChQwdUr14dAKCnp4fGjRsDKJjfqi6JgiBAEAQ0atSoRK+LY1+82rVrq4/ZKtWqVVP//8uXL2FnZwcASEtLw9WrV9GpUyf1ctXY5OTkwNLSEtbW1gW2ZW1tXajL5cmTJ2Fqaoo+ffqgZ8+eSEpKKrB827ZtGDx4cKlej1qJSgYNXq86/vvf/wp//PGHIAj/VlydOnUSHj16JGRlZQl2dnbC+fPnhaioqAJnxarq6tmzZ4IgCEJ6errQoUMHQRDyK6ypU6cKgiAIly5dEvz8/ARByK+cPDw8hJycHKFVq1bCy5cvhYyMDEEikWisKDU9VxAEwcbGRvj7778L7evs2bPqs7RLly4Jvr6+hdZ/1ebNmwWpVFrgZ8SIEYXWO3DggLB48eICfwOVU6dOCV5eXgUeK2pm4Pjx48KECRPUv7dv314QBEGIi4sTvvzyywJ/V5Xr168LnTp1KlCVql4r3mJmgONc/DirfPnll8K6desKPPbqWU9R45icnCw0b95ccHBwELp16ybk5eUJJ06cEGrXri2kpaUJN27cEFxcXArtLzU1VRAEQfj555+FmTNnCoLw5vdcUWeIHOPSjXFgYKDQrVs3ITc3VwgKChK+/vpr9Xqv57JqrFJTU4U2bdoIaWlpwvfffy/s2LFDEIT8GQdPT89C++vVq5fw/PnzQmfPeXl5Qt++fYXIyEj1YytXrhSaNWsm9OjRQz07qPqbaBp3QeDYl3TsNR1jQ0NDhdatWwsdOnQQHj9+LAiCIAQEBAh//vlnoZmgb7/9VmjYsKHw6aefCnl5eRr38erMQFBQkNCyZUvh5cuXwokTJ9QzSIKQP8Nob28vZGdnF3j+284MvPO3CVTGjBmD7777Dn5+fvD29sbIkSOhVCrVVY/q8w89Pb1XCxH1/wcFBWHDhg3Q19cvcKbbuXNnAPmfl54+fRoymQwA8OLFCzx69Ah169aFiYkJTExM0KxZM42xaXouAFhYWMDBwaHQvhISEtChQwcAQIsWLZCSkqJxfRU/Pz/4+fkV+zf69ddfsWPHDpw5c6bA40lJSZg8ebL6M6XiSCQSZGRkqH9XnQ0EBgZi8eLF6s+YVdLT0zFixAisX7/+nfuqc5yLH2cA+O2335CTk4NRo0YVuU5R4xgQEICffvoJHh4emDNnDjZt2oT27dvD0dEREokEEokESqWy0PZq1qwJABg8eDDGjBkDoOj33JtwjEs3xlZWVpDL5dDX14enp6f6c11NVGNVs2ZNODk5ITExscD7ISMjo9BnwHv27IFMJoOZmVmh7c2YMQNOTk6Qy+Xqx/z9/TF27FhMmDABO3fuxPDhw4t9TRz7ko29Jr6+vvD19cWiRYuwfPlyfPrpp0hKSkL79u3Vsz8qc+bMwcyZM9G3b18cP34cLi4ub9y2lZUVnJ2dYWJiAicnJ9y6dUu9LDIyEq6uruq7OJZWmRUDpqam+PHHHyEIAuzt7eHn5wcjIyM8fvwYlpaWuHDhAoD8A+CdO3cAQH2xCwAsWbIEcXFxePr0KVq0aKF+XHWAtLOzg4uLC9atWwcg/yYVBgYGSElJgVKpRFZWVqEL6VQ0PffVbb++L1tbW2zfvh1A/rSZapqvqCn2oKAgrFq1qsBjDRo0wIYNG9S/P336FCkpKejXrx/S0tLw8OFDyGQySKVSDB06FGvWrEGtWrU0bv91TZo0wZUrV5CZmYkbN27gww8/BADcvHkTH3/8MV6+fIkrV65g5cqVGD16NAYNGoTAwMAik+xtcJzfPM4AsG/fPhw4cADBwcEat6NS1Djm5uaqD77W1tbIyMhAkyZNkJ6eDqVSiefPnxeapnz+/DlMTExgYGAAhUKBJk2aFPmee/XCJ004xqUbY1dXV8yfPx8AcO7cOfVd9TTJyMiApaUlsrKycPbsWcybNw/Gxsb4/vvv8cknnyAsLAzOzs4FnnPp0iVERkYiPDwcFy9exKhRoxAaGooVK1YgPT0dCxcuVK+rVCphbGwMPT09WFpaaiwgNOHYFz/2mqj+3kD+P9zPnz9HXFwckpOT4e3tjcTEROzZswetWrVCvXr1YGxsDENDQ5ibm5dobDp16oTFixdDEAQkJCSgdu3a6mXbtm0rUaFXnDIrBoKCgrB+/XoIggAfHx8YGhoiMDAQnp6eaNiwoXogWrZsiezsbHh4eMDR0VH9fJlMBhcXF7Rr1w5WVlaFtt+qVSs4ODhAKpVCX18fTk5O6s/Hu3TpgmbNmqFhw4YaYyvquUVp164dGjVqBGdnZ+jr62PlypVvfO0lqSgtLCzUSaNQKBASEoI+ffogICAAd+7cwbhx4wDkV4xubm6YMmUKDh48iJycHCQkJGDZsmVYt24dbG1t4eLigunTp0Mul6NatWr4/fffAeRfZQzkzzRMmjQJn332GVasWIHY2FjMnTsXQP7ZwrtUvxzn4v92n3zyCRo0aAB3d3cYGRkhPDwcDx48wLBhw3DhwgX06dMHkydPRu/evTWO48yZMzFhwgQYGhrCxMQEW7duhaGhIWbOnAl3d3fk5ORgwYIFAICFCxdiwIAByMjIwNixY2Fubg4LCwusXbu2yPdccTjGpRvj5s2bo2nTppBKpTA0NMT69esBQGMuT5kyBfHx8cjNzYW/vz9q1aqFWrVqoXHjxujatSvef/999czCpEmTMH/+fMyaNQuzZs1S/43Xr1+PZ8+e4YsvvkDnzp0hk8nU/3gtX74coaGhyMvLQ5MmTdC7d+9iXxPAsS/J2G/YsAGrVq3CtWvX4OHhgX379iEkJAQrVqyAvr4+JBIJ1q9fDwsLC3h4eADIv26iTZs2aNy4MQICAnD69GlkZ2dDKpWibdu2uH//Pn799VfMnz8f4eHh+OGHH3D9+nV4eHhg9erVsLGxweDBg+Hq6gpBELBixQoAQHZ2Nk6ePKk+dryTEn2YoMHbfh4xatQo4fz586XdHZWTsv42Ace58ijtVeUc48rtXb5NwLGvPES7ZkBbXL16VX0VrMrKlSvLZIqctAfHuerjGOsujn3Fq7BiQPU5T3lr1qwZFApFheyLCuM4V30cY93Fsa+6qlQHwpJ6vbNTeZBKpbCysirwDYE5c+ZAKpWiQ4cO+OWXXwDkXyw2duxYdO3aFWPHjlV3CiuuAyG9m/J+D1y+fBkuLi5wdXVFjx491H0DYmNj4eTkhK5du+I///lPucZABVVE3mvqAqmpYx2VP7GO89euXYOrqyu6dOlS7EXE2kQni4GKsGXLlkLtX2fPno3o6GicPHkSy5YtQ3Z2Ng4cOAArKyscO3YMVlZWOHDgAID85h2aWs5S5fD+++9j//79OHr0KHx9ffHbb78BAH7++Wf897//xbFjx5Ceno6LFy+KHCmVJVtbWygUCigUCvXXxfz8/BARESFyZFQeNB3nZ86ciRUrViAiIgILFixATk6OOMG9Ja0sBjR1wSqqs5Wfnx+6d+8Od3d3/PHHH3B3d4eXlxdyc3OhUCjg6ekJX19ftGvXrtB3rVNTU9GnTx+4ublh4MCByMzM1Ljv0tDUdUz1HX+lUonGjRujWrVqRXYde1MHQl1Q2d8D1tbW6o6Dr3aGc3BwwJMnTyAIAp4/f67ximtdVdnHHNDcBVJTxzqqGuOt6TidlJSE5s2bw9TUFPb29khMTCz19iuSVl5AeOjQIfj7++Pjjz9GXl4eAGD8+PGYOnUqnjx5Ai8vL/j6+gIAmjZtinnz5mHs2LG4c+cOIiIiMHbsWPXX7J4+fYpDhw4hOTkZY8aMQVRUlHo/CxcuxPjx4+Hl5YVly5Zh06ZNyMjIKLTvV40cORLJyckFHnubr+uNHz8ewcHBGD9+PID8hkCqfzSsrKzULUx1XVV5D6SlpWH58uXqGZ9u3bqhd+/emDx5svqrYJSvKoz59evXUbNmTfzyyy9YuHAhvv/++zL9G1UlVWG8NXl1e5XpmK6VxYCmLlhFdbZq3bo1gPwe/K1atVL/f1paGiwtLeHo6Ah9fX3Y2NgUuhNWfHw8Tp06hcDAQGRmZsLX1xf/+c9/Cu37VSVpQPEmy5cvx5IlS+Di4oLRo0cX23VMV1WF94BSqcSQIUPwv//9T91EaNy4cTh48CCaNGmC0aNH49ixY+jatWtp/0xVSlUYc01dIEmzqjDemrza1KgyHdO1shjQ1AWrqM5Wr7bF1NQi88KFCxAEAcnJyYWmZO3s7NCrVy91C8+srCzk5uYW2verbR7fpWJUdakyMTGBmZkZTE1N4ezsjPDwcLi7uyMsLKzYtpS6orK/BwRBwJgxYzB69OgCYyoIAmrUqAE9PT3UrFmzQDtiXVfZx1xTF0gqWmUf76I0bNgQ8fHxaNy4MeLj49U3tdJ2WlkMaOqCVVxnq6JIJBL4+voiJSUFy5YtK7Bs1qxZ8Pf3x3fffQcAmDt3LhITEwvt+1UlrRiHDx+OEydOIDg4GBcvXsScOXPg7++PW7duISsrC35+frC2tkaPHj0QGhqKrl27omnTpujevTsAzV3LdEllfw8cPHgQe/fuRUpKClatWgVfX1989dVX+L//+z/07NkTRkZGqFOnDrp161bi11HVVfYxv3LlSqEukKrnvt6x7tW7Heqqyj7egObjfGBgIMaOHYvs7GwEBAS88z0DKkxFdTcSg6Y7tVFBZd2BUNvwPVA0Me9rX5445m9W1cad463Z2x6/tfLbBERERFRxKsn8Remomn+Q7uJ7QPdwzHULx7tscGaAiIhIx1WJYqAi2k6+TlMbyuLajg4bNkxdxZqamiI9PR3bt29Hp06d4OLigokTJxZY/+nTp7C2ti6wD8pXUWM+ceJEdOnSBU5OTvjzzz8B5F9p7OzsDCcnJ4SHhxd6jqb3wdOnT9GnTx/I5XKMGzdO/V1ktp0uWkWMcVpaGjp27Ahzc3PExsaqHz98+DA6duwIJycnzJ8/HwCQk5ODIUOGwNXVFa6urrhx40aJtgXktx13cHBQj7OquY5MJkOtWrXUDXZ0XUWM+e3bt9GlSxfIZDK4ubkhJSUFADB27Fh1u/gdO3YAgPq28TKZDD179iy0rTctT0pKgrGxsfq9oOn4r02qRDEgBk1tKItrO7p582YoFAqsXbsWzs7OkEgk6NixI06cOIGYmBg8fPgQp06dUq//v//9Dx06dCivl0DFSE5ORnx8PE6cOIE1a9ZgwYIFAPLvMXH8+HEcOHAAM2bMKPQ8Te+DVatWwcfHB1FRUZBIJDh48CAAtp0Wm4WFBQ4cOIABAwYUeHzBggUIDg7GyZMnsX//fmRkZODEiROwsLDA0aNHMWPGDPz6668l2hYAbNq0CTY2Nurfp06dqm5bXKtWLXh6epbL66PC6tati5iYGCgUCowcORKrVq0CACxbtgzR0dGIjIzEt99+q17/888/h0KhwL59+zRur6jlCxcuLPC1Yk3Hf22itcXAF198ob7Rx19//YWxY8fiwYMHkMvlcHV1Ra9evaBUKgs8Z/To0eoqbN68eeoz6gULFkAqlcLZ2RmnT58uk/g0taEsadvRbdu2YdCgQQAAGxsb9XNebVublpaGq1evolOnTmUSb2WgbWNes2ZNmJmZITc3F+np6erGQarvDRf19TBN74PExET1WU/btm1x9OhRALrXdlrbxrhatWrqcX2Vqm10VlYW9PX1YWRkBBsbG2RlZQFAgfdDcdvKzs7Grl27MHDgwELLTp06hVatWsHU1LRU8VcG2jbmBgYG6uPsq/0MVO3iX7x4AQcHB/X6K1euRNeuXfHHH39o3J6m5VevXoWRkRHq169faP1Xj//aRGuLgaFDh2Lr1q0A8s/C/fz8IJFIEB4ejqNHj6Jly5bYu3dvsduJi4tDXFwcoqOjERoaitmzZxdax8vLSz19o/qJjIws89eksmvXLvTr16/AY6dPn8a9e/fQsWNHAMAPP/yAr776qtxi0EbaNuZmZmaoX78+7OzsMGDAAEyePLnA8unTp+PLL78s0Wtr0aKFerbg8OHDWjdFWFG0bYyL0q9fP3h7e6Np06bo0aMHTE1N8cEHH+Dp06dwcHDA7Nmz8emnn5ZoW6tXr8bIkSMLNMtR2bZtGwYPHlyi7VRW2jjmZ86cQadOnfDLL7+gXbt26sf79euHli1bwsfHBwDQp08f/P333zh06BA2bNiAK1euFNhOUcsDAwMREBCg8XVoOv5rA639NkGXLl0wefJkZGdnIyYmBosXL8bDhw8xbtw4pKWl4cGDB6hdu3aB52jqTBUfH4/Tp0+rrzZ98eJFoX1p+tz3dUuXLkVwcDDc3NwwZ86cUr+ua9euoWbNmgXOIJKSkjB58mR19Xv//n0kJSWhffv2RU5NVUXaNuaHDx9GRkYGrl27hsTEREyYMEH9vN9++w05OTkYNWpUiV7bJ598gi+++ALu7u6wt7fXeAapC7RtjIvyxRdf4Ny5c5BIJOjevTuGDRuGQ4cOwcHBAbt370ZkZCRmzJhR5NmiSmZmJvbu3Yv9+/dj/fr1BZYJgoDw8HAsXLiw1HFWBto45h07dsTp06exbds2fP/991i9ejUAYPfu3UhLS0OHDh0wYsQIdeMjMzMz9OjRA5cuXYKdnZ16O5qWK5VKSCQS1KtXr9B+NR3/tYXWFgMA4Obmhm+//RbOzs7Q19fH5s2b4eXlhXHjxmHGjBnqN4mKRCLBnTt30KZNG5w/fx6Ojo6ws7ODi4sL1q1bBwDqab5XeXl5FXp8zpw5cHNzU/8+ceLEQhf4lcbrZwLp6ekYOnQo1qxZg1q1agHIr4CTk5Ph7e2NxMRE7NmzB61atULjxo3fef/aTpvGPDc3FzVr1oSenh5q1Kihvgvdvn37cODAgbe6V7mJiQl+//13AMDkyZN1+t722jTGRTE0NMR7770HQ0NDmJub4+nTp8jNzVUfxK2trUvUSvrmzZt49OgRfHx8cPfuXeTk5KBTp05wcnJCTEwM2rdvD2Nj42K3U9lp05ir2sID+f+Ym5mZFXi8evXqsLCwgIGBATIyMmBpaYm8vDwcO3as0LUdmpafP38e586dg7e3Ny5duoSEhASEh4ejevXqWj0TpNXFgJ+fHxwdHdUX1bm7u2PEiBEICwuDhYVFoWpy9OjRGDFiBNasWaMe7FatWsHBwQFSqRT6+vpwcnJCYGBggeeV5gxCUxtKTW1Hr1y5gpiYGHz++ecAgODg4ALTVgsXLsSdO3cwbtw4APlvXA8PD3h4eADI/7ysTZs2OlEIANo15l5eXti4cSNcXV2RmZmpbmf6ySefoEGDBnB3d4eRkRHCw8MRGxurHuei3geTJk2CgYEB+vbtq75+QBfbTmvTGAOAh4cH4uPjceXKFXz88cfw9/dHQEAAXF1dYWBggPbt26NFixZo2LAhBg8ejJCQEGRlZakvIJw0aRLmz58Pc3NzjdtSfQtl3bp1ePLkCZycnABo72fH5UGbxvzUqVP45ptvYGBgAGNjY/Xsjq+vL5RKJbKysjBr1izo6elhyZIl6m326NEDjo6OAIBPP/0Uq1ev1rjc0dERo0ePVr+OSZMmoXr16gAKH/+1SkW1OiTtVNXbEVPRqlpbWioZjrtuYDtiIiIieissBoiIiHQciwEiIiId984XEF6+fLks4iCRlGT8OMZV0+vjynHWDRx33fDW41raixNu3bolmJmZCQD4U8l/zMzMhFu3bnGMdfDHzMxMOHHiBMdZx3447rrxU9SxXRM9QXjtC55vITk5GampqaV9OmkJa2trNGjQQOMyjnHVphp7jrNu4bjrhjcd21/3TsUAERERVX68gJCIiEjHsRggIiLScSwGiIiIdByLASIiIh3HYoCIiEjHsRggIiLScSwGiIiIdByLASIiIh3HYoCIiEjHsRggIiLScSwGiIiIdByLASIiIh3HYoCIiEjHsRggIiLScSwGiIiIdByLASIiIh3HYoCIiEjHsRggIiLScSwGiIiIdByLASIiIh3HYoCIiEjHsRggIiLScSwGiIiIdByLASIiIh3HYoCIiEjHsRggIiLScSwGiIiIdByLASIiIh3HYoCIiEjHsRggIiLScSwGiIiIdByLASIiIh3HYoCIiEjHsRggIiLScSwGiIiIdByLASIiIh3HYoCIiEjHsRggIiLScSwGiIiIdByLASIiIh3HYoCIiEjHsRggIiLScSwGiIiIdByLASIiIh3HYoCIiEjHsRggIiLScSwGiIiIdByLASIiIh3HYoCIiEjHsRggIiLScf8Pv2lYE59h5TUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "tree.fit(train_set['features'].tolist(), train_set['lambda'])\n",
    "plot_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two - Compute the swaps but _scaled_ to current model's error\n",
    "\n",
    "LambdaMART is an _ensemble_ model. It's not just about the first model, but collecting a series of models where each model makes a gradual improvement on the current model. The technique used is known as [Gradient Boosting]()\n",
    "\n",
    "To build a model that compensates for the current model's error, we scale the next set of dependent vars to predict based on the correctness of the existing model in ranking. In this way, we eliminate where the model currently does a good job (no need to learn these) and leave in places where the model isn't doing a good job (this is where. we want ot learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "      <th>delta</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>213.776822</td>\n",
       "      <td>427.553645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>4.416508</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>48.987136</td>\n",
       "      <td>97.974272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>31.835338</td>\n",
       "      <td>63.670676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>1.292030</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>6.723500</td>\n",
       "      <td>13.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>5.802792</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>41.948006</td>\n",
       "      <td>83.896012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>1385</td>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-9.846078</td>\n",
       "      <td>-19.692155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1386</td>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-9.903461</td>\n",
       "      <td>-19.806921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1387</td>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.205847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-9.957655</td>\n",
       "      <td>-19.915309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1388</td>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-10.008949</td>\n",
       "      <td>-20.017899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1389</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.289065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-10.057598</td>\n",
       "      <td>-20.115197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index        uid  qid   keywords   docId  grade  \\\n",
       "qid                                                       \n",
       "1   0       0     1_7555    1      rambo    7555      4   \n",
       "    1       1     1_1370    1      rambo    1370      3   \n",
       "    2       2     1_1369    1      rambo    1369      3   \n",
       "    3       3    1_13258    1      rambo   13258      2   \n",
       "    4       4     1_1368    1      rambo    1368      4   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1385   40_37079   40  star wars   37079      0   \n",
       "    26   1386  40_126757   40  star wars  126757      0   \n",
       "    27   1387   40_39797   40  star wars   39797      0   \n",
       "    28   1388   40_18112   40  star wars   18112      0   \n",
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  last_prediction  display_rank  discount  \\\n",
       "qid                                                                       \n",
       "1   0   [11.657399, 10.083591]              0.0             0  1.000000   \n",
       "    1    [9.456276, 13.265001]              0.0             1  0.630930   \n",
       "    2    [6.036743, 11.113943]              0.0             2  0.500000   \n",
       "    3          [0.0, 6.869545]              0.0             3  0.430677   \n",
       "    4         [0.0, 11.113943]              0.0             4  0.386853   \n",
       "...                        ...              ...           ...       ...   \n",
       "40  25              [0.0, 0.0]              0.0            25  0.210310   \n",
       "    26              [0.0, 0.0]              0.0            26  0.208015   \n",
       "    27              [0.0, 0.0]              0.0            27  0.205847   \n",
       "    28              [0.0, 0.0]              0.0            28  0.203795   \n",
       "    29              [0.0, 0.0]              0.0             9  0.289065   \n",
       "\n",
       "             gain        dcg      lambda       delta  \n",
       "qid                                                   \n",
       "1   0   15.000000  30.700871  213.776822  427.553645  \n",
       "    1    4.416508  30.700871   48.987136   97.974272  \n",
       "    2    3.500000  30.700871   31.835338   63.670676  \n",
       "    3    1.292030  30.700871    6.723500   13.447000  \n",
       "    4    5.802792  30.700871   41.948006   83.896012  \n",
       "...           ...        ...         ...         ...  \n",
       "40  25   0.000000  30.207651   -9.846078  -19.692155  \n",
       "    26   0.000000  30.207651   -9.903461  -19.806921  \n",
       "    27   0.000000  30.207651   -9.957655  -19.915309  \n",
       "    28   0.000000  30.207651  -10.008949  -20.017899  \n",
       "    29   0.000000  30.207651  -10.057598  -20.115197  \n",
       "\n",
       "[1390 rows x 14 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "judgments['last_prediction'] = tree.predict(judgments['features'].tolist()) * learning_rate\n",
    "\n",
    "def compute_swaps_scaled(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Important - stable sort. Otherwise DCG swaps get kind of wonky due to position discounts\n",
    "    query_judgments = query_judgments.sort_values('last_prediction', ascending=False, kind='stable').reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    best_dcg = query_judgments.loc[0, 'dcg']\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    query_judgments['delta'] = 0.0\n",
    "    \n",
    "    for better in range(0,len(query_judgments)):\n",
    "        for worse in range(better+1,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                break\n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                swap_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                dcg_after_swap = metric(swap_judgments, at=at)\n",
    "\n",
    "                delta = abs(best_dcg - dcg_after_swap)\n",
    "\n",
    "                if delta > 0.0:\n",
    "                    \n",
    "                    # --------------\n",
    "                    # NEW!\n",
    "                    model_score_diff = query_judgments.loc[better, 'last_prediction'] - query_judgments.loc[worse, 'last_prediction']\n",
    "                    rho = 1.0 / (1.0 + exp(model_score_diff))    \n",
    "                    # --------------\n",
    "                    # rho works as follows\n",
    "                    # \n",
    "                    # model ranks                    rho\n",
    "                    # better higher than worse       approaches 0      <-- model currently doing well!\n",
    "                    # better same as worse.          0.5  \n",
    "                    # worse higher than better       approaches 1      <-- model currently doing poorly!\n",
    "                    # \n",
    "                    query_judgments.loc[better, 'delta'] += delta\n",
    "                    \n",
    "                    # Use rho to scale the lambdas\n",
    "                    query_judgments.loc[better, 'lambda'] += delta * rho\n",
    "        \n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta * rho\n",
    "                    query_judgments.loc[worse, 'delta'] -= delta\n",
    "\n",
    "    return query_judgments\n",
    "\n",
    "judgments = judgments_to_dataframe(ftr_logger.logged, unnest=False)\n",
    "judgments['last_prediction'] = 0.0\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps_scaled, axis=1)\n",
    "#\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero in on 2 swapped by each result worse than it in query `ramba`\n",
    "\n",
    "```\n",
    "better_grade worse_grade, model_score_diffs, rho,                 dcg_delta\n",
    "2 1                       0.758706128029972  0.31892724571177816  0.02502724555344038\n",
    "2 1                       0.981162516523929  0.2726611758805124   0.04377062727053094\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.11698017724693699\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.14090604137532914\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.08043118677314176\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.17784892734690594\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.19254512210920538\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.20543079947538878\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.21685570619866112\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.22708156316293504\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.23630863576764227\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.24469312448475122\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.2523588995024131\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.25940563061320177\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.2659145510893115\n",
    "...\n",
    "2 0                       1.142439045359345  0.24187283082372052 0.34214193097965406\n",
    "```\n",
    "\n",
    "Summing all the model score diffs, we see those are rather high. This results in a high-ish rho between (for each value here 0.25-0.31). So each dcg_delta is added to the model.\n",
    "\n",
    "What's the intuition here? The model hasn't entirely nailed this example, the model feels there's more 'dcg_delta' to learn to push it away from those less relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zooming out to more of `rambo`\n",
    "\n",
    "We see a similar pattern in results with mediocre grades (2 and 3) where the resulting rho-scaled lambda's are higher than you might expect. The model's happy with the position of 0, but the ranking of other results could be separated more. The model diff should be higher when compared to the dcg diff to push the middling results away from the irrelevant result.\n",
    "\n",
    "So the next tree learns these lambdas using the resulting features moreso than other results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>grade</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>delta</th>\n",
       "      <th>lambda</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>427.553645</td>\n",
       "      <td>213.776822</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rambo</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.868699</td>\n",
       "      <td>-9.934349</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rambo</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.149295</td>\n",
       "      <td>-10.074647</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rambo</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.276314</td>\n",
       "      <td>-10.138157</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rambo</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.395685</td>\n",
       "      <td>-10.197842</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rambo</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.508155</td>\n",
       "      <td>-10.254078</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rambo</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.336529</td>\n",
       "      <td>-10.168264</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rambo</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.432963</td>\n",
       "      <td>-10.216481</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rambo</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.524423</td>\n",
       "      <td>-10.262211</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rambo</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.611330</td>\n",
       "      <td>-10.305665</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rambo</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.694056</td>\n",
       "      <td>-10.347028</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>rambo</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.069351</td>\n",
       "      <td>-10.534675</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>rambo</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.147879</td>\n",
       "      <td>-10.573939</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>rambo</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.222977</td>\n",
       "      <td>-10.611488</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>rambo</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.294893</td>\n",
       "      <td>-10.647447</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>rambo</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.363851</td>\n",
       "      <td>-10.681926</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>rambo</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.430053</td>\n",
       "      <td>-10.715026</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>rambo</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.493681</td>\n",
       "      <td>-10.746841</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>rambo</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.554902</td>\n",
       "      <td>-10.777451</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rambo</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.013760</td>\n",
       "      <td>-10.006880</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rambo</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.712923</td>\n",
       "      <td>-9.856462</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.974272</td>\n",
       "      <td>48.987136</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rambo</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.545028</td>\n",
       "      <td>-9.772514</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rambo</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.670676</td>\n",
       "      <td>31.835338</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rambo</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.447000</td>\n",
       "      <td>6.723500</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rambo</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.896012</td>\n",
       "      <td>41.948006</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rambo</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.400912</td>\n",
       "      <td>-4.200456</td>\n",
       "      <td>[0.0, 7.8627386]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rambo</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.024956</td>\n",
       "      <td>-5.012478</td>\n",
       "      <td>[0.0, 4.563677]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rambo</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.243092</td>\n",
       "      <td>-7.621546</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rambo</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.950401</td>\n",
       "      <td>-7.975200</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rambo</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-16.536694</td>\n",
       "      <td>-8.268347</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rambo</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.032666</td>\n",
       "      <td>-8.516333</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rambo</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.459201</td>\n",
       "      <td>-8.729601</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rambo</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.831043</td>\n",
       "      <td>-8.915522</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rambo</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.158927</td>\n",
       "      <td>-9.079464</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rambo</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.450871</td>\n",
       "      <td>-9.225435</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rambo</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.712994</td>\n",
       "      <td>-9.356497</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rambo</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.950060</td>\n",
       "      <td>-9.475030</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rambo</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.165834</td>\n",
       "      <td>-9.582917</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rambo</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.363338</td>\n",
       "      <td>-9.681669</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>rambo</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.613868</td>\n",
       "      <td>-10.806934</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keywords  display_rank  grade  last_prediction       delta      lambda  \\\n",
       "0     rambo             0      4              0.0  427.553645  213.776822   \n",
       "21    rambo            21      0              0.0  -19.868699   -9.934349   \n",
       "23    rambo            23      0              0.0  -20.149295  -10.074647   \n",
       "24    rambo            24      0              0.0  -20.276314  -10.138157   \n",
       "25    rambo            25      0              0.0  -20.395685  -10.197842   \n",
       "26    rambo            26      0              0.0  -20.508155  -10.254078   \n",
       "27    rambo            27      1              0.0  -20.336529  -10.168264   \n",
       "28    rambo            28      1              0.0  -20.432963  -10.216481   \n",
       "29    rambo            29      1              0.0  -20.524423  -10.262211   \n",
       "30    rambo            30      1              0.0  -20.611330  -10.305665   \n",
       "31    rambo            31      1              0.0  -20.694056  -10.347028   \n",
       "32    rambo            32      0              0.0  -21.069351  -10.534675   \n",
       "33    rambo            33      0              0.0  -21.147879  -10.573939   \n",
       "34    rambo            34      0              0.0  -21.222977  -10.611488   \n",
       "35    rambo            35      0              0.0  -21.294893  -10.647447   \n",
       "36    rambo            36      0              0.0  -21.363851  -10.681926   \n",
       "37    rambo            37      0              0.0  -21.430053  -10.715026   \n",
       "38    rambo            38      0              0.0  -21.493681  -10.746841   \n",
       "39    rambo            39      0              0.0  -21.554902  -10.777451   \n",
       "22    rambo            22      0              0.0  -20.013760  -10.006880   \n",
       "20    rambo            20      0              0.0  -19.712923   -9.856462   \n",
       "1     rambo             1      3              0.0   97.974272   48.987136   \n",
       "19    rambo            19      0              0.0  -19.545028   -9.772514   \n",
       "2     rambo             2      3              0.0   63.670676   31.835338   \n",
       "3     rambo             3      2              0.0   13.447000    6.723500   \n",
       "4     rambo             4      4              0.0   83.896012   41.948006   \n",
       "5     rambo             5      1              0.0   -8.400912   -4.200456   \n",
       "6     rambo            40      1              0.0  -10.024956   -5.012478   \n",
       "7     rambo             7      0              0.0  -15.243092   -7.621546   \n",
       "8     rambo             8      0              0.0  -15.950401   -7.975200   \n",
       "9     rambo             9      0              0.0  -16.536694   -8.268347   \n",
       "10    rambo            10      0              0.0  -17.032666   -8.516333   \n",
       "11    rambo            11      0              0.0  -17.459201   -8.729601   \n",
       "12    rambo            12      0              0.0  -17.831043   -8.915522   \n",
       "13    rambo            13      0              0.0  -18.158927   -9.079464   \n",
       "14    rambo            14      0              0.0  -18.450871   -9.225435   \n",
       "15    rambo            15      0              0.0  -18.712994   -9.356497   \n",
       "16    rambo            16      0              0.0  -18.950060   -9.475030   \n",
       "17    rambo            17      0              0.0  -19.165834   -9.582917   \n",
       "18    rambo            18      0              0.0  -19.363338   -9.681669   \n",
       "40    rambo             6      0              0.0  -21.613868  -10.806934   \n",
       "\n",
       "                  features  \n",
       "0   [11.657399, 10.083591]  \n",
       "21              [0.0, 0.0]  \n",
       "23              [0.0, 0.0]  \n",
       "24              [0.0, 0.0]  \n",
       "25              [0.0, 0.0]  \n",
       "26              [0.0, 0.0]  \n",
       "27              [0.0, 0.0]  \n",
       "28              [0.0, 0.0]  \n",
       "29              [0.0, 0.0]  \n",
       "30              [0.0, 0.0]  \n",
       "31              [0.0, 0.0]  \n",
       "32              [0.0, 0.0]  \n",
       "33              [0.0, 0.0]  \n",
       "34              [0.0, 0.0]  \n",
       "35              [0.0, 0.0]  \n",
       "36              [0.0, 0.0]  \n",
       "37              [0.0, 0.0]  \n",
       "38              [0.0, 0.0]  \n",
       "39              [0.0, 0.0]  \n",
       "22              [0.0, 0.0]  \n",
       "20              [0.0, 0.0]  \n",
       "1    [9.456276, 13.265001]  \n",
       "19              [0.0, 0.0]  \n",
       "2    [6.036743, 11.113943]  \n",
       "3          [0.0, 6.869545]  \n",
       "4         [0.0, 11.113943]  \n",
       "5         [0.0, 7.8627386]  \n",
       "6          [0.0, 4.563677]  \n",
       "7               [0.0, 0.0]  \n",
       "8               [0.0, 0.0]  \n",
       "9               [0.0, 0.0]  \n",
       "10              [0.0, 0.0]  \n",
       "11              [0.0, 0.0]  \n",
       "12              [0.0, 0.0]  \n",
       "13              [0.0, 0.0]  \n",
       "14              [0.0, 0.0]  \n",
       "15              [0.0, 0.0]  \n",
       "16              [0.0, 0.0]  \n",
       "17              [0.0, 0.0]  \n",
       "18              [0.0, 0.0]  \n",
       "40              [0.0, 0.0]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.loc[1, :][['keywords', 'display_rank',  'grade', 'last_prediction', 'delta', 'lambda', 'features']].sort_values('last_prediction', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set\n",
    "\n",
    "tree2 = DecisionTreeRegressor()\n",
    "tree2.fit(train_set['features'].tolist(), train_set['lambda'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More 'oomph' in second tree for the last tree's error cases\n",
    "\n",
    "We see in the following lambdas our next tree learns more about the areas the last model seemed to need correction. \n",
    "\n",
    "The first example is well covered by the first tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([173.48027244])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2.predict([[11.6, 10.08]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second example reflects some of the middling ranked results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.72350002])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2.predict([[0.0, 6.869545]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three - Weigh each leaf's predictions\n",
    "\n",
    "Because we're dealing with trees, each leaf corresponds to a set of examples that have been grouped to this node. In addition to per-swap 'rho' we also care about a per-swap 'weight', referred to in gradient boosting as 'gamma'. \n",
    "\n",
    "Gamma means picking a weight for this sub-model that best predicts the final function.\n",
    "\n",
    "First we group by the paths in the tree to uniquely identify each leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>uid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>train_dcg</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>213.776822</td>\n",
       "      <td>106.888411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>4.416508</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>48.010828</td>\n",
       "      <td>26.458003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>31.382749</td>\n",
       "      <td>18.143963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>1.292030</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>6.460558</td>\n",
       "      <td>7.448315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>5.802792</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>43.639845</td>\n",
       "      <td>21.819923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>40</td>\n",
       "      <td>40_37079</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.846078</td>\n",
       "      <td>4.923039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>40</td>\n",
       "      <td>40_126757</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.903461</td>\n",
       "      <td>4.951730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>40</td>\n",
       "      <td>40_39797</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.205847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.957655</td>\n",
       "      <td>4.978827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>40</td>\n",
       "      <td>40_18112</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-10.008949</td>\n",
       "      <td>5.004475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>40</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.289065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-10.057598</td>\n",
       "      <td>5.028799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      qid        uid   keywords   docId  grade                features  \\\n",
       "0       1     1_7555      rambo    7555      4  [11.657399, 10.083591]   \n",
       "1       1     1_1370      rambo    1370      3   [9.456276, 13.265001]   \n",
       "2       1     1_1369      rambo    1369      3   [6.036743, 11.113943]   \n",
       "3       1    1_13258      rambo   13258      2         [0.0, 6.869545]   \n",
       "4       1     1_1368      rambo    1368      4        [0.0, 11.113943]   \n",
       "...   ...        ...        ...     ...    ...                     ...   \n",
       "1385   40   40_37079  star wars   37079      0              [0.0, 0.0]   \n",
       "1386   40  40_126757  star wars  126757      0              [0.0, 0.0]   \n",
       "1387   40   40_39797  star wars   39797      0              [0.0, 0.0]   \n",
       "1388   40   40_18112  star wars   18112      0              [0.0, 0.0]   \n",
       "1389   40   40_43052  star wars   43052      0              [0.0, 0.0]   \n",
       "\n",
       "      last_prediction  display_rank  discount       gain  train_dcg  \\\n",
       "0                   0             0  1.000000  15.000000  30.700871   \n",
       "1                   0             1  0.630930   4.416508  30.700871   \n",
       "2                   0             2  0.500000   3.500000  30.700871   \n",
       "3                   0             3  0.430677   1.292030  30.700871   \n",
       "4                   0             4  0.386853   5.802792  30.700871   \n",
       "...               ...           ...       ...        ...        ...   \n",
       "1385                0            25  0.210310   0.000000  30.207651   \n",
       "1386                0            26  0.208015   0.000000  30.207651   \n",
       "1387                0            27  0.205847   0.000000  30.207651   \n",
       "1388                0            28  0.203795   0.000000  30.207651   \n",
       "1389                0             9  0.289065   0.000000  30.207651   \n",
       "\n",
       "            dcg      lambda      weight  \n",
       "0     30.552986  213.776822  106.888411  \n",
       "1     30.552986   48.010828   26.458003  \n",
       "2     30.552986   31.382749   18.143963  \n",
       "3     30.552986    6.460558    7.448315  \n",
       "4     30.552986   43.639845   21.819923  \n",
       "...         ...         ...         ...  \n",
       "1385  30.120435   -9.846078    4.923039  \n",
       "1386  30.120435   -9.903461    4.951730  \n",
       "1387  30.120435   -9.957655    4.978827  \n",
       "1388  30.120435  -10.008949    5.004475  \n",
       "1389  30.120435  -10.057598    5.028799  \n",
       "\n",
       "[1390 rows x 14 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_swaps_scaled_with_weights(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort to see ideal ordering\n",
    "    # This isn't strictly nescesarry, but it's helpful to understand the algorithm\n",
    "    query_judgments = query_judgments.sort_values('last_prediction', ascending=False, kind='stable').reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['train_dcg'] = query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    train_dcg = query_judgments.loc[0, 'dcg']\n",
    " \n",
    "    qid = query_judgments.loc[0, 'qid']\n",
    "    keywords = query_judgments.loc[0, 'keywords']\n",
    "\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    query_judgments['weight'] = 0.0\n",
    "\n",
    "    for better in range(0,len(query_judgments)):\n",
    "         for worse in range(0,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                return query_judgments\n",
    "                \n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                query_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "\n",
    "                dcg_after_swap = query_judgments.loc[0, 'dcg']\n",
    "                delta = abs(train_dcg - dcg_after_swap)\n",
    "\n",
    "                if delta != 0.0:\n",
    "                    last_model_score_diff = query_judgments.loc[better, 'last_prediction'] - query_judgments.loc[worse, 'last_prediction']\n",
    "                    rho = 1.0 / (1.0 + exp(last_model_score_diff)) \n",
    "\n",
    "                    assert(delta >= 0.0)\n",
    "                    assert(rho >= 0.0)\n",
    "                   \n",
    "                    query_judgments.loc[better, 'lambda'] += delta * rho\n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta * rho\n",
    "            \n",
    "                    # --------------\n",
    "                    # NEW!\n",
    "                    #  last_model_score_diff        rho         weight\n",
    "                    #      0.0                      0.5         0.25 (max possible value)\n",
    "                    #      100.0                    0.0000      0.0  (max possible value)\n",
    "                    # \n",
    "                    # If the current model has an ambiguous prediction, we include more of the delta in the weight\n",
    "                    # If the current model has a strong prediction, weight approaches 0\n",
    "                    query_judgments.loc[better, 'weight'] += rho * (1.0 - rho) * delta;\n",
    "                    query_judgments.loc[worse, 'weight'] += rho * (1.0 - rho) * delta;\n",
    "                    #\n",
    "                    # These will be used to rescale each decision tree node's predictions\n",
    "                    # If many results in a leaf node have last model score ~ ambiguous\n",
    "                    #     the resulting model will have a high denominator ~ (1 / deltaDCG)\n",
    "                    # If many results in a leaf node have last model score - not ambiguous, positive\n",
    "                    #     the resulting model will have a low denominator\n",
    "                    #\n",
    "                    # Apparently we want to cancel out the deltas if last model was ambiguous?\n",
    "                    # ---------------\n",
    "\n",
    "                    \n",
    "\n",
    "    return query_judgments\n",
    "\n",
    "# Convert to Pandas Dataframe\n",
    "judgments = judgments_to_dataframe(ftr_logger.logged, unnest=False)\n",
    "judgments['last_prediction'] = 0\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps_scaled_with_weights, axis=1)\n",
    "lambdas_per_query = lambdas_per_query.drop('qid', axis=1).reset_index().drop(['level_1', 'index'], axis=1)\n",
    "\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_leaf_nodes=4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set\n",
    "\n",
    "tree3 = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "tree3.fit(train_set['features'].tolist(), train_set['lambda'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label each row with its unique prediction (ie tree path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_paths(tree, X):\n",
    "    paths_as_array = tree.decision_path(X).toarray()\n",
    "    paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "    return paths\n",
    "\n",
    "lambdas_per_query['path'] = tree_paths(tree3, train_set['features'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override outputs using our own weighted average\n",
    "\n",
    "The typical decision tree uses either the [median or mean of the target values](https://scikit-learn.org/stable/modules/tree.html#regression-criteria) classified to a given leaf node as the prediction. However, in the case of lambdaMART, we want to use a weighted average that accounts for how much of the DCG error out there has been accounted for. Thus the psuedoresponses are summed and divided by the remaining error DCG.\n",
    "\n",
    "rho=0, then an example is weighed by `1/0.25*deltaNDCG` as there's a lot of outstanding DCG error left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "1010001    1673.720249\n",
       "1010010    1467.050422\n",
       "1100100     538.598514\n",
       "1101000    4655.114588\n",
       "Name: weight, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.groupby('path')['weight'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "1010001    3334.073882\n",
       "1010010    2787.629391\n",
       "1100100     893.120752\n",
       "1101000   -7014.824025\n",
       "Name: lambda, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.groupby('path')['lambda'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1010001': 1.9920138295288714,\n",
       " '1010010': 1.9001592234365985,\n",
       " '1100100': 1.658230999946508,\n",
       " '1101000': -1.5069068425436136}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "round_predictions.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0.8333333333333334, 'X[0] <= 10.328\\nsquared_error = 883.211\\nsamples = 1390\\nvalue = -0.0'),\n",
       " Text(0.25, 0.5, 'X[0] <= 9.182\\nsquared_error = 179.235\\nsamples = 1324\\nvalue = -4.624'),\n",
       " Text(0.125, 0.16666666666666666, 'squared_error = 62.9\\nsamples = 1301\\nvalue = -5.392'),\n",
       " Text(0.375, 0.16666666666666666, 'squared_error = 4838.036\\nsamples = 23\\nvalue = 38.831'),\n",
       " Text(0.75, 0.5, 'X[0] <= 13.782\\nsquared_error = 5973.405\\nsamples = 66\\nvalue = 92.753'),\n",
       " Text(0.625, 0.16666666666666666, 'squared_error = 5828.45\\nsamples = 39\\nvalue = 71.478'),\n",
       " Text(0.875, 0.16666666666666666, 'squared_error = 4584.565\\nsamples = 27\\nvalue = 123.484')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tree(tree3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap the original tree, and provide new value\n",
    "\n",
    "Instead of directly using the provided decision tree, we want to use our prediction for each leaf. This `OverridenDecisionTree` takes the original tree, looks up the prediction path in the original tree, but uses our values for the predicted variable instead of what's here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.50690684,  1.658231  ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OverridenRegressionTree:\n",
    "    def __init__(self, predictions, tree):\n",
    "        self.predictions = predictions\n",
    "        self.tree = tree\n",
    "        \n",
    "    def predict(self, X, use_original=False):\n",
    "        if use_original:\n",
    "            return self.predict(X)\n",
    "        path = self.tree.decision_path(X).toarray().astype(str)\n",
    "        path = \"\".join(path[0])\n",
    "        \n",
    "        paths_as_array = self.tree.decision_path(X).toarray()\n",
    "        paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "        \n",
    "        predictions = self.predictions[paths]\n",
    "        \n",
    "        # Any NaN predictions is a red flag, debug\n",
    "        if np.any(predictions.isnull()):\n",
    "            print(predictions[predictions.isnull()])\n",
    "            print(pd.DataFrame(X)[predictions.isnull().reset_index(drop=True)])\n",
    "            raise AssertionError(\"No prediction should be NaN\")\n",
    "        return np.array(self.predictions[paths].tolist())\n",
    "        \n",
    "override_tree = OverridenRegressionTree(predictions = round_predictions, tree=tree3)\n",
    "override_tree.predict([[0.0, 6.869545], [10.0, 10.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Four - Putting it all together, from the top!\n",
    "\n",
    "Now we can put together the full lambdamart algorithm that \n",
    "\n",
    "1. Uses pair-wise swaps on our our metric (ie DCG) to generate decision tree predictors (the 'lambdas')\n",
    "2. Focuses in on predicting where current model makes the wrong call when ranking by DCG\n",
    "3. Predicts using a weighted average, weighed by 1 / (remaining DCG)\n",
    "\n",
    "TODOs / known issues\n",
    "* While DCG converges, it does sometimes wander a tad, so there might be more room for improvement\n",
    "* Speeding up the inner loop of the `compute_swaps_scaled_with_weights` that must run for ever query's swaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uid', 'qid', 'keywords', 'docId', 'grade', 'features'], dtype='object')\n",
      "round 0\n",
      "Train DCGs\n",
      "mean    20.03521601393222\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 1\n",
      "Train DCGs\n",
      "mean    20.572514984109958\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 2\n",
      "Train DCGs\n",
      "mean    20.572514984109958\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 3\n",
      "Train DCGs\n",
      "mean    20.572514984109958\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 4\n",
      "Train DCGs\n",
      "mean    20.558839639948378\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 5\n",
      "Train DCGs\n",
      "mean    20.53155110954963\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 6\n",
      "Train DCGs\n",
      "mean    20.53155110954963\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 7\n",
      "Train DCGs\n",
      "mean    20.53155110954963\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 8\n",
      "Train DCGs\n",
      "mean    20.53155110954963\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 9\n",
      "Train DCGs\n",
      "mean    20.52107778379093\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 10\n",
      "Train DCGs\n",
      "mean    20.621960320672194\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 11\n",
      "Train DCGs\n",
      "mean    20.61883906072036\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 12\n",
      "Train DCGs\n",
      "mean    20.61883906072036\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 13\n",
      "Train DCGs\n",
      "mean    20.519913007038184\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 14\n",
      "Train DCGs\n",
      "mean    20.533588351199768\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 15\n",
      "Train DCGs\n",
      "mean    20.563842431095157\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 16\n",
      "Train DCGs\n",
      "mean    20.563842431095157\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 17\n",
      "Train DCGs\n",
      "mean    20.563842431095157\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 18\n",
      "Train DCGs\n",
      "mean    20.56191351702752\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 19\n",
      "Train DCGs\n",
      "mean    20.558476727978864\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 20\n",
      "Train DCGs\n",
      "mean    20.558476727978864\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 21\n",
      "Train DCGs\n",
      "mean    20.55948263670465\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 22\n",
      "Train DCGs\n",
      "mean    20.55948263670465\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 23\n",
      "Train DCGs\n",
      "mean    20.55948263670465\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 24\n",
      "Train DCGs\n",
      "mean    20.550810680749187\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 25\n",
      "Train DCGs\n",
      "mean    20.550810680749187\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 26\n",
      "Train DCGs\n",
      "mean    20.550810680749187\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 27\n",
      "Train DCGs\n",
      "mean    20.56689750665213\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 28\n",
      "Train DCGs\n",
      "mean    20.503985515709587\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 29\n",
      "Train DCGs\n",
      "mean    20.51849950081418\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 30\n",
      "Train DCGs\n",
      "mean    20.64945922172733\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 31\n",
      "Train DCGs\n",
      "mean    20.72515257027024\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 32\n",
      "Train DCGs\n",
      "mean    20.7231021798589\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 33\n",
      "Train DCGs\n",
      "mean    20.7231021798589\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 34\n",
      "Train DCGs\n",
      "mean    20.807701939752825\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 35\n",
      "Train DCGs\n",
      "mean    20.807701939752825\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 36\n",
      "Train DCGs\n",
      "mean    20.84599869297361\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 37\n",
      "Train DCGs\n",
      "mean    20.86404986536433\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 38\n",
      "Train DCGs\n",
      "mean    20.861574312122436\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 39\n",
      "Train DCGs\n",
      "mean    20.82140029663695\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 40\n",
      "Train DCGs\n",
      "mean    20.81392753573653\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 41\n",
      "Train DCGs\n",
      "mean    20.87286373201179\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 42\n",
      "Train DCGs\n",
      "mean    20.887642668254014\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 43\n",
      "Train DCGs\n",
      "mean    20.878882619993473\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 44\n",
      "Train DCGs\n",
      "mean    20.897199204390727\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 45\n",
      "Train DCGs\n",
      "mean    20.897199204390727\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 46\n",
      "Train DCGs\n",
      "mean    20.907937328064932\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 47\n",
      "Train DCGs\n",
      "mean    20.93711302124987\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 48\n",
      "Train DCGs\n",
      "mean    20.933960038871213\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 49\n",
      "Train DCGs\n",
      "mean    20.95297454045271\n",
      "median  19.416508275000204\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "\n",
    "def predict(ensemble, X, learning_rate=0.1):\n",
    "    prediction = 0\n",
    "    for tree in ensemble:\n",
    "        prediction += tree.predict(X) * learning_rate\n",
    "    return prediction.rename('prediction')\n",
    "\n",
    "\n",
    "def tree_paths(tree, X):\n",
    "    paths_as_array = tree.decision_path(X).toarray()\n",
    "    paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "    return paths\n",
    "\n",
    "ensemble=[]\n",
    "def lambda_mart(judgments, rounds=20, learning_rate=0.1, max_leaf_nodes=8, metric=dcg):\n",
    "\n",
    "    print(judgments.columns)\n",
    "    # Convert to Pandas Dataframe\n",
    "    lambdas_per_query = judgments.copy()\n",
    "\n",
    "\n",
    "    lambdas_per_query['last_prediction'] = 0.0\n",
    "\n",
    "    for i in range(0, rounds):\n",
    "        print(f\"round {i}\")\n",
    "\n",
    "        # ------------------\n",
    "        #1. Build pair-wise predictors for this round\n",
    "        lambdas_per_query = lambdas_per_query.groupby('qid').apply(compute_swaps_scaled_with_weights, \n",
    "                                                                   axis=1, metric=dcg)\n",
    "\n",
    "        # ------------------\n",
    "        #2. Train a regression tree on this round's lambdas\n",
    "        features = lambdas_per_query['features'].tolist()\n",
    "        tree = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes)\n",
    "        tree.fit(features, lambdas_per_query['lambda'])    \n",
    "\n",
    "        # ------------------\n",
    "        #3. Reweight based on LambdaMART's weighted average\n",
    "        # Add each tree's paths\n",
    "        lambdas_per_query['path'] = tree_paths(tree, features)\n",
    "        predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "        predictions = predictions.fillna(0.0) # for divide by 0\n",
    "\n",
    "        # -------------------\n",
    "        #4. Add to ensemble, recreate last prediction\n",
    "        new_tree = OverridenRegressionTree(predictions=predictions, tree=tree)\n",
    "        ensemble.append(new_tree)\n",
    "        next_predictions = new_tree.predict(features)\n",
    "        lambdas_per_query['last_prediction'] += (next_predictions * learning_rate) \n",
    "        \n",
    "        print(\"Train DCGs\")\n",
    "        print(\"mean   \", lambdas_per_query['train_dcg'].mean())\n",
    "        print(\"median \", lambdas_per_query['train_dcg'].median())\n",
    "        print(\"----------\")\n",
    "\n",
    "        \n",
    "        # Reset the dataframe for further processing\n",
    "\n",
    "        lambdas_per_query = lambdas_per_query.drop('qid', axis=1).reset_index().drop(['level_1', 'index'], axis=1)\n",
    "\n",
    "judgments = judgments_to_dataframe(ftr_logger.logged, unnest=False)\n",
    "lambdas_per_query = lambda_mart(judgments=judgments, rounds=50, max_leaf_nodes=10, learning_rate=0.1, metric=dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/33/jx0mw87156q2hmtrr_r82s7r0000gs/T/ipykernel_15085/3870208638.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlambdas_per_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m282\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "lambdas_per_query.iloc[282]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble[0].predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(ensemble[0].tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to ranklib output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.ranklib import train\n",
    "trainLog  = train(client,\n",
    "                  training_set=ftr_logger.logged,\n",
    "                  index='tmdb',\n",
    "                  trees=10,\n",
    "                  featureSet='movies',\n",
    "                  modelName='title')\n",
    "\n",
    "print(\"Every N rounds of ranklib\")\n",
    "trainLog.trainingLogs[0].rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine queries we learned\n",
    "\n",
    "Try out some queries, look at the final model prediction `last_prediction` compare to the correct ordering `grade`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_per_query[lambdas_per_query['qid'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pure Pandas Implementation?\n",
    "\n",
    "Can we make it faster by vectorizing with pandas?\n",
    "\n",
    "Turns out Yes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_per_query = judgments.copy()\n",
    "\n",
    "\n",
    "lambdas_per_query['last_prediction'] = 0.0\n",
    "lambdas_per_query.sort_values(['qid', 'last_prediction'], ascending=[True, False], kind='stable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_per_query.sort_values(['qid', 'last_prediction'], ascending=[True, False], kind='stable')\n",
    "lambdas_per_query['display_rank'] = lambdas_per_query.groupby('qid').cumcount()\n",
    "\n",
    "#TBD - How do generalize this?\n",
    "lambdas_per_query['discount'] = 1 / np.log2(2 + lambdas_per_query['display_rank'])\n",
    "lambdas_per_query['gain'] = (2**lambdas_per_query['grade'] - 1) # * lambdas_per_query['discount']\n",
    "\n",
    "lambdas_per_query[['qid', 'display_rank', 'discount', 'grade', 'gain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise deltas\n",
    "\n",
    "Delta captures pair-wise difference of the ranking metric (ie DCG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each group paired with each other group\n",
    "swaps = lambdas_per_query.merge(lambdas_per_query, on='qid', how='outer')\n",
    "# changes[j][i] = changes[i][j] = (discount(i) - discount(j)) * (gain(rel[i]) - gain(rel[j]));\n",
    "swaps['delta'] = np.abs((swaps['discount_x'] - swaps['discount_y']) * (swaps['gain_x'] - swaps['gain_y']))\n",
    "swaps[['qid', 'display_rank_x', 'display_rank_y', 'delta']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise rhos\n",
    "\n",
    "Rho captures pair-wise difference of the current model's prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swaps['rho'] = 1 / (1 + np.exp(swaps['last_prediction_x'] - swaps['last_prediction_y']))\n",
    "swaps[['qid', 'display_rank_x', 'display_rank_y', 'delta', 'last_prediction_x', 'last_prediction_y', 'rho']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute lambdas\n",
    "\n",
    "For every row where grade_x > grade_y,  compute `delta*rho`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swaps['lambda'] = 0\n",
    "slice_x_better =swaps[swaps['grade_x'] > swaps['grade_y']]\n",
    "swaps.loc[swaps['grade_x'] > swaps['grade_y'], 'lambda'] = slice_x_better['delta'] * slice_x_better['rho']\n",
    "swaps[['qid', 'display_rank_x', 'display_rank_y', 'delta', 'last_prediction_x', 'last_prediction_y', 'rho', 'lambda']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get per-key lambdas\n",
    "\n",
    "We merge back together the xs minuse the ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better minus worse\n",
    "lambdas_x = swaps.groupby(['qid', 'display_rank_x'])['lambda'].sum().rename('lambda')\n",
    "lambdas_y = swaps.groupby(['qid', 'display_rank_y'])['lambda'].sum().rename('lambda')\n",
    "lambdas = lambdas_x - lambdas_y\n",
    "lambdas\n",
    "lambdas_per_query = lambdas_per_query.merge(lambdas, left_on=['qid', 'display_rank'], right_on=['qid', 'display_rank_x'], how='left')\n",
    "lambdas_per_query[['qid', 'docId', 'grade', 'features', 'lambda']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_per_query.merge(lambdas, left_on=['qid', 'display_rank'], right_on=['qid', 'display_rank_x'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Train a regression tree on this round's lambdas\n",
    "features = lambdas_per_query['features'].tolist()\n",
    "tree = DecisionTreeRegressor(max_leaf_nodes=10)\n",
    "tree.fit(features, lambdas_per_query['lambda'])    \n",
    "\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.append(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lambdas(lambdas_per_query):\n",
    "    lambdas_per_query = lambdas_per_query.sort_values(['qid', 'last_prediction'], ascending=[True, False], kind='stable')\n",
    "    lambdas_per_query['display_rank'] = lambdas_per_query.groupby('qid').cumcount()\n",
    "\n",
    "    #TBD - How do generalize this to any metric?\n",
    "    lambdas_per_query['discount'] = 1 / np.log2(2 + lambdas_per_query['display_rank'])\n",
    "    lambdas_per_query['gain'] = (2**lambdas_per_query['grade'] - 1)\n",
    "\n",
    "    # swaps dataframe holds each pair-wise swap computed (shrink columns for memory?)   \n",
    "    # Optimization of swaps = lambdas_per_query.merge(lambdas_per_query, on='qid', how='outer')\n",
    "    # to limit to just needed columns\n",
    "    to_swap = lambdas_per_query[['qid', 'display_rank', 'grade', 'last_prediction', 'discount', 'gain']]\n",
    "    #to_swap = lambdas_per_query\n",
    "    swaps = to_swap.merge(to_swap, on='qid', how='outer')\n",
    "\n",
    "    # delta - delta in DCG due to swap\n",
    "    swaps['delta'] = np.abs((swaps['discount_x'] - swaps['discount_y']) * (swaps['gain_x'] - swaps['gain_y']))\n",
    "    \n",
    "    # rho - based on current model prediction delta\n",
    "    swaps['rho'] = 1 / (1 + np.exp(swaps['last_prediction_x'] - swaps['last_prediction_y']))\n",
    "    \n",
    "    # If you want to be pure gradient boosting, weight reweights each models prediction\n",
    "    # I haven't found this to matter in practice\n",
    "    swaps['weight'] = swaps['rho'] * (1.0 - swaps['rho']) * swaps['delta']\n",
    "\n",
    "    # Compute lambdas (the next model in ensemble's predictors) when grade_x > grade_y\n",
    "    swaps['lambda'] = 0\n",
    "    slice_x_better =swaps[swaps['grade_x'] > swaps['grade_y']]\n",
    "    swaps.loc[swaps['grade_x'] > swaps['grade_y'], 'lambda'] = slice_x_better['delta'] * slice_x_better['rho']\n",
    "    \n",
    "    # accumulate lambdas and add back to model\n",
    "    lambdas_x = swaps.groupby(['qid', 'display_rank_x'])['lambda'].sum().rename('lambda')\n",
    "    lambdas_y = swaps.groupby(['qid', 'display_rank_y'])['lambda'].sum().rename('lambda')\n",
    "\n",
    "    weights_x = swaps.groupby(['qid', 'display_rank_x'])['weight'].sum().rename('weight')\n",
    "    weights_y = swaps.groupby(['qid', 'display_rank_y'])['weight'].sum().rename('weight')\n",
    "    \n",
    "    weights = weights_x + weights_y\n",
    "    lambdas = lambdas_x - lambdas_y\n",
    "\n",
    "    lambdas_per_query = lambdas_per_query.merge(lambdas, \n",
    "                                                left_on=['qid', 'display_rank'], \n",
    "                                                right_on=['qid', 'display_rank_x'], \n",
    "                                                how='left')\n",
    "    lambdas_per_query = lambdas_per_query.merge(weights, \n",
    "                                                left_on=['qid', 'display_rank'], \n",
    "                                                right_on=['qid', 'display_rank_x'], \n",
    "                                                how='left')\n",
    "\n",
    "    return lambdas_per_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "ensemble=[]\n",
    "def lambda_mart_pure(judgments, rounds=20, learning_rate=0.1, max_leaf_nodes=8, metric=dcg):\n",
    "\n",
    "    print(judgments.columns)\n",
    "    # Convert to Pandas Dataframe\n",
    "    lambdas_per_query = judgments.copy()\n",
    "\n",
    "\n",
    "    lambdas_per_query['last_prediction'] = 0.0\n",
    "\n",
    "    for i in range(0, rounds):\n",
    "        print(f\"round {i}\")\n",
    "\n",
    "        # ------------------\n",
    "        #1. Build pair-wise predictors for this round\n",
    "        lambdas_per_query = compute_lambdas(lambdas_per_query)\n",
    "\n",
    "        # ------------------\n",
    "        #2. Train a regression tree on this round's lambdas\n",
    "        features = lambdas_per_query['features'].tolist()\n",
    "        tree = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes)\n",
    "        tree.fit(features, lambdas_per_query['lambda'])    \n",
    "\n",
    "        # ------------------\n",
    "        #3. Reweight based on LambdaMART's weighted average\n",
    "        # Add each tree's paths\n",
    "        lambdas_per_query['path'] = tree_paths(tree, features)\n",
    "        predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "        predictions = predictions.fillna(0.0) # for divide by 0\n",
    "\n",
    "        # -------------------\n",
    "        #4. Add to ensemble, recreate last prediction\n",
    "        new_tree = OverridenRegressionTree(predictions=predictions, tree=tree)\n",
    "        ensemble.append(new_tree)\n",
    "        next_predictions = new_tree.predict(features)\n",
    "        lambdas_per_query['last_prediction'] += (next_predictions * learning_rate) \n",
    "        \n",
    "        print(lambdas_per_query.loc[0, ['grade', 'last_prediction']])\n",
    "        \n",
    "        print(\"Train DCGs\")\n",
    "        lambdas_per_query['discounted_gain'] = lambdas_per_query['gain'] * lambdas_per_query['discount'] \n",
    "        dcg = lambdas_per_query[lambdas_per_query['display_rank'] < 10].groupby('qid')['discounted_gain'].sum().mean()\n",
    "        print(\"mean   \", dcg)\n",
    "        print(\"----------\")\n",
    "        \n",
    "        lambdas_per_query = lambdas_per_query.drop(['lambda', 'weight'], axis=1)\n",
    "    return lambdas_per_query\n",
    "\n",
    "\n",
    "judgments = judgments_to_dataframe(ftr_logger.logged, unnest=False)\n",
    "lambdas_per_query = lambda_mart_pure(judgments=judgments, rounds=50, max_leaf_nodes=10, learning_rate=0.01, metric=dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judgments = judgments_to_dataframe(ftr_logger.logged, unnest=False)\n",
    "%prun -s cumtime lambdas_per_query = lambda_mart_pure(judgments=judgments, rounds=50, max_leaf_nodes=10, learning_rate=0.01, metric=dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.ranklib import train\n",
    "%prun -s cumtime trainLog  = train(client, training_set=ftr_logger.logged, index='tmdb', trees=10, featureSet='movies', modelName='title')\n",
    "\n",
    "print(\"Every N rounds of ranklib\")\n",
    "trainLog.trainingLogs[0].rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverridenRegressionTree:\n",
    "    def __init__(self, predictions, tree):\n",
    "        self.predictions = predictions\n",
    "        self.tree = tree\n",
    "        \n",
    "    def predict(self, X, use_original=False):\n",
    "        if use_original:\n",
    "            return self.predict(X)\n",
    "        path = self.tree.decision_path(X).toarray().astype(str)\n",
    "        path = \"\".join(path[0])\n",
    "        \n",
    "        paths_as_array = self.tree.decision_path(X).toarray()\n",
    "        paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "        \n",
    "        predictions = self.predictions[paths]\n",
    "        \n",
    "        # Any NaN predictions is a red flag, debug\n",
    "        if np.any(predictions.isnull()):\n",
    "            print(predictions[predictions.isnull()])\n",
    "            print(pd.DataFrame(X)[predictions.isnull().reset_index(drop=True)])\n",
    "            raise AssertionError(\"No prediction should be NaN\")\n",
    "        return np.array(self.predictions[paths].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
