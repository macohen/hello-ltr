{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LambdaMART in Python\n",
    "\n",
    "This is an implementation of LambdaMART in Python using sklearn and pandas. This is for educational purposes. \n",
    "\n",
    "But a secondary goal in getting this into Python is to more easily hack the algorithm to try new ideas. For example, this [blog article on two-sided marketplaces](https://opensourceconnections.com/blog/2017/07/04/optimizing-user-product-match-economies/), perhaps as more of an online algorithm (retiring old trees in the ensemble, adding new ones over time), perhaps with different model architectures in the ensemble (BERTy transformery things?) but all that preserve some of the nice things about LambdaMART (directly optimizing a list-wise metric)\n",
    "\n",
    "This is adapted from [RankLib](https://github.com/o19s/RankyMcRankFace/blob/master/src/ciir/umass/edu/learning/tree/LambdaMART.java#L444) based on [this paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf) from Microsoft Research.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup - TheMovieDB corpus and log OpenSearch features from TMDB](#Part-Zero---Setup---Get-TheMovieDB-Corpus-and-Log-Simple-Features) - plumbing to interact with OpenSearch Learning to Rank to log a few basic features for our exploration\n",
    "2. [Pairwise swapping](#Part-One---Collect-pair-wise-DCG-diffs) - here we demonstrate the core operation of LambdaMART - pairwise swapping of pairs and examining DCG (or another ranking metric) impact\n",
    "3. [Scale to learn errors, not just swaps](#Part-Two---Compute-the-swaps-but-scaled-to-current-model's-error) - here we show how LambdaMART isn't just about learning the pairwise DCG difference of a swap, but the error currently in the model at predicting the DCG impact of that swap\n",
    "4. [Weigh predictions](#Part-Three---Weigh-each-leaf's-predictions) - here we weigh the predictions of the next model in the ensemble based on how much DCG remains to be learned. \n",
    "5. [Putting it all together](#Part-Four---Putting-it-all-together,-from-the-top!) - the full algorithm in one place. You can also compare this notebook's output and learning to Ranklib.\n",
    "\n",
    "---\n",
    "\n",
    "6. [A Pandas version!](#5.-Pure-Pandas-Implementation?) -- walking through a faster version computing the per-tree training data using Pandas - a much for useful toy example.\n",
    "\n",
    "## Known Issues\n",
    "\n",
    "I'm still learning the nooks and crannies of the algorithm. So there are some known issues as this is actively being developed.\n",
    "\n",
    "1. **Performance** - a single training round takes about 9 seconds. There's room for improvement in the hot part of the loop (dcg computation and swapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Zero - Setup - Get TheMovieDB Corpus and Log Simple Features\n",
    "\n",
    "In this step we download TheMovieDB Corpus and log some featurs (title and overview BM25). At the end we have a simple dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:9201/_ltr; <OpenSearch([{'host': 'localhost', 'port': 9201}])>\n"
     ]
    }
   ],
   "source": [
    "from ltr.client import OpenSearchClient\n",
    "client = OpenSearchClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and index TMDB corpus and training set\n",
    "\n",
    "Download [TheMovieDB](http://themoviedb.org) corpus and small toy training set with 40 queries labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/tmdb.json already exists\n",
      "data/title_judgments.txt already exists\n",
      "Index tmdb already exists. Use `force = True` to delete and recreate\n"
     ]
    }
   ],
   "source": [
    "from ltr import download\n",
    "corpus='http://es-learn-to-rank.labs.o19s.com/tmdb.json'\n",
    "judgments='http://es-learn-to-rank.labs.o19s.com/title_judgments.txt'\n",
    "\n",
    "download([corpus, judgments], dest='data/');\n",
    "from ltr.index import rebuild\n",
    "from ltr.helpers.movies import indexable_movies\n",
    "\n",
    "movies=indexable_movies(movies='data/tmdb.json')\n",
    "rebuild(client, index='tmdb', doc_src=movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log two features - title & overview\n",
    "\n",
    "Using the Elasticsearch Learning to Rank plugin, we:\n",
    "\n",
    "1. Log two features: title and overview bm25\n",
    "2. Create a pandas dataframe containing the labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Default LTR feature store [Status: 200]\n",
      "Initialize Default LTR feature store [Status: 200]\n",
      "Create movies feature set [Status: 201]\n",
      "Recognizing 40 queries\n"
     ]
    }
   ],
   "source": [
    "from ltr.log import FeatureLogger\n",
    "from ltr.judgments import judgments_open\n",
    "from itertools import groupby\n",
    "from ltr.judgments import judgments_to_dataframe\n",
    "\n",
    "client.reset_ltr(index='tmdb')\n",
    "\n",
    "config = {\"validation\": {\n",
    "              \"index\": \"tmdb\",\n",
    "              \"params\": {\n",
    "                  \"keywords\": \"rambo\"\n",
    "              }\n",
    "    \n",
    "           },\n",
    "           \"featureset\": {\n",
    "            \"features\": [\n",
    "                { #1\n",
    "                    \"name\": \"title_bm25\",\n",
    "                    \"params\": [\"keywords\"],\n",
    "                    \"template\": {\n",
    "                        \"match\": {\"title\": \"{{keywords}}\"}\n",
    "                    }\n",
    "                },\n",
    "                { #2\n",
    "                    \"name\": \"overview_bm25\",\n",
    "                    \"params\": [\"keywords\"],\n",
    "                    \"template\": {\n",
    "                        \"match\": {\"overview\": \"{{keywords}}\"}\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "    }}\n",
    "\n",
    "\n",
    "client.create_featureset(index='tmdb', name='movies', ftr_config=config)\n",
    "\n",
    "# Log features for each query\n",
    "ftr_logger=FeatureLogger(client, index='tmdb', feature_set='movies')\n",
    "with judgments_open('data/title_judgments.txt') as judgment_list:\n",
    "    for qid, query_judgments in groupby(judgment_list, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(judgments=query_judgments, \n",
    "                               qid=qid,\n",
    "                               keywords=judgment_list.keywords(qid))\n",
    "        \n",
    "# Convert to Pandas Dataframe\n",
    "judgments = judgments_to_dataframe(ftr_logger.logged, unnest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine judgments dataframe\n",
    "\n",
    "In the dataframe we have a set of (query, document, grade) that label how relevant a document (movie) is for each query.\n",
    "\n",
    "* qid - 'query id' - a unique identifier for this query\n",
    "* docId - an identifier for the document (here movie) being labeled\n",
    "* grade - how relevant a movie is on a 0-4 scale\n",
    "* keywords - the query keywords that go along with the query id\n",
    "* features - the two features we logged, 0th is title_bm25, 1st is overview_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            uid  qid   keywords   docId  grade                features\n",
       "0        1_7555    1      rambo    7555      4  [11.657399, 10.083591]\n",
       "1        1_1370    1      rambo    1370      3   [9.456276, 13.265001]\n",
       "2        1_1369    1      rambo    1369      3   [6.036743, 11.113943]\n",
       "3       1_13258    1      rambo   13258      2         [0.0, 6.869545]\n",
       "4        1_1368    1      rambo    1368      4        [0.0, 11.113943]\n",
       "...         ...  ...        ...     ...    ...                     ...\n",
       "1385   40_37079   40  star wars   37079      0              [0.0, 0.0]\n",
       "1386  40_126757   40  star wars  126757      0              [0.0, 0.0]\n",
       "1387   40_39797   40  star wars   39797      0              [0.0, 0.0]\n",
       "1388   40_18112   40  star wars   18112      0              [0.0, 0.0]\n",
       "1389   40_43052   40  star wars   43052      0              [0.0, 0.0]\n",
       "\n",
       "[1390 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judgments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One - Collect pair-wise DCG diffs\n",
    "\n",
    "The first-pass iteration of LambdaMART, for each query, we examine the DCG\\* impact of swapping each result with another result in the listing.\n",
    "\n",
    "\\* replace DCG with your metric of interest: MAP, Precision@N, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>427.587115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>9.463946</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>219.800566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>62.204093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>3.014736</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>43.694734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>1.160558</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>5.542298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>1385</td>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>25</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.598524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1386</td>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>26</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.715586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1387</td>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>27</td>\n",
       "      <td>0.205847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.826142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1388</td>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>28</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.930783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1389</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-21.030027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index        uid  qid   keywords   docId  grade  \\\n",
       "qid                                                       \n",
       "1   0       0     1_7555    1      rambo    7555      4   \n",
       "    1       4     1_1368    1      rambo    1368      4   \n",
       "    2       1     1_1370    1      rambo    1370      3   \n",
       "    3       2     1_1369    1      rambo    1369      3   \n",
       "    4       3    1_13258    1      rambo   13258      2   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1385   40_37079   40  star wars   37079      0   \n",
       "    26   1386  40_126757   40  star wars  126757      0   \n",
       "    27   1387   40_39797   40  star wars   39797      0   \n",
       "    28   1388   40_18112   40  star wars   18112      0   \n",
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  display_rank  discount       gain        dcg  \\\n",
       "qid                                                                            \n",
       "1   0   [11.657399, 10.083591]             0  1.000000  15.000000  33.734341   \n",
       "    1         [0.0, 11.113943]             1  0.630930   9.463946  33.734341   \n",
       "    2    [9.456276, 13.265001]             2  0.500000   3.500000  33.734341   \n",
       "    3    [6.036743, 11.113943]             3  0.430677   3.014736  33.734341   \n",
       "    4          [0.0, 6.869545]             4  0.386853   1.160558  33.734341   \n",
       "...                        ...           ...       ...        ...        ...   \n",
       "40  25              [0.0, 0.0]            25  0.210310   0.000000  31.225149   \n",
       "    26              [0.0, 0.0]            26  0.208015   0.000000  31.225149   \n",
       "    27              [0.0, 0.0]            27  0.205847   0.000000  31.225149   \n",
       "    28              [0.0, 0.0]            28  0.203795   0.000000  31.225149   \n",
       "    29              [0.0, 0.0]             8  0.301030   0.000000  31.225149   \n",
       "\n",
       "            lambda  \n",
       "qid                 \n",
       "1   0   427.587115  \n",
       "    1   219.800566  \n",
       "    2    62.204093  \n",
       "    3    43.694734  \n",
       "    4     5.542298  \n",
       "...            ...  \n",
       "40  25  -20.598524  \n",
       "    26  -20.715586  \n",
       "    27  -20.826142  \n",
       "    28  -20.930783  \n",
       "    29  -21.030027  \n",
       "\n",
       "[1390 rows x 12 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log, exp\n",
    "import numpy as np \n",
    "\n",
    "def rank_with_swap(ranked_list, rank1=0, rank2=0):\n",
    "    \"\"\" Set the display rank of positions given the provided swap \"\"\"\n",
    "    ranked_list['display_rank'] = ranked_list.index.to_series()\n",
    "    \n",
    "    if rank1 != rank2:\n",
    "        ranked_list.loc[rank1, 'display_rank'] = rank2\n",
    "        ranked_list.loc[rank2, 'display_rank'] = rank1\n",
    "    return ranked_list\n",
    "    \n",
    "\n",
    "def dcg(ranked_list, at=10):\n",
    "    \"\"\"Given a list, compute DCG -- \n",
    "       uses same variant as lambdamart 2**grade / log2(displayrank)\n",
    "    \"\"\"\n",
    "    ranked_list['discount'] = 1 / np.log2(2 + ranked_list['display_rank'])\n",
    "    ranked_list['gain'] = (2**ranked_list['grade'] - 1) * ranked_list['discount'] # TODO - precompute gain on swapping\n",
    "    return sum(ranked_list['gain'].head(at))\n",
    "\n",
    "def compute_swaps(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\"\"\"\n",
    "    \n",
    "    # Sort to see ideal ordering\n",
    "    # This isn't strictly nescesarry, but it's helpful to understand the algorithm\n",
    "    query_judgments = query_judgments.sort_values('grade', kind='stable', ascending=False).reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    best_dcg = query_judgments.loc[0, 'dcg']\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    \n",
    "    # TODO - redo inner body as \n",
    "    for better in range(0,len(query_judgments)):\n",
    "        for worse in range(0,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                break\n",
    "\n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                query_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "\n",
    "                dcg_after_swap = query_judgments.loc[0, 'dcg']\n",
    "                delta = abs(best_dcg - dcg_after_swap)\n",
    "\n",
    "                if delta > 0.0:\n",
    "\n",
    "                    # Add delta to better's lambda (-delta to worse's lambda)\n",
    "                    query_judgments.loc[better, 'lambda'] += delta\n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta\n",
    "\n",
    "    # print(query_judgments[['keywords', 'docId', 'grade', 'lambda', 'features']])\n",
    "    return query_judgments\n",
    "\n",
    "# For each query, compute lambdas\n",
    "# %prun -s cumulative lambdas_per_query = judgments.groupby('qid').apply(compute_swaps, axis=1)\n",
    "# judgments\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps, axis=1)\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Precision instead of DCG\n",
    "\n",
    "We can really use any ranking metric to achieve goals important to our product. This includes potentially ones we invent or come up with ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>5_603</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>603</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.040129]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150</td>\n",
       "      <td>5_604</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>604</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 9.392262]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>151</td>\n",
       "      <td>5_605</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>605</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 0.0]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>152</td>\n",
       "      <td>5_55931</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>55931</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 10.798681]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "      <td>5_73262</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>73262</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>32</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>153</td>\n",
       "      <td>5_1857</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>1857</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 9.65805]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>154</td>\n",
       "      <td>5_10999</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>10999</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 11.466951]</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>155</td>\n",
       "      <td>5_4247</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>4247</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 8.114125]</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>156</td>\n",
       "      <td>5_21874</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>21874</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 7.8627386]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>157</td>\n",
       "      <td>5_181886</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>181886</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>158</td>\n",
       "      <td>5_21208</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>21208</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>159</td>\n",
       "      <td>5_125607</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>125607</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>160</td>\n",
       "      <td>5_56441</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>56441</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>12</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>161</td>\n",
       "      <td>5_124080</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>124080</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>13</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>162</td>\n",
       "      <td>5_1487</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>1487</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>14</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>163</td>\n",
       "      <td>5_72867</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>72867</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>15</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>164</td>\n",
       "      <td>5_11253</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>11253</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>165</td>\n",
       "      <td>5_213110</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>213110</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>17</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>166</td>\n",
       "      <td>5_13805</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>13805</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>18</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>167</td>\n",
       "      <td>5_104221</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>104221</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>19</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>168</td>\n",
       "      <td>5_183894</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>183894</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>20</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>169</td>\n",
       "      <td>5_3573</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>3573</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>21</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>170</td>\n",
       "      <td>5_12254</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>12254</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>22</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>171</td>\n",
       "      <td>5_17813</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>17813</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>23</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>173</td>\n",
       "      <td>5_17960</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>17960</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>24</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>174</td>\n",
       "      <td>5_33068</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>33068</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>25</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>175</td>\n",
       "      <td>5_28377</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>28377</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>26</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>176</td>\n",
       "      <td>5_13300</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>13300</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>27</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>177</td>\n",
       "      <td>5_680</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>28</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>178</td>\n",
       "      <td>5_28131</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>28131</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>29</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>179</td>\n",
       "      <td>5_37988</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>37988</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>180</td>\n",
       "      <td>5_18451</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>18451</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>31</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>181</td>\n",
       "      <td>5_75404</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>75404</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index       uid  qid keywords   docId  grade                features  \\\n",
       "0     149     5_603    5   matrix     603      4  [11.657399, 10.040129]   \n",
       "1     150     5_604    5   matrix     604      3    [9.456276, 9.392262]   \n",
       "2     151     5_605    5   matrix     605      3         [9.456276, 0.0]   \n",
       "3     152   5_55931    5   matrix   55931      2        [0.0, 10.798681]   \n",
       "4     172   5_73262    5   matrix   73262      1              [0.0, 0.0]   \n",
       "5     153    5_1857    5   matrix    1857      0          [0.0, 9.65805]   \n",
       "6     154   5_10999    5   matrix   10999      0        [0.0, 11.466951]   \n",
       "7     155    5_4247    5   matrix    4247      0         [0.0, 8.114125]   \n",
       "8     156   5_21874    5   matrix   21874      0        [0.0, 7.8627386]   \n",
       "9     157  5_181886    5   matrix  181886      0              [0.0, 0.0]   \n",
       "10    158   5_21208    5   matrix   21208      0              [0.0, 0.0]   \n",
       "11    159  5_125607    5   matrix  125607      0              [0.0, 0.0]   \n",
       "12    160   5_56441    5   matrix   56441      0              [0.0, 0.0]   \n",
       "13    161  5_124080    5   matrix  124080      0              [0.0, 0.0]   \n",
       "14    162    5_1487    5   matrix    1487      0              [0.0, 0.0]   \n",
       "15    163   5_72867    5   matrix   72867      0              [0.0, 0.0]   \n",
       "16    164   5_11253    5   matrix   11253      0              [0.0, 0.0]   \n",
       "17    165  5_213110    5   matrix  213110      0              [0.0, 0.0]   \n",
       "18    166   5_13805    5   matrix   13805      0              [0.0, 0.0]   \n",
       "19    167  5_104221    5   matrix  104221      0              [0.0, 0.0]   \n",
       "20    168  5_183894    5   matrix  183894      0              [0.0, 0.0]   \n",
       "21    169    5_3573    5   matrix    3573      0              [0.0, 0.0]   \n",
       "22    170   5_12254    5   matrix   12254      0              [0.0, 0.0]   \n",
       "23    171   5_17813    5   matrix   17813      0              [0.0, 0.0]   \n",
       "24    173   5_17960    5   matrix   17960      0              [0.0, 0.0]   \n",
       "25    174   5_33068    5   matrix   33068      0              [0.0, 0.0]   \n",
       "26    175   5_28377    5   matrix   28377      0              [0.0, 0.0]   \n",
       "27    176   5_13300    5   matrix   13300      0              [0.0, 0.0]   \n",
       "28    177     5_680    5   matrix     680      0              [0.0, 0.0]   \n",
       "29    178   5_28131    5   matrix   28131      0              [0.0, 0.0]   \n",
       "30    179   5_37988    5   matrix   37988      0              [0.0, 0.0]   \n",
       "31    180   5_18451    5   matrix   18451      0              [0.0, 0.0]   \n",
       "32    181   5_75404    5   matrix   75404      0              [0.0, 0.0]   \n",
       "\n",
       "    display_rank  dcg  lambda  \n",
       "0              0  0.3   2.300  \n",
       "1              1  0.3   1.725  \n",
       "2              2  0.3   1.725  \n",
       "3              3  0.3   1.150  \n",
       "4             32  0.3   0.575  \n",
       "5              5  0.3   0.000  \n",
       "6              6  0.3   0.000  \n",
       "7              7  0.3   0.000  \n",
       "8              8  0.3   0.000  \n",
       "9              9  0.3   0.000  \n",
       "10            10  0.3  -0.325  \n",
       "11            11  0.3  -0.325  \n",
       "12            12  0.3  -0.325  \n",
       "13            13  0.3  -0.325  \n",
       "14            14  0.3  -0.325  \n",
       "15            15  0.3  -0.325  \n",
       "16            16  0.3  -0.325  \n",
       "17            17  0.3  -0.325  \n",
       "18            18  0.3  -0.325  \n",
       "19            19  0.3  -0.325  \n",
       "20            20  0.3  -0.325  \n",
       "21            21  0.3  -0.325  \n",
       "22            22  0.3  -0.325  \n",
       "23            23  0.3  -0.325  \n",
       "24            24  0.3  -0.325  \n",
       "25            25  0.3  -0.325  \n",
       "26            26  0.3  -0.325  \n",
       "27            27  0.3  -0.325  \n",
       "28            28  0.3  -0.325  \n",
       "29            29  0.3  -0.325  \n",
       "30            30  0.3  -0.325  \n",
       "31            31  0.3  -0.325  \n",
       "32             4  0.3  -0.325  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision(ranked_list, max_grade=4.0, at=10):\n",
    "    \"\"\"Given a list, compute simple precision. Really this is cumalitive gain.\"\"\"\n",
    "    above_n = ranked_list[ranked_list['display_rank'] < at]\n",
    "    \n",
    "    if (max_grade * at) == 0.0:\n",
    "        print(\"0\")\n",
    "        return 0.0\n",
    "    \n",
    "    return float(sum(above_n['grade'])) / (max_grade * at)\n",
    "\n",
    "\n",
    "lambdas_per_query_prec = judgments.groupby('qid').apply(compute_swaps, axis=1, metric=precision)\n",
    "lambdas_per_query_prec.loc[5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a model on the lambdas\n",
    "\n",
    "The core operation is fitting an operation on the lambdas (the accumulated pairwise differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>lambda</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>427.587115</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>219.800566</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.204093</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43.694734</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.542298</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>-20.598524</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-20.715586</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-20.826142</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-20.930783</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-21.030027</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            lambda                features\n",
       "qid                                       \n",
       "1   0   427.587115  [11.657399, 10.083591]\n",
       "    1   219.800566        [0.0, 11.113943]\n",
       "    2    62.204093   [9.456276, 13.265001]\n",
       "    3    43.694734   [6.036743, 11.113943]\n",
       "    4     5.542298         [0.0, 6.869545]\n",
       "...            ...                     ...\n",
       "40  25  -20.598524              [0.0, 0.0]\n",
       "    26  -20.715586              [0.0, 0.0]\n",
       "    27  -20.826142              [0.0, 0.0]\n",
       "    28  -20.930783              [0.0, 0.0]\n",
       "    29  -21.030027              [0.0, 0.0]\n",
       "\n",
       "[1390 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(train_set['features'].tolist(), train_set['lambda'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCG-based Lambda Predictions\n",
    "\n",
    "We show predicting some known examples. In the first case, strong title and overview scores. In the second case, no title or overview scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([445.85277429])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.predict([[11.1, 10.08]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-15.3987952])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.predict([[0.0, 0.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's more typical we would restrict the complexity of each tree in the ensemble. We can dump the tree see [understanding sklearn's tree structure](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0.8333333333333334, 'x[0] <= 10.666\\nsquared_error = 4161.543\\nsamples = 1390\\nvalue = -0.0'),\n",
       " Text(0.25, 0.5, 'x[0] <= 9.182\\nsquared_error = 1038.341\\nsamples = 1329\\nvalue = -9.745'),\n",
       " Text(0.125, 0.16666666666666666, 'squared_error = 421.042\\nsamples = 1301\\nvalue = -11.724'),\n",
       " Text(0.375, 0.16666666666666666, 'squared_error = 21086.54\\nsamples = 28\\nvalue = 82.191'),\n",
       " Text(0.75, 0.5, 'x[0] <= 18.186\\nsquared_error = 25061.546\\nsamples = 61\\nvalue = 212.311'),\n",
       " Text(0.625, 0.16666666666666666, 'squared_error = 26150.423\\nsamples = 51\\nvalue = 188.147'),\n",
       " Text(0.875, 0.16666666666666666, 'squared_error = 1343.167\\nsamples = 10\\nvalue = 335.547')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "tree.fit(train_set['features'].tolist(), train_set['lambda'])\n",
    "plot_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two - Compute the swaps but _scaled_ to current model's error\n",
    "\n",
    "LambdaMART is an _ensemble_ model. It's not just about the first model, but collecting a series of models where each model makes a gradual improvement on the current model. The technique used is known as [Gradient Boosting]()\n",
    "\n",
    "To build a model that compensates for the current model's error, we scale the next set of dependent vars to predict based on the correctness of the existing model in ranking. In this way, we eliminate where the model currently does a good job (no need to learn these) and leave in places where the model isn't doing a good job (this is where. we want ot learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "      <th>delta</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>213.776822</td>\n",
       "      <td>427.553645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>4.416508</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>48.987136</td>\n",
       "      <td>97.974272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>31.835338</td>\n",
       "      <td>63.670676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>1.292030</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>6.723500</td>\n",
       "      <td>13.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>5.802792</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>41.948006</td>\n",
       "      <td>83.896012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>1385</td>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-9.846078</td>\n",
       "      <td>-19.692155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1386</td>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-9.903461</td>\n",
       "      <td>-19.806921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1387</td>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.205847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-9.957655</td>\n",
       "      <td>-19.915309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1388</td>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-10.008949</td>\n",
       "      <td>-20.017899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1389</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.289065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-10.057598</td>\n",
       "      <td>-20.115197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index        uid  qid   keywords   docId  grade  \\\n",
       "qid                                                       \n",
       "1   0       0     1_7555    1      rambo    7555      4   \n",
       "    1       1     1_1370    1      rambo    1370      3   \n",
       "    2       2     1_1369    1      rambo    1369      3   \n",
       "    3       3    1_13258    1      rambo   13258      2   \n",
       "    4       4     1_1368    1      rambo    1368      4   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1385   40_37079   40  star wars   37079      0   \n",
       "    26   1386  40_126757   40  star wars  126757      0   \n",
       "    27   1387   40_39797   40  star wars   39797      0   \n",
       "    28   1388   40_18112   40  star wars   18112      0   \n",
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  last_prediction  display_rank  discount  \\\n",
       "qid                                                                       \n",
       "1   0   [11.657399, 10.083591]              0.0             0  1.000000   \n",
       "    1    [9.456276, 13.265001]              0.0             1  0.630930   \n",
       "    2    [6.036743, 11.113943]              0.0             2  0.500000   \n",
       "    3          [0.0, 6.869545]              0.0             3  0.430677   \n",
       "    4         [0.0, 11.113943]              0.0             4  0.386853   \n",
       "...                        ...              ...           ...       ...   \n",
       "40  25              [0.0, 0.0]              0.0            25  0.210310   \n",
       "    26              [0.0, 0.0]              0.0            26  0.208015   \n",
       "    27              [0.0, 0.0]              0.0            27  0.205847   \n",
       "    28              [0.0, 0.0]              0.0            28  0.203795   \n",
       "    29              [0.0, 0.0]              0.0             9  0.289065   \n",
       "\n",
       "             gain        dcg      lambda       delta  \n",
       "qid                                                   \n",
       "1   0   15.000000  30.700871  213.776822  427.553645  \n",
       "    1    4.416508  30.700871   48.987136   97.974272  \n",
       "    2    3.500000  30.700871   31.835338   63.670676  \n",
       "    3    1.292030  30.700871    6.723500   13.447000  \n",
       "    4    5.802792  30.700871   41.948006   83.896012  \n",
       "...           ...        ...         ...         ...  \n",
       "40  25   0.000000  30.207651   -9.846078  -19.692155  \n",
       "    26   0.000000  30.207651   -9.903461  -19.806921  \n",
       "    27   0.000000  30.207651   -9.957655  -19.915309  \n",
       "    28   0.000000  30.207651  -10.008949  -20.017899  \n",
       "    29   0.000000  30.207651  -10.057598  -20.115197  \n",
       "\n",
       "[1390 rows x 14 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "judgments['last_prediction'] = tree.predict(judgments['features'].tolist()) * learning_rate\n",
    "\n",
    "def compute_swaps_scaled(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Important - stable sort. Otherwise DCG swaps get kind of wonky due to position discounts\n",
    "    query_judgments = query_judgments.sort_values('last_prediction', ascending=False, kind='stable').reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    best_dcg = query_judgments.loc[0, 'dcg']\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    query_judgments['delta'] = 0.0\n",
    "    \n",
    "    for better in range(0,len(query_judgments)):\n",
    "        for worse in range(better+1,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                break\n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                swap_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                dcg_after_swap = metric(swap_judgments, at=at)\n",
    "\n",
    "                delta = abs(best_dcg - dcg_after_swap)\n",
    "\n",
    "                if delta > 0.0:\n",
    "                    \n",
    "                    # --------------\n",
    "                    # NEW!\n",
    "                    model_score_diff = query_judgments.loc[better, 'last_prediction'] - query_judgments.loc[worse, 'last_prediction']\n",
    "                    rho = 1.0 / (1.0 + exp(model_score_diff))    \n",
    "                    # --------------\n",
    "                    # rho works as follows\n",
    "                    # \n",
    "                    # model ranks                    rho\n",
    "                    # better higher than worse       approaches 0      <-- model currently doing well!\n",
    "                    # better same as worse.          0.5  \n",
    "                    # worse higher than better       approaches 1      <-- model currently doing poorly!\n",
    "                    # \n",
    "                    query_judgments.loc[better, 'delta'] += delta\n",
    "                    \n",
    "                    # Use rho to scale the lambdas\n",
    "                    query_judgments.loc[better, 'lambda'] += delta * rho\n",
    "        \n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta * rho\n",
    "                    query_judgments.loc[worse, 'delta'] -= delta\n",
    "\n",
    "    return query_judgments\n",
    "\n",
    "judgments = judgments_to_dataframe(ftr_logger.logged, unnest=False)\n",
    "judgments['last_prediction'] = 0.0\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps_scaled, axis=1)\n",
    "#\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero in on 2 swapped by each result worse than it in query `ramba`\n",
    "\n",
    "```\n",
    "better_grade worse_grade, model_score_diffs, rho,                 dcg_delta\n",
    "2 1                       0.758706128029972  0.31892724571177816  0.02502724555344038\n",
    "2 1                       0.981162516523929  0.2726611758805124   0.04377062727053094\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.11698017724693699\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.14090604137532914\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.08043118677314176\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.17784892734690594\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.19254512210920538\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.20543079947538878\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.21685570619866112\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.22708156316293504\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.23630863576764227\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.24469312448475122\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.2523588995024131\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.25940563061320177\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.2659145510893115\n",
    "...\n",
    "2 0                       1.142439045359345  0.24187283082372052 0.34214193097965406\n",
    "```\n",
    "\n",
    "Summing all the model score diffs, we see those are rather high. This results in a high-ish rho between (for each value here 0.25-0.31). So each dcg_delta is added to the model.\n",
    "\n",
    "What's the intuition here? The model hasn't entirely nailed this example, the model feels there's more 'dcg_delta' to learn to push it away from those less relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zooming out to more of `rambo`\n",
    "\n",
    "We see a similar pattern in results with mediocre grades (2 and 3) where the resulting rho-scaled lambda's are higher than you might expect. The model's happy with the position of 0, but the ranking of other results could be separated more. The model diff should be higher when compared to the dcg diff to push the middling results away from the irrelevant result.\n",
    "\n",
    "So the next tree learns these lambdas using the resulting features moreso than other results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>grade</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>delta</th>\n",
       "      <th>lambda</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>427.553645</td>\n",
       "      <td>213.776822</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rambo</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.868699</td>\n",
       "      <td>-9.934349</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rambo</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.149295</td>\n",
       "      <td>-10.074647</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rambo</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.276314</td>\n",
       "      <td>-10.138157</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rambo</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.395685</td>\n",
       "      <td>-10.197842</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rambo</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.508155</td>\n",
       "      <td>-10.254078</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rambo</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.336529</td>\n",
       "      <td>-10.168264</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rambo</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.432963</td>\n",
       "      <td>-10.216481</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rambo</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.524423</td>\n",
       "      <td>-10.262211</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rambo</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.611330</td>\n",
       "      <td>-10.305665</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rambo</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.694056</td>\n",
       "      <td>-10.347028</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>rambo</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.069351</td>\n",
       "      <td>-10.534675</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>rambo</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.147879</td>\n",
       "      <td>-10.573939</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>rambo</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.222977</td>\n",
       "      <td>-10.611488</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>rambo</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.294893</td>\n",
       "      <td>-10.647447</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>rambo</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.363851</td>\n",
       "      <td>-10.681926</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>rambo</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.430053</td>\n",
       "      <td>-10.715026</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>rambo</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.493681</td>\n",
       "      <td>-10.746841</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>rambo</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.554902</td>\n",
       "      <td>-10.777451</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rambo</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.013760</td>\n",
       "      <td>-10.006880</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rambo</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.712923</td>\n",
       "      <td>-9.856462</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.974272</td>\n",
       "      <td>48.987136</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rambo</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.545028</td>\n",
       "      <td>-9.772514</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rambo</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.670676</td>\n",
       "      <td>31.835338</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rambo</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.447000</td>\n",
       "      <td>6.723500</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rambo</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.896012</td>\n",
       "      <td>41.948006</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rambo</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.400912</td>\n",
       "      <td>-4.200456</td>\n",
       "      <td>[0.0, 7.8627386]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rambo</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.024956</td>\n",
       "      <td>-5.012478</td>\n",
       "      <td>[0.0, 4.563677]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rambo</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.243092</td>\n",
       "      <td>-7.621546</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rambo</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.950401</td>\n",
       "      <td>-7.975200</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rambo</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-16.536694</td>\n",
       "      <td>-8.268347</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rambo</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.032666</td>\n",
       "      <td>-8.516333</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rambo</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.459201</td>\n",
       "      <td>-8.729601</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rambo</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.831043</td>\n",
       "      <td>-8.915522</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rambo</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.158927</td>\n",
       "      <td>-9.079464</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rambo</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.450871</td>\n",
       "      <td>-9.225435</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rambo</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.712994</td>\n",
       "      <td>-9.356497</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rambo</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.950060</td>\n",
       "      <td>-9.475030</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rambo</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.165834</td>\n",
       "      <td>-9.582917</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rambo</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.363338</td>\n",
       "      <td>-9.681669</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>rambo</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.613868</td>\n",
       "      <td>-10.806934</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keywords  display_rank  grade  last_prediction       delta      lambda  \\\n",
       "0     rambo             0      4              0.0  427.553645  213.776822   \n",
       "21    rambo            21      0              0.0  -19.868699   -9.934349   \n",
       "23    rambo            23      0              0.0  -20.149295  -10.074647   \n",
       "24    rambo            24      0              0.0  -20.276314  -10.138157   \n",
       "25    rambo            25      0              0.0  -20.395685  -10.197842   \n",
       "26    rambo            26      0              0.0  -20.508155  -10.254078   \n",
       "27    rambo            27      1              0.0  -20.336529  -10.168264   \n",
       "28    rambo            28      1              0.0  -20.432963  -10.216481   \n",
       "29    rambo            29      1              0.0  -20.524423  -10.262211   \n",
       "30    rambo            30      1              0.0  -20.611330  -10.305665   \n",
       "31    rambo            31      1              0.0  -20.694056  -10.347028   \n",
       "32    rambo            32      0              0.0  -21.069351  -10.534675   \n",
       "33    rambo            33      0              0.0  -21.147879  -10.573939   \n",
       "34    rambo            34      0              0.0  -21.222977  -10.611488   \n",
       "35    rambo            35      0              0.0  -21.294893  -10.647447   \n",
       "36    rambo            36      0              0.0  -21.363851  -10.681926   \n",
       "37    rambo            37      0              0.0  -21.430053  -10.715026   \n",
       "38    rambo            38      0              0.0  -21.493681  -10.746841   \n",
       "39    rambo            39      0              0.0  -21.554902  -10.777451   \n",
       "22    rambo            22      0              0.0  -20.013760  -10.006880   \n",
       "20    rambo            20      0              0.0  -19.712923   -9.856462   \n",
       "1     rambo             1      3              0.0   97.974272   48.987136   \n",
       "19    rambo            19      0              0.0  -19.545028   -9.772514   \n",
       "2     rambo             2      3              0.0   63.670676   31.835338   \n",
       "3     rambo             3      2              0.0   13.447000    6.723500   \n",
       "4     rambo             4      4              0.0   83.896012   41.948006   \n",
       "5     rambo             5      1              0.0   -8.400912   -4.200456   \n",
       "6     rambo            40      1              0.0  -10.024956   -5.012478   \n",
       "7     rambo             7      0              0.0  -15.243092   -7.621546   \n",
       "8     rambo             8      0              0.0  -15.950401   -7.975200   \n",
       "9     rambo             9      0              0.0  -16.536694   -8.268347   \n",
       "10    rambo            10      0              0.0  -17.032666   -8.516333   \n",
       "11    rambo            11      0              0.0  -17.459201   -8.729601   \n",
       "12    rambo            12      0              0.0  -17.831043   -8.915522   \n",
       "13    rambo            13      0              0.0  -18.158927   -9.079464   \n",
       "14    rambo            14      0              0.0  -18.450871   -9.225435   \n",
       "15    rambo            15      0              0.0  -18.712994   -9.356497   \n",
       "16    rambo            16      0              0.0  -18.950060   -9.475030   \n",
       "17    rambo            17      0              0.0  -19.165834   -9.582917   \n",
       "18    rambo            18      0              0.0  -19.363338   -9.681669   \n",
       "40    rambo             6      0              0.0  -21.613868  -10.806934   \n",
       "\n",
       "                  features  \n",
       "0   [11.657399, 10.083591]  \n",
       "21              [0.0, 0.0]  \n",
       "23              [0.0, 0.0]  \n",
       "24              [0.0, 0.0]  \n",
       "25              [0.0, 0.0]  \n",
       "26              [0.0, 0.0]  \n",
       "27              [0.0, 0.0]  \n",
       "28              [0.0, 0.0]  \n",
       "29              [0.0, 0.0]  \n",
       "30              [0.0, 0.0]  \n",
       "31              [0.0, 0.0]  \n",
       "32              [0.0, 0.0]  \n",
       "33              [0.0, 0.0]  \n",
       "34              [0.0, 0.0]  \n",
       "35              [0.0, 0.0]  \n",
       "36              [0.0, 0.0]  \n",
       "37              [0.0, 0.0]  \n",
       "38              [0.0, 0.0]  \n",
       "39              [0.0, 0.0]  \n",
       "22              [0.0, 0.0]  \n",
       "20              [0.0, 0.0]  \n",
       "1    [9.456276, 13.265001]  \n",
       "19              [0.0, 0.0]  \n",
       "2    [6.036743, 11.113943]  \n",
       "3          [0.0, 6.869545]  \n",
       "4         [0.0, 11.113943]  \n",
       "5         [0.0, 7.8627386]  \n",
       "6          [0.0, 4.563677]  \n",
       "7               [0.0, 0.0]  \n",
       "8               [0.0, 0.0]  \n",
       "9               [0.0, 0.0]  \n",
       "10              [0.0, 0.0]  \n",
       "11              [0.0, 0.0]  \n",
       "12              [0.0, 0.0]  \n",
       "13              [0.0, 0.0]  \n",
       "14              [0.0, 0.0]  \n",
       "15              [0.0, 0.0]  \n",
       "16              [0.0, 0.0]  \n",
       "17              [0.0, 0.0]  \n",
       "18              [0.0, 0.0]  \n",
       "40              [0.0, 0.0]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.loc[1, :][['keywords', 'display_rank',  'grade', 'last_prediction', 'delta', 'lambda', 'features']].sort_values('last_prediction', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set\n",
    "\n",
    "tree2 = DecisionTreeRegressor()\n",
    "tree2.fit(train_set['features'].tolist(), train_set['lambda'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More 'oomph' in second tree for the last tree's error cases\n",
    "\n",
    "We see in the following lambdas our next tree learns more about the areas the last model seemed to need correction. \n",
    "\n",
    "The first example is well covered by the first tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([173.48027244])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2.predict([[11.6, 10.08]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second example reflects some of the middling ranked results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.72350002])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2.predict([[0.0, 6.869545]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three - Weigh each leaf's predictions\n",
    "\n",
    "Because we're dealing with trees, each leaf corresponds to a set of examples that have been grouped to this node. In addition to per-swap 'rho' we also care about a per-swap 'weight', referred to in gradient boosting as 'gamma'. \n",
    "\n",
    "Gamma means picking a weight for this sub-model that best predicts the final function.\n",
    "\n",
    "First we group by the paths in the tree to uniquely identify each leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>uid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>train_dcg</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>213.776822</td>\n",
       "      <td>106.888411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>4.416508</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>48.010828</td>\n",
       "      <td>26.458003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>31.382749</td>\n",
       "      <td>18.143963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>1.292030</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>6.460558</td>\n",
       "      <td>7.448315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>5.802792</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>43.639845</td>\n",
       "      <td>21.819923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>40</td>\n",
       "      <td>40_37079</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.846078</td>\n",
       "      <td>4.923039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>40</td>\n",
       "      <td>40_126757</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.903461</td>\n",
       "      <td>4.951730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>40</td>\n",
       "      <td>40_39797</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.205847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.957655</td>\n",
       "      <td>4.978827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>40</td>\n",
       "      <td>40_18112</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-10.008949</td>\n",
       "      <td>5.004475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>40</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.289065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-10.057598</td>\n",
       "      <td>5.028799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      qid        uid   keywords   docId  grade                features  \\\n",
       "0       1     1_7555      rambo    7555      4  [11.657399, 10.083591]   \n",
       "1       1     1_1370      rambo    1370      3   [9.456276, 13.265001]   \n",
       "2       1     1_1369      rambo    1369      3   [6.036743, 11.113943]   \n",
       "3       1    1_13258      rambo   13258      2         [0.0, 6.869545]   \n",
       "4       1     1_1368      rambo    1368      4        [0.0, 11.113943]   \n",
       "...   ...        ...        ...     ...    ...                     ...   \n",
       "1385   40   40_37079  star wars   37079      0              [0.0, 0.0]   \n",
       "1386   40  40_126757  star wars  126757      0              [0.0, 0.0]   \n",
       "1387   40   40_39797  star wars   39797      0              [0.0, 0.0]   \n",
       "1388   40   40_18112  star wars   18112      0              [0.0, 0.0]   \n",
       "1389   40   40_43052  star wars   43052      0              [0.0, 0.0]   \n",
       "\n",
       "      last_prediction  display_rank  discount       gain  train_dcg  \\\n",
       "0                   0             0  1.000000  15.000000  30.700871   \n",
       "1                   0             1  0.630930   4.416508  30.700871   \n",
       "2                   0             2  0.500000   3.500000  30.700871   \n",
       "3                   0             3  0.430677   1.292030  30.700871   \n",
       "4                   0             4  0.386853   5.802792  30.700871   \n",
       "...               ...           ...       ...        ...        ...   \n",
       "1385                0            25  0.210310   0.000000  30.207651   \n",
       "1386                0            26  0.208015   0.000000  30.207651   \n",
       "1387                0            27  0.205847   0.000000  30.207651   \n",
       "1388                0            28  0.203795   0.000000  30.207651   \n",
       "1389                0             9  0.289065   0.000000  30.207651   \n",
       "\n",
       "            dcg      lambda      weight  \n",
       "0     30.552986  213.776822  106.888411  \n",
       "1     30.552986   48.010828   26.458003  \n",
       "2     30.552986   31.382749   18.143963  \n",
       "3     30.552986    6.460558    7.448315  \n",
       "4     30.552986   43.639845   21.819923  \n",
       "...         ...         ...         ...  \n",
       "1385  30.120435   -9.846078    4.923039  \n",
       "1386  30.120435   -9.903461    4.951730  \n",
       "1387  30.120435   -9.957655    4.978827  \n",
       "1388  30.120435  -10.008949    5.004475  \n",
       "1389  30.120435  -10.057598    5.028799  \n",
       "\n",
       "[1390 rows x 14 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_swaps_scaled_with_weights(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort to see ideal ordering\n",
    "    # This isn't strictly nescesarry, but it's helpful to understand the algorithm\n",
    "    query_judgments = query_judgments.sort_values('last_prediction', ascending=False, kind='stable').reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['train_dcg'] = query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    train_dcg = query_judgments.loc[0, 'dcg']\n",
    " \n",
    "    qid = query_judgments.loc[0, 'qid']\n",
    "    keywords = query_judgments.loc[0, 'keywords']\n",
    "\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    query_judgments['weight'] = 0.0\n",
    "\n",
    "    for better in range(0,len(query_judgments)):\n",
    "         for worse in range(0,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                return query_judgments\n",
    "                \n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                query_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "\n",
    "                dcg_after_swap = query_judgments.loc[0, 'dcg']\n",
    "                delta = abs(train_dcg - dcg_after_swap)\n",
    "\n",
    "                if delta != 0.0:\n",
    "                    last_model_score_diff = query_judgments.loc[better, 'last_prediction'] - query_judgments.loc[worse, 'last_prediction']\n",
    "                    rho = 1.0 / (1.0 + exp(last_model_score_diff)) \n",
    "\n",
    "                    assert(delta >= 0.0)\n",
    "                    assert(rho >= 0.0)\n",
    "                   \n",
    "                    query_judgments.loc[better, 'lambda'] += delta * rho\n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta * rho\n",
    "            \n",
    "                    # --------------\n",
    "                    # NEW!\n",
    "                    #  last_model_score_diff        rho         weight\n",
    "                    #      0.0                      0.5         0.25 (max possible value)\n",
    "                    #      100.0                    0.0000      0.0  (max possible value)\n",
    "                    # \n",
    "                    # If the current model has an ambiguous prediction, we include more of the delta in the weight\n",
    "                    # If the current model has a strong prediction, weight approaches 0\n",
    "                    query_judgments.loc[better, 'weight'] += rho * (1.0 - rho) * delta;\n",
    "                    query_judgments.loc[worse, 'weight'] += rho * (1.0 - rho) * delta;\n",
    "                    #\n",
    "                    # These will be used to rescale each decision tree node's predictions\n",
    "                    # If many results in a leaf node have last model score ~ ambiguous\n",
    "                    #     the resulting model will have a high denominator ~ (1 / deltaDCG)\n",
    "                    # If many results in a leaf node have last model score - not ambiguous, positive\n",
    "                    #     the resulting model will have a low denominator\n",
    "                    #\n",
    "                    # Apparently we want to cancel out the deltas if last model was ambiguous?\n",
    "                    # ---------------\n",
    "\n",
    "                    \n",
    "\n",
    "    return query_judgments\n",
    "\n",
    "# Convert to Pandas Dataframe\n",
    "judgments = judgments_to_dataframe(ftr_logger.logged, unnest=False)\n",
    "judgments['last_prediction'] = 0\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps_scaled_with_weights, axis=1)\n",
    "lambdas_per_query = lambdas_per_query.drop('qid', axis=1).reset_index().drop(['level_1', 'index'], axis=1)\n",
    "\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_leaf_nodes=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_leaf_nodes=4)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_leaf_nodes=4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set\n",
    "\n",
    "tree3 = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "tree3.fit(train_set['features'].tolist(), train_set['lambda'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label each row with its unique prediction (ie tree path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_paths(tree, X):\n",
    "    paths_as_array = tree.decision_path(X).toarray()\n",
    "    paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "    return paths\n",
    "\n",
    "lambdas_per_query['path'] = tree_paths(tree3, train_set['features'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override outputs using our own weighted average\n",
    "\n",
    "The typical decision tree uses either the [median or mean of the target values](https://scikit-learn.org/stable/modules/tree.html#regression-criteria) classified to a given leaf node as the prediction. However, in the case of lambdaMART, we want to use a weighted average that accounts for how much of the DCG error out there has been accounted for. Thus the psuedoresponses are summed and divided by the remaining error DCG.\n",
    "\n",
    "rho=0, then an example is weighed by `1/0.25*deltaNDCG` as there's a lot of outstanding DCG error left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "1010001    1673.720249\n",
       "1010010    1467.050422\n",
       "1100100     538.598514\n",
       "1101000    4655.114588\n",
       "Name: weight, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.groupby('path')['weight'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "1010001    3334.073882\n",
       "1010010    2787.629391\n",
       "1100100     893.120752\n",
       "1101000   -7014.824025\n",
       "Name: lambda, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.groupby('path')['lambda'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1010001': 1.9920138295288714,\n",
       " '1010010': 1.9001592234365985,\n",
       " '1100100': 1.658230999946508,\n",
       " '1101000': -1.5069068425436136}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "round_predictions.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0.8333333333333334, 'x[0] <= 10.328\\nsquared_error = 883.211\\nsamples = 1390\\nvalue = -0.0'),\n",
       " Text(0.25, 0.5, 'x[0] <= 9.182\\nsquared_error = 179.235\\nsamples = 1324\\nvalue = -4.624'),\n",
       " Text(0.125, 0.16666666666666666, 'squared_error = 62.9\\nsamples = 1301\\nvalue = -5.392'),\n",
       " Text(0.375, 0.16666666666666666, 'squared_error = 4838.036\\nsamples = 23\\nvalue = 38.831'),\n",
       " Text(0.75, 0.5, 'x[0] <= 13.782\\nsquared_error = 5973.405\\nsamples = 66\\nvalue = 92.753'),\n",
       " Text(0.625, 0.16666666666666666, 'squared_error = 5828.45\\nsamples = 39\\nvalue = 71.478'),\n",
       " Text(0.875, 0.16666666666666666, 'squared_error = 4584.565\\nsamples = 27\\nvalue = 123.484')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tree(tree3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap the original tree, and provide new value\n",
    "\n",
    "Instead of directly using the provided decision tree, we want to use our prediction for each leaf. This `OverridenDecisionTree` takes the original tree, looks up the prediction path in the original tree, but uses our values for the predicted variable instead of what's here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.50690684,  1.658231  ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OverridenRegressionTree:\n",
    "    def __init__(self, predictions, tree):\n",
    "        self.predictions = predictions\n",
    "        self.tree = tree\n",
    "        \n",
    "    def predict(self, X, use_original=False):\n",
    "        if use_original:\n",
    "            return self.predict(X)\n",
    "        path = self.tree.decision_path(X).toarray().astype(str)\n",
    "        path = \"\".join(path[0])\n",
    "        \n",
    "        paths_as_array = self.tree.decision_path(X).toarray()\n",
    "        paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "        \n",
    "        predictions = self.predictions[paths]\n",
    "        \n",
    "        # Any NaN predictions is a red flag, debug\n",
    "        if np.any(predictions.isnull()):\n",
    "            print(predictions[predictions.isnull()])\n",
    "            print(pd.DataFrame(X)[predictions.isnull().reset_index(drop=True)])\n",
    "            raise AssertionError(\"No prediction should be NaN\")\n",
    "        return np.array(self.predictions[paths].tolist())\n",
    "        \n",
    "override_tree = OverridenRegressionTree(predictions = round_predictions, tree=tree3)\n",
    "override_tree.predict([[0.0, 6.869545], [10.0, 10.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Four - Putting it all together, from the top!\n",
    "\n",
    "Now we can put together the full lambdamart algorithm that \n",
    "\n",
    "1. Uses pair-wise swaps on our our metric (ie DCG) to generate decision tree predictors (the 'lambdas')\n",
    "2. Focuses in on predicting where current model makes the wrong call when ranking by DCG\n",
    "3. Predicts using a weighted average, weighed by 1 / (remaining DCG)\n",
    "\n",
    "TODOs / known issues\n",
    "* While DCG converges, it does sometimes wander a tad, so there might be more room for improvement\n",
    "* Speeding up the inner loop of the `compute_swaps_scaled_with_weights` that must run for ever query's swaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uid', 'qid', 'keywords', 'docId', 'grade', 'features'], dtype='object')\n",
      "round 0\n",
      "Train DCGs\n",
      "mean    20.03521601393222\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 1\n",
      "Train DCGs\n",
      "mean    20.572514984109958\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 2\n",
      "Train DCGs\n",
      "mean    20.572514984109958\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 3\n",
      "Train DCGs\n",
      "mean    20.572514984109958\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 4\n",
      "Train DCGs\n",
      "mean    20.558839639948378\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 5\n",
      "Train DCGs\n",
      "mean    20.53155110954963\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 6\n",
      "Train DCGs\n",
      "mean    20.53155110954963\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 7\n",
      "Train DCGs\n",
      "mean    20.53155110954963\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 8\n",
      "Train DCGs\n",
      "mean    20.53155110954963\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 9\n",
      "Train DCGs\n",
      "mean    20.52107778379093\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 10\n",
      "Train DCGs\n",
      "mean    20.621960320672194\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 11\n",
      "Train DCGs\n",
      "mean    20.61883906072036\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 12\n",
      "Train DCGs\n",
      "mean    20.61883906072036\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 13\n",
      "Train DCGs\n",
      "mean    20.519913007038184\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 14\n",
      "Train DCGs\n",
      "mean    20.533588351199768\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 15\n",
      "Train DCGs\n",
      "mean    20.563842431095157\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 16\n",
      "Train DCGs\n",
      "mean    20.563842431095157\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 17\n",
      "Train DCGs\n",
      "mean    20.563842431095157\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 18\n",
      "Train DCGs\n",
      "mean    20.56191351702752\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 19\n",
      "Train DCGs\n",
      "mean    20.558476727978864\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 20\n",
      "Train DCGs\n",
      "mean    20.558476727978864\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 21\n",
      "Train DCGs\n",
      "mean    20.55948263670465\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 22\n",
      "Train DCGs\n",
      "mean    20.55948263670465\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 23\n",
      "Train DCGs\n",
      "mean    20.55948263670465\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 24\n",
      "Train DCGs\n",
      "mean    20.550810680749187\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 25\n",
      "Train DCGs\n",
      "mean    20.550810680749187\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 26\n",
      "Train DCGs\n",
      "mean    20.550810680749187\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 27\n",
      "Train DCGs\n",
      "mean    20.56689750665213\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 28\n",
      "Train DCGs\n",
      "mean    20.503985515709587\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 29\n",
      "Train DCGs\n",
      "mean    20.51849950081418\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 30\n",
      "Train DCGs\n",
      "mean    20.64945922172733\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 31\n",
      "Train DCGs\n",
      "mean    20.72515257027024\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 32\n",
      "Train DCGs\n",
      "mean    20.7231021798589\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 33\n",
      "Train DCGs\n",
      "mean    20.7231021798589\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 34\n",
      "Train DCGs\n",
      "mean    20.807701939752825\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 35\n",
      "Train DCGs\n",
      "mean    20.807701939752825\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 36\n",
      "Train DCGs\n",
      "mean    20.84599869297361\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 37\n",
      "Train DCGs\n",
      "mean    20.86404986536433\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 38\n",
      "Train DCGs\n",
      "mean    20.861574312122436\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 39\n",
      "Train DCGs\n",
      "mean    20.82140029663695\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 40\n",
      "Train DCGs\n",
      "mean    20.81392753573653\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 41\n",
      "Train DCGs\n",
      "mean    20.87286373201179\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 42\n",
      "Train DCGs\n",
      "mean    20.887642668254014\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 43\n",
      "Train DCGs\n",
      "mean    20.878882619993473\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 44\n",
      "Train DCGs\n",
      "mean    20.897199204390727\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 45\n",
      "Train DCGs\n",
      "mean    20.897199204390727\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 46\n",
      "Train DCGs\n",
      "mean    20.907937328064932\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 47\n",
      "Train DCGs\n",
      "mean    20.93711302124987\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 48\n",
      "Train DCGs\n",
      "mean    20.933960038871213\n",
      "median  19.416508275000204\n",
      "----------\n",
      "round 49\n",
      "Train DCGs\n",
      "mean    20.95297454045271\n",
      "median  19.416508275000204\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "\n",
    "def predict(ensemble, X, learning_rate=0.1):\n",
    "    prediction = 0\n",
    "    for tree in ensemble:\n",
    "        prediction += tree.predict(X) * learning_rate\n",
    "    return prediction.rename('prediction')\n",
    "\n",
    "\n",
    "def tree_paths(tree, X):\n",
    "    paths_as_array = tree.decision_path(X).toarray()\n",
    "    paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "    return paths\n",
    "\n",
    "ensemble=[]\n",
    "def lambda_mart(judgments, rounds=20, learning_rate=0.1, max_leaf_nodes=8, metric=dcg):\n",
    "\n",
    "    print(judgments.columns)\n",
    "    # Convert to Pandas Dataframe\n",
    "    lambdas_per_query = judgments.copy()\n",
    "\n",
    "\n",
    "    lambdas_per_query['last_prediction'] = 0.0\n",
    "\n",
    "    for i in range(0, rounds):\n",
    "        print(f\"round {i}\")\n",
    "\n",
    "        # ------------------\n",
    "        #1. Build pair-wise predictors for this round\n",
    "        lambdas_per_query = lambdas_per_query.groupby('qid').apply(compute_swaps_scaled_with_weights, \n",
    "                                                                   axis=1, metric=dcg)\n",
    "\n",
    "        # ------------------\n",
    "        #2. Train a regression tree on this round's lambdas\n",
    "        features = lambdas_per_query['features'].tolist()\n",
    "        tree = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes)\n",
    "        tree.fit(features, lambdas_per_query['lambda'])    \n",
    "\n",
    "        # ------------------\n",
    "        #3. Reweight based on LambdaMART's weighted average\n",
    "        # Add each tree's paths\n",
    "        lambdas_per_query['path'] = tree_paths(tree, features)\n",
    "        predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "        predictions = predictions.fillna(0.0) # for divide by 0\n",
    "\n",
    "        # -------------------\n",
    "        #4. Add to ensemble, recreate last prediction\n",
    "        new_tree = OverridenRegressionTree(predictions=predictions, tree=tree)\n",
    "        ensemble.append(new_tree)\n",
    "        next_predictions = new_tree.predict(features)\n",
    "        lambdas_per_query['last_prediction'] += (next_predictions * learning_rate) \n",
    "        \n",
    "        print(\"Train DCGs\")\n",
    "        print(\"mean   \", lambdas_per_query['train_dcg'].mean())\n",
    "        print(\"median \", lambdas_per_query['train_dcg'].median())\n",
    "        print(\"----------\")\n",
    "\n",
    "        \n",
    "        # Reset the dataframe for further processing\n",
    "\n",
    "        lambdas_per_query = lambdas_per_query.drop('qid', axis=1).reset_index().drop(['level_1', 'index'], axis=1)\n",
    "\n",
    "judgments = judgments_to_dataframe(ftr_logger.logged, unnest=False)\n",
    "lambdas_per_query = lambda_mart(judgments=judgments, rounds=50, max_leaf_nodes=10, learning_rate=0.1, metric=dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "1010001000000000000    1.992014\n",
       "1010010000001000000    1.787036\n",
       "1010010000010010000    2.000000\n",
       "1010010000010100100    1.840137\n",
       "1010010000010101001    2.000000\n",
       "1010010000010101010    1.847242\n",
       "1100100010100000000    1.723854\n",
       "1100100011000000000    2.000000\n",
       "1100100100000000000    0.097368\n",
       "1101000000000000000   -1.506907\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble[0].predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0.9285714285714286, 'x[0] <= 10.328\\nsquared_error = 883.211\\nsamples = 1390\\nvalue = -0.0'),\n",
       " Text(0.16666666666666666, 0.7857142857142857, 'x[0] <= 9.182\\nsquared_error = 179.235\\nsamples = 1324\\nvalue = -4.624'),\n",
       " Text(0.08333333333333333, 0.6428571428571429, 'squared_error = 62.9\\nsamples = 1301\\nvalue = -5.392'),\n",
       " Text(0.25, 0.6428571428571429, 'x[1] <= 4.527\\nsquared_error = 4838.036\\nsamples = 23\\nvalue = 38.831'),\n",
       " Text(0.16666666666666666, 0.5, 'squared_error = 139.057\\nsamples = 9\\nvalue = 0.585'),\n",
       " Text(0.3333333333333333, 0.5, 'x[1] <= 6.671\\nsquared_error = 6313.913\\nsamples = 14\\nvalue = 63.418'),\n",
       " Text(0.25, 0.35714285714285715, 'squared_error = 968.954\\nsamples = 2\\nvalue = 190.374'),\n",
       " Text(0.4166666666666667, 0.35714285714285715, 'squared_error = 4070.717\\nsamples = 12\\nvalue = 42.259'),\n",
       " Text(0.8333333333333334, 0.7857142857142857, 'x[0] <= 13.782\\nsquared_error = 5973.405\\nsamples = 66\\nvalue = 92.753'),\n",
       " Text(0.75, 0.6428571428571429, 'x[0] <= 12.02\\nsquared_error = 5828.45\\nsamples = 39\\nvalue = 71.478'),\n",
       " Text(0.6666666666666666, 0.5, 'x[0] <= 11.612\\nsquared_error = 6706.834\\nsamples = 26\\nvalue = 85.437'),\n",
       " Text(0.5833333333333334, 0.35714285714285715, 'x[1] <= 6.106\\nsquared_error = 6180.481\\nsamples = 23\\nvalue = 73.081'),\n",
       " Text(0.5, 0.21428571428571427, 'x[1] <= 0.89\\nsquared_error = 7629.179\\nsamples = 13\\nvalue = 95.374'),\n",
       " Text(0.4166666666666667, 0.07142857142857142, 'squared_error = 5152.412\\nsamples = 9\\nvalue = 54.653'),\n",
       " Text(0.5833333333333334, 0.07142857142857142, 'squared_error = 1076.183\\nsamples = 4\\nvalue = 186.997'),\n",
       " Text(0.6666666666666666, 0.21428571428571427, 'squared_error = 2811.196\\nsamples = 10\\nvalue = 44.1'),\n",
       " Text(0.75, 0.35714285714285715, 'squared_error = 598.201\\nsamples = 3\\nvalue = 180.165'),\n",
       " Text(0.8333333333333334, 0.5, 'squared_error = 2902.562\\nsamples = 13\\nvalue = 43.56'),\n",
       " Text(0.9166666666666666, 0.6428571428571429, 'squared_error = 4584.565\\nsamples = 27\\nvalue = 123.484')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tree(ensemble[0].tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to ranklib output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/33/jx0mw87156q2hmtrr_r82s7r0000gs/T/RankyMcRankFace.jar already exists\n",
      "Running java -jar /var/folders/33/jx0mw87156q2hmtrr_r82s7r0000gs/T/RankyMcRankFace.jar -ranker 6 -shrinkage 0.1 -metric2t DCG@10 -tree 10 -bag 1 -leaf 10 -frate 1.0 -srate 1.0 -train /var/folders/33/jx0mw87156q2hmtrr_r82s7r0000gs/T/training.txt -save data/title_model.txt \n",
      "Delete model title: 404\n",
      "Created Model title [Status: 201]\n",
      "Model saved\n",
      "Every N rounds of ranklib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[20.692,\n",
       " 20.669,\n",
       " 20.669,\n",
       " 20.669,\n",
       " 20.669,\n",
       " 20.73,\n",
       " 20.7506,\n",
       " 20.6507,\n",
       " 20.7237,\n",
       " 20.7237]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ltr.ranklib import train\n",
    "trainLog  = train(client,\n",
    "                  training_set=ftr_logger.logged,\n",
    "                  index='tmdb',\n",
    "                  trees=10,\n",
    "                  featureSet='movies',\n",
    "                  modelName='title')\n",
    "\n",
    "print(\"Every N rounds of ranklib\")\n",
    "trainLog.trainingLogs[0].rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine queries we learned\n",
    "\n",
    "Try out some queries, look at the final model prediction `last_prediction` compare to the correct ordering `grade`.\n",
    "\n",
    "NB: the notebook is referencing lambdas_per_query, which is None at this point in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambdas_per_query[lambdas_per_query['qid'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pure Pandas Implementation?\n",
    "\n",
    "Can we make it faster by vectorizing with pandas?\n",
    "\n",
    "Turns out Yes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            uid  qid   keywords   docId  grade                features  \\\n",
       "0        1_7555    1      rambo    7555      4  [11.657399, 10.083591]   \n",
       "1        1_1370    1      rambo    1370      3   [9.456276, 13.265001]   \n",
       "2        1_1369    1      rambo    1369      3   [6.036743, 11.113943]   \n",
       "3       1_13258    1      rambo   13258      2         [0.0, 6.869545]   \n",
       "4        1_1368    1      rambo    1368      4        [0.0, 11.113943]   \n",
       "...         ...  ...        ...     ...    ...                     ...   \n",
       "1385   40_37079   40  star wars   37079      0              [0.0, 0.0]   \n",
       "1386  40_126757   40  star wars  126757      0              [0.0, 0.0]   \n",
       "1387   40_39797   40  star wars   39797      0              [0.0, 0.0]   \n",
       "1388   40_18112   40  star wars   18112      0              [0.0, 0.0]   \n",
       "1389   40_43052   40  star wars   43052      0              [0.0, 0.0]   \n",
       "\n",
       "      last_prediction  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  \n",
       "...               ...  \n",
       "1385              0.0  \n",
       "1386              0.0  \n",
       "1387              0.0  \n",
       "1388              0.0  \n",
       "1389              0.0  \n",
       "\n",
       "[1390 rows x 7 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query = judgments.copy()\n",
    "\n",
    "\n",
    "lambdas_per_query['last_prediction'] = 0.0\n",
    "lambdas_per_query.sort_values(['qid', 'last_prediction'], ascending=[True, False], kind='stable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>grade</th>\n",
       "      <th>gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>40</td>\n",
       "      <td>26</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "      <td>0.205847</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>40</td>\n",
       "      <td>28</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>0.201849</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      qid  display_rank  discount  grade  gain\n",
       "0       1             0  1.000000      4    15\n",
       "1       1             1  0.630930      3     7\n",
       "2       1             2  0.500000      3     7\n",
       "3       1             3  0.430677      2     3\n",
       "4       1             4  0.386853      4    15\n",
       "...   ...           ...       ...    ...   ...\n",
       "1385   40            25  0.210310      0     0\n",
       "1386   40            26  0.208015      0     0\n",
       "1387   40            27  0.205847      0     0\n",
       "1388   40            28  0.203795      0     0\n",
       "1389   40            29  0.201849      0     0\n",
       "\n",
       "[1390 rows x 5 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.sort_values(['qid', 'last_prediction'], ascending=[True, False], kind='stable')\n",
    "lambdas_per_query['display_rank'] = lambdas_per_query.groupby('qid').cumcount()\n",
    "\n",
    "#TBD - How do generalize this?\n",
    "lambdas_per_query['discount'] = 1 / np.log2(2 + lambdas_per_query['display_rank'])\n",
    "lambdas_per_query['gain'] = (2**lambdas_per_query['grade'] - 1) # * lambdas_per_query['discount']\n",
    "\n",
    "lambdas_per_query[['qid', 'display_rank', 'discount', 'grade', 'gain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise deltas\n",
    "\n",
    "Delta captures pair-wise difference of the ranking metric (ie DCG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>display_rank_x</th>\n",
       "      <th>display_rank_y</th>\n",
       "      <th>delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.952562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.831881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49019</th>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49020</th>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49021</th>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49022</th>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49023</th>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49024 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid  display_rank_x  display_rank_y     delta\n",
       "0        1               0               0  0.000000\n",
       "1        1               0               1  2.952562\n",
       "2        1               0               2  4.000000\n",
       "3        1               0               3  6.831881\n",
       "4        1               0               4  0.000000\n",
       "...    ...             ...             ...       ...\n",
       "49019   40              29              25  0.000000\n",
       "49020   40              29              26  0.000000\n",
       "49021   40              29              27  0.000000\n",
       "49022   40              29              28  0.000000\n",
       "49023   40              29              29  0.000000\n",
       "\n",
       "[49024 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each group paired with each other group\n",
    "swaps = lambdas_per_query.merge(lambdas_per_query, on='qid', how='outer')\n",
    "# changes[j][i] = changes[i][j] = (discount(i) - discount(j)) * (gain(rel[i]) - gain(rel[j]));\n",
    "swaps['delta'] = np.abs((swaps['discount_x'] - swaps['discount_y']) * (swaps['gain_x'] - swaps['gain_y']))\n",
    "swaps[['qid', 'display_rank_x', 'display_rank_y', 'delta']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise rhos\n",
    "\n",
    "Rho captures pair-wise difference of the current model's prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>display_rank_x</th>\n",
       "      <th>display_rank_y</th>\n",
       "      <th>delta</th>\n",
       "      <th>last_prediction_x</th>\n",
       "      <th>last_prediction_y</th>\n",
       "      <th>rho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.952562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.831881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49019</th>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49020</th>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49021</th>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49022</th>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49023</th>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49024 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid  display_rank_x  display_rank_y     delta  last_prediction_x  \\\n",
       "0        1               0               0  0.000000                0.0   \n",
       "1        1               0               1  2.952562                0.0   \n",
       "2        1               0               2  4.000000                0.0   \n",
       "3        1               0               3  6.831881                0.0   \n",
       "4        1               0               4  0.000000                0.0   \n",
       "...    ...             ...             ...       ...                ...   \n",
       "49019   40              29              25  0.000000                0.0   \n",
       "49020   40              29              26  0.000000                0.0   \n",
       "49021   40              29              27  0.000000                0.0   \n",
       "49022   40              29              28  0.000000                0.0   \n",
       "49023   40              29              29  0.000000                0.0   \n",
       "\n",
       "       last_prediction_y  rho  \n",
       "0                    0.0  0.5  \n",
       "1                    0.0  0.5  \n",
       "2                    0.0  0.5  \n",
       "3                    0.0  0.5  \n",
       "4                    0.0  0.5  \n",
       "...                  ...  ...  \n",
       "49019                0.0  0.5  \n",
       "49020                0.0  0.5  \n",
       "49021                0.0  0.5  \n",
       "49022                0.0  0.5  \n",
       "49023                0.0  0.5  \n",
       "\n",
       "[49024 rows x 7 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swaps['rho'] = 1 / (1 + np.exp(swaps['last_prediction_x'] - swaps['last_prediction_y']))\n",
    "swaps[['qid', 'display_rank_x', 'display_rank_y', 'delta', 'last_prediction_x', 'last_prediction_y', 'rho']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute lambdas\n",
    "\n",
    "For every row where grade_x > grade_y,  compute `delta*rho`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>display_rank_x</th>\n",
       "      <th>display_rank_y</th>\n",
       "      <th>delta</th>\n",
       "      <th>last_prediction_x</th>\n",
       "      <th>last_prediction_y</th>\n",
       "      <th>rho</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.952562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.476281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.831881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.415941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49019</th>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49020</th>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49021</th>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49022</th>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49023</th>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49024 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid  display_rank_x  display_rank_y     delta  last_prediction_x  \\\n",
       "0        1               0               0  0.000000                0.0   \n",
       "1        1               0               1  2.952562                0.0   \n",
       "2        1               0               2  4.000000                0.0   \n",
       "3        1               0               3  6.831881                0.0   \n",
       "4        1               0               4  0.000000                0.0   \n",
       "...    ...             ...             ...       ...                ...   \n",
       "49019   40              29              25  0.000000                0.0   \n",
       "49020   40              29              26  0.000000                0.0   \n",
       "49021   40              29              27  0.000000                0.0   \n",
       "49022   40              29              28  0.000000                0.0   \n",
       "49023   40              29              29  0.000000                0.0   \n",
       "\n",
       "       last_prediction_y  rho    lambda  \n",
       "0                    0.0  0.5  0.000000  \n",
       "1                    0.0  0.5  1.476281  \n",
       "2                    0.0  0.5  2.000000  \n",
       "3                    0.0  0.5  3.415941  \n",
       "4                    0.0  0.5  0.000000  \n",
       "...                  ...  ...       ...  \n",
       "49019                0.0  0.5  0.000000  \n",
       "49020                0.0  0.5  0.000000  \n",
       "49021                0.0  0.5  0.000000  \n",
       "49022                0.0  0.5  0.000000  \n",
       "49023                0.0  0.5  0.000000  \n",
       "\n",
       "[49024 rows x 8 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swaps['lambda'] = 0\n",
    "slice_x_better =swaps[swaps['grade_x'] > swaps['grade_y']]\n",
    "swaps.loc[swaps['grade_x'] > swaps['grade_y'], 'lambda'] = slice_x_better['delta'] * slice_x_better['rho']\n",
    "swaps[['qid', 'display_rank_x', 'display_rank_y', 'delta', 'last_prediction_x', 'last_prediction_y', 'rho', 'lambda']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get per-key lambdas\n",
    "\n",
    "We merge back together the xs minuse the ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>211.781688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>46.938369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>30.637615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>5.888732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>43.177578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>40</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-9.853045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>40</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-9.911575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>40</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-9.966853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>40</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-10.019174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>40</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-10.068796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      qid   docId  grade                features      lambda\n",
       "0       1    7555      4  [11.657399, 10.083591]  211.781688\n",
       "1       1    1370      3   [9.456276, 13.265001]   46.938369\n",
       "2       1    1369      3   [6.036743, 11.113943]   30.637615\n",
       "3       1   13258      2         [0.0, 6.869545]    5.888732\n",
       "4       1    1368      4        [0.0, 11.113943]   43.177578\n",
       "...   ...     ...    ...                     ...         ...\n",
       "1385   40   37079      0              [0.0, 0.0]   -9.853045\n",
       "1386   40  126757      0              [0.0, 0.0]   -9.911575\n",
       "1387   40   39797      0              [0.0, 0.0]   -9.966853\n",
       "1388   40   18112      0              [0.0, 0.0]  -10.019174\n",
       "1389   40   43052      0              [0.0, 0.0]  -10.068796\n",
       "\n",
       "[1390 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better minus worse\n",
    "lambdas_x = swaps.groupby(['qid', 'display_rank_x'])['lambda'].sum().rename('lambda')\n",
    "lambdas_y = swaps.groupby(['qid', 'display_rank_y'])['lambda'].sum().rename('lambda')\n",
    "lambdas = lambdas_x - lambdas_y\n",
    "lambdas\n",
    "lambdas_per_query = lambdas_per_query.merge(lambdas, left_on=['qid', 'display_rank'], right_on=['qid', 'display_rank_x'], how='left')\n",
    "lambdas_per_query[['qid', 'docId', 'grade', 'features', 'lambda']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>lambda_x</th>\n",
       "      <th>lambda_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>211.781688</td>\n",
       "      <td>211.781688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>7</td>\n",
       "      <td>46.938369</td>\n",
       "      <td>46.938369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7</td>\n",
       "      <td>30.637615</td>\n",
       "      <td>30.637615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>3</td>\n",
       "      <td>5.888732</td>\n",
       "      <td>5.888732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>15</td>\n",
       "      <td>43.177578</td>\n",
       "      <td>43.177578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0</td>\n",
       "      <td>-9.853045</td>\n",
       "      <td>-9.853045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0</td>\n",
       "      <td>-9.911575</td>\n",
       "      <td>-9.911575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.205847</td>\n",
       "      <td>0</td>\n",
       "      <td>-9.966853</td>\n",
       "      <td>-9.966853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0</td>\n",
       "      <td>-10.019174</td>\n",
       "      <td>-10.019174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.201849</td>\n",
       "      <td>0</td>\n",
       "      <td>-10.068796</td>\n",
       "      <td>-10.068796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            uid  qid   keywords   docId  grade                features  \\\n",
       "0        1_7555    1      rambo    7555      4  [11.657399, 10.083591]   \n",
       "1        1_1370    1      rambo    1370      3   [9.456276, 13.265001]   \n",
       "2        1_1369    1      rambo    1369      3   [6.036743, 11.113943]   \n",
       "3       1_13258    1      rambo   13258      2         [0.0, 6.869545]   \n",
       "4        1_1368    1      rambo    1368      4        [0.0, 11.113943]   \n",
       "...         ...  ...        ...     ...    ...                     ...   \n",
       "1385   40_37079   40  star wars   37079      0              [0.0, 0.0]   \n",
       "1386  40_126757   40  star wars  126757      0              [0.0, 0.0]   \n",
       "1387   40_39797   40  star wars   39797      0              [0.0, 0.0]   \n",
       "1388   40_18112   40  star wars   18112      0              [0.0, 0.0]   \n",
       "1389   40_43052   40  star wars   43052      0              [0.0, 0.0]   \n",
       "\n",
       "      last_prediction  display_rank  discount  gain    lambda_x    lambda_y  \n",
       "0                 0.0             0  1.000000    15  211.781688  211.781688  \n",
       "1                 0.0             1  0.630930     7   46.938369   46.938369  \n",
       "2                 0.0             2  0.500000     7   30.637615   30.637615  \n",
       "3                 0.0             3  0.430677     3    5.888732    5.888732  \n",
       "4                 0.0             4  0.386853    15   43.177578   43.177578  \n",
       "...               ...           ...       ...   ...         ...         ...  \n",
       "1385              0.0            25  0.210310     0   -9.853045   -9.853045  \n",
       "1386              0.0            26  0.208015     0   -9.911575   -9.911575  \n",
       "1387              0.0            27  0.205847     0   -9.966853   -9.966853  \n",
       "1388              0.0            28  0.203795     0  -10.019174  -10.019174  \n",
       "1389              0.0            29  0.201849     0  -10.068796  -10.068796  \n",
       "\n",
       "[1390 rows x 12 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.merge(lambdas, left_on=['qid', 'display_rank'], right_on=['qid', 'display_rank_x'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_leaf_nodes=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_leaf_nodes=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_leaf_nodes=10)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Train a regression tree on this round's lambdas\n",
    "features = lambdas_per_query['features'].tolist()\n",
    "tree = DecisionTreeRegressor(max_leaf_nodes=10)\n",
    "tree.fit(features, lambdas_per_query['lambda'])    \n",
    "\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.append(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lambdas(lambdas_per_query):\n",
    "    lambdas_per_query = lambdas_per_query.sort_values(['qid', 'last_prediction'], ascending=[True, False], kind='stable')\n",
    "    lambdas_per_query['display_rank'] = lambdas_per_query.groupby('qid').cumcount()\n",
    "\n",
    "    #TBD - How do generalize this to any metric?\n",
    "    lambdas_per_query['discount'] = 1 / np.log2(2 + lambdas_per_query['display_rank'])\n",
    "    lambdas_per_query['gain'] = (2**lambdas_per_query['grade'] - 1)\n",
    "\n",
    "    # swaps dataframe holds each pair-wise swap computed (shrink columns for memory?)   \n",
    "    # Optimization of swaps = lambdas_per_query.merge(lambdas_per_query, on='qid', how='outer')\n",
    "    # to limit to just needed columns\n",
    "    to_swap = lambdas_per_query[['qid', 'display_rank', 'grade', 'last_prediction', 'discount', 'gain']]\n",
    "    #to_swap = lambdas_per_query\n",
    "    swaps = to_swap.merge(to_swap, on='qid', how='outer')\n",
    "\n",
    "    # delta - delta in DCG due to swap\n",
    "    swaps['delta'] = np.abs((swaps['discount_x'] - swaps['discount_y']) * (swaps['gain_x'] - swaps['gain_y']))\n",
    "    \n",
    "    # rho - based on current model prediction delta\n",
    "    swaps['rho'] = 1 / (1 + np.exp(swaps['last_prediction_x'] - swaps['last_prediction_y']))\n",
    "    \n",
    "    # If you want to be pure gradient boosting, weight reweights each models prediction\n",
    "    # I haven't found this to matter in practice\n",
    "    swaps['weight'] = swaps['rho'] * (1.0 - swaps['rho']) * swaps['delta']\n",
    "\n",
    "    # Compute lambdas (the next model in ensemble's predictors) when grade_x > grade_y\n",
    "    swaps['lambda'] = 0\n",
    "    slice_x_better =swaps[swaps['grade_x'] > swaps['grade_y']]\n",
    "    swaps.loc[swaps['grade_x'] > swaps['grade_y'], 'lambda'] = slice_x_better['delta'] * slice_x_better['rho']\n",
    "    \n",
    "    # accumulate lambdas and add back to model\n",
    "    lambdas_x = swaps.groupby(['qid', 'display_rank_x'])['lambda'].sum().rename('lambda')\n",
    "    lambdas_y = swaps.groupby(['qid', 'display_rank_y'])['lambda'].sum().rename('lambda')\n",
    "\n",
    "    weights_x = swaps.groupby(['qid', 'display_rank_x'])['weight'].sum().rename('weight')\n",
    "    weights_y = swaps.groupby(['qid', 'display_rank_y'])['weight'].sum().rename('weight')\n",
    "    \n",
    "    weights = weights_x + weights_y\n",
    "    lambdas = lambdas_x - lambdas_y\n",
    "\n",
    "    lambdas_per_query = lambdas_per_query.merge(lambdas, \n",
    "                                                left_on=['qid', 'display_rank'], \n",
    "                                                right_on=['qid', 'display_rank_x'], \n",
    "                                                how='left')\n",
    "    lambdas_per_query = lambdas_per_query.merge(weights, \n",
    "                                                left_on=['qid', 'display_rank'], \n",
    "                                                right_on=['qid', 'display_rank_x'], \n",
    "                                                how='left')\n",
    "\n",
    "    return lambdas_per_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uid', 'qid', 'keywords', 'docId', 'grade', 'features'], dtype='object')\n",
      "round 0\n",
      "grade                 4\n",
      "last_prediction    0.01\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.193092700772873\n",
      "----------\n",
      "round 1\n",
      "grade                     4\n",
      "last_prediction    0.019915\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 2\n",
      "grade                     4\n",
      "last_prediction    0.029745\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 3\n",
      "grade                     4\n",
      "last_prediction    0.039492\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 4\n",
      "grade                     4\n",
      "last_prediction    0.049159\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 5\n",
      "grade                     4\n",
      "last_prediction    0.058748\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 6\n",
      "grade                    4\n",
      "last_prediction    0.06826\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 7\n",
      "grade                     4\n",
      "last_prediction    0.077698\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 8\n",
      "grade                     4\n",
      "last_prediction    0.087063\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 9\n",
      "grade                     4\n",
      "last_prediction    0.096357\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 10\n",
      "grade                     4\n",
      "last_prediction    0.105582\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 11\n",
      "grade                     4\n",
      "last_prediction    0.114739\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 12\n",
      "grade                    4\n",
      "last_prediction    0.12383\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 13\n",
      "grade                     4\n",
      "last_prediction    0.132857\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 14\n",
      "grade                    4\n",
      "last_prediction    0.14182\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 15\n",
      "grade                     4\n",
      "last_prediction    0.150722\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 16\n",
      "grade                     4\n",
      "last_prediction    0.159564\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 17\n",
      "grade                     4\n",
      "last_prediction    0.168347\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 18\n",
      "grade                     4\n",
      "last_prediction    0.177072\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 19\n",
      "grade                     4\n",
      "last_prediction    0.185742\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 20\n",
      "grade                     4\n",
      "last_prediction    0.194356\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 21\n",
      "grade                     4\n",
      "last_prediction    0.202916\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 22\n",
      "grade                     4\n",
      "last_prediction    0.211424\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 23\n",
      "grade                    4\n",
      "last_prediction    0.21988\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 24\n",
      "grade                     4\n",
      "last_prediction    0.228285\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 25\n",
      "grade                     4\n",
      "last_prediction    0.236641\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 26\n",
      "grade                     4\n",
      "last_prediction    0.244949\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 27\n",
      "grade                     4\n",
      "last_prediction    0.253209\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 28\n",
      "grade                     4\n",
      "last_prediction    0.261423\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 29\n",
      "grade                     4\n",
      "last_prediction    0.269591\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 30\n",
      "grade                     4\n",
      "last_prediction    0.277715\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 31\n",
      "grade                     4\n",
      "last_prediction    0.285795\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 32\n",
      "grade                     4\n",
      "last_prediction    0.293832\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 33\n",
      "grade                     4\n",
      "last_prediction    0.301828\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 34\n",
      "grade                     4\n",
      "last_prediction    0.309782\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 35\n",
      "grade                     4\n",
      "last_prediction    0.317695\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 36\n",
      "grade                     4\n",
      "last_prediction    0.325569\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 37\n",
      "grade                     4\n",
      "last_prediction    0.333405\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 38\n",
      "grade                     4\n",
      "last_prediction    0.341202\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 39\n",
      "grade                     4\n",
      "last_prediction    0.348962\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 40\n",
      "grade                     4\n",
      "last_prediction    0.356685\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 41\n",
      "grade                     4\n",
      "last_prediction    0.364372\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 42\n",
      "grade                     4\n",
      "last_prediction    0.372024\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 43\n",
      "grade                     4\n",
      "last_prediction    0.379641\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 44\n",
      "grade                     4\n",
      "last_prediction    0.387224\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 45\n",
      "grade                     4\n",
      "last_prediction    0.394774\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 46\n",
      "grade                     4\n",
      "last_prediction    0.402291\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 47\n",
      "grade                     4\n",
      "last_prediction    0.409776\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 48\n",
      "grade                     4\n",
      "last_prediction    0.417229\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 49\n",
      "grade                     4\n",
      "last_prediction    0.424651\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "ensemble=[]\n",
    "def lambda_mart_pure(judgments, rounds=20, learning_rate=0.1, max_leaf_nodes=8, metric=dcg):\n",
    "\n",
    "    print(judgments.columns)\n",
    "    # Convert to Pandas Dataframe\n",
    "    lambdas_per_query = judgments.copy()\n",
    "\n",
    "\n",
    "    lambdas_per_query['last_prediction'] = 0.0\n",
    "\n",
    "    for i in range(0, rounds):\n",
    "        print(f\"round {i}\")\n",
    "\n",
    "        # ------------------\n",
    "        #1. Build pair-wise predictors for this round\n",
    "        lambdas_per_query = compute_lambdas(lambdas_per_query)\n",
    "\n",
    "        # ------------------\n",
    "        #2. Train a regression tree on this round's lambdas\n",
    "        features = lambdas_per_query['features'].tolist()\n",
    "        tree = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes)\n",
    "        tree.fit(features, lambdas_per_query['lambda'])    \n",
    "\n",
    "        # ------------------\n",
    "        #3. Reweight based on LambdaMART's weighted average\n",
    "        # Add each tree's paths\n",
    "        lambdas_per_query['path'] = tree_paths(tree, features)\n",
    "        predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "        predictions = predictions.fillna(0.0) # for divide by 0\n",
    "\n",
    "        # -------------------\n",
    "        #4. Add to ensemble, recreate last prediction\n",
    "        new_tree = OverridenRegressionTree(predictions=predictions, tree=tree)\n",
    "        ensemble.append(new_tree)\n",
    "        next_predictions = new_tree.predict(features)\n",
    "        lambdas_per_query['last_prediction'] += (next_predictions * learning_rate) \n",
    "        \n",
    "        print(lambdas_per_query.loc[0, ['grade', 'last_prediction']])\n",
    "        \n",
    "        print(\"Train DCGs\")\n",
    "        lambdas_per_query['discounted_gain'] = lambdas_per_query['gain'] * lambdas_per_query['discount'] \n",
    "        dcg = lambdas_per_query[lambdas_per_query['display_rank'] < 10].groupby('qid')['discounted_gain'].sum().mean()\n",
    "        print(\"mean   \", dcg)\n",
    "        print(\"----------\")\n",
    "        \n",
    "        lambdas_per_query = lambdas_per_query.drop(['lambda', 'weight'], axis=1)\n",
    "    return lambdas_per_query\n",
    "\n",
    "\n",
    "judgments = judgments_to_dataframe(ftr_logger.logged, unnest=False)\n",
    "lambdas_per_query = lambda_mart_pure(judgments=judgments, rounds=50, max_leaf_nodes=10, learning_rate=0.01, metric=dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uid', 'qid', 'keywords', 'docId', 'grade', 'features'], dtype='object')\n",
      "round 0\n",
      "grade                 4\n",
      "last_prediction    0.01\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.193092700772873\n",
      "----------\n",
      "round 1\n",
      "grade                     4\n",
      "last_prediction    0.019915\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 2\n",
      "grade                     4\n",
      "last_prediction    0.029745\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 3\n",
      "grade                     4\n",
      "last_prediction    0.039492\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 4\n",
      "grade                     4\n",
      "last_prediction    0.049159\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 5\n",
      "grade                     4\n",
      "last_prediction    0.058748\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 6\n",
      "grade                    4\n",
      "last_prediction    0.06826\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 7\n",
      "grade                     4\n",
      "last_prediction    0.077698\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 8\n",
      "grade                     4\n",
      "last_prediction    0.087063\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 9\n",
      "grade                     4\n",
      "last_prediction    0.096357\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 10\n",
      "grade                     4\n",
      "last_prediction    0.105582\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 11\n",
      "grade                     4\n",
      "last_prediction    0.114739\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 12\n",
      "grade                    4\n",
      "last_prediction    0.12383\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 13\n",
      "grade                     4\n",
      "last_prediction    0.132857\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 14\n",
      "grade                    4\n",
      "last_prediction    0.14182\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 15\n",
      "grade                     4\n",
      "last_prediction    0.150722\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 16\n",
      "grade                     4\n",
      "last_prediction    0.159564\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 17\n",
      "grade                     4\n",
      "last_prediction    0.168347\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 18\n",
      "grade                     4\n",
      "last_prediction    0.177072\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 19\n",
      "grade                     4\n",
      "last_prediction    0.185742\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 20\n",
      "grade                     4\n",
      "last_prediction    0.194356\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 21\n",
      "grade                     4\n",
      "last_prediction    0.202916\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 22\n",
      "grade                     4\n",
      "last_prediction    0.211424\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 23\n",
      "grade                    4\n",
      "last_prediction    0.21988\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 24\n",
      "grade                     4\n",
      "last_prediction    0.228285\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 25\n",
      "grade                     4\n",
      "last_prediction    0.236641\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 26\n",
      "grade                     4\n",
      "last_prediction    0.244949\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 27\n",
      "grade                     4\n",
      "last_prediction    0.253209\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 28\n",
      "grade                     4\n",
      "last_prediction    0.261423\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 29\n",
      "grade                     4\n",
      "last_prediction    0.269591\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 30\n",
      "grade                     4\n",
      "last_prediction    0.277715\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 31\n",
      "grade                     4\n",
      "last_prediction    0.285795\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 32\n",
      "grade                     4\n",
      "last_prediction    0.293832\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 33\n",
      "grade                     4\n",
      "last_prediction    0.301828\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 34\n",
      "grade                     4\n",
      "last_prediction    0.309782\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 35\n",
      "grade                     4\n",
      "last_prediction    0.317695\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 36\n",
      "grade                     4\n",
      "last_prediction    0.325569\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 37\n",
      "grade                     4\n",
      "last_prediction    0.333405\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 38\n",
      "grade                     4\n",
      "last_prediction    0.341202\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 39\n",
      "grade                     4\n",
      "last_prediction    0.348962\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 40\n",
      "grade                     4\n",
      "last_prediction    0.356685\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 41\n",
      "grade                     4\n",
      "last_prediction    0.364372\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 42\n",
      "grade                     4\n",
      "last_prediction    0.372024\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 43\n",
      "grade                     4\n",
      "last_prediction    0.379641\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 44\n",
      "grade                     4\n",
      "last_prediction    0.387224\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 45\n",
      "grade                     4\n",
      "last_prediction    0.394774\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 46\n",
      "grade                     4\n",
      "last_prediction    0.402291\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 47\n",
      "grade                     4\n",
      "last_prediction    0.409776\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 48\n",
      "grade                     4\n",
      "last_prediction    0.417229\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 49\n",
      "grade                     4\n",
      "last_prediction    0.424651\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      " "
     ]
    }
   ],
   "source": [
    "judgments = judgments_to_dataframe(ftr_logger.logged, unnest=False)\n",
    "%prun -s cumtime lambdas_per_query = lambda_mart_pure(judgments=judgments, rounds=50, max_leaf_nodes=10, learning_rate=0.01, metric=dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/33/jx0mw87156q2hmtrr_r82s7r0000gs/T/RankyMcRankFace.jar already exists\n",
      "Running java -jar /var/folders/33/jx0mw87156q2hmtrr_r82s7r0000gs/T/RankyMcRankFace.jar -ranker 6 -shrinkage 0.1 -metric2t DCG@10 -tree 10 -bag 1 -leaf 10 -frate 1.0 -srate 1.0 -train /var/folders/33/jx0mw87156q2hmtrr_r82s7r0000gs/T/training.txt -save data/title_model.txt \n",
      "Delete model title: 200\n",
      "Created Model title [Status: 201]\n",
      "Model saved\n",
      " Every N rounds of ranklib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[20.692,\n",
       " 20.669,\n",
       " 20.669,\n",
       " 20.669,\n",
       " 20.669,\n",
       " 20.73,\n",
       " 20.7506,\n",
       " 20.6507,\n",
       " 20.7237,\n",
       " 20.7237]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ltr.ranklib import train\n",
    "%prun -s cumtime trainLog  = train(client, training_set=ftr_logger.logged, index='tmdb', trees=10, featureSet='movies', modelName='title')\n",
    "\n",
    "print(\"Every N rounds of ranklib\")\n",
    "trainLog.trainingLogs[0].rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverridenRegressionTree:\n",
    "    def __init__(self, predictions, tree):\n",
    "        self.predictions = predictions\n",
    "        self.tree = tree\n",
    "        \n",
    "    def predict(self, X, use_original=False):\n",
    "        if use_original:\n",
    "            return self.predict(X)\n",
    "        path = self.tree.decision_path(X).toarray().astype(str)\n",
    "        path = \"\".join(path[0])\n",
    "        \n",
    "        paths_as_array = self.tree.decision_path(X).toarray()\n",
    "        paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "        \n",
    "        predictions = self.predictions[paths]\n",
    "        \n",
    "        # Any NaN predictions is a red flag, debug\n",
    "        if np.any(predictions.isnull()):\n",
    "            print(predictions[predictions.isnull()])\n",
    "            print(pd.DataFrame(X)[predictions.isnull().reset_index(drop=True)])\n",
    "            raise AssertionError(\"No prediction should be NaN\")\n",
    "        return np.array(self.predictions[paths].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
